{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bVCCno75TOA1",
        "25_F1VSkTSdt",
        "ciMogawiUOgi",
        "3uWILnqKWiL0",
        "XemoZATc5XCI",
        "ICHQ_-a5UfKm",
        "xo8rHsibCJk3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Library Installation and Loading**"
      ],
      "metadata": {
        "id": "bVCCno75TOA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4 --upgrade --force-reinstall --quiet\n",
        "!pip install --upgrade pennylane pennylane-lightning\n",
        "!pip install gensim\n",
        "!pip install pytorch-crf --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emke0PbTTWHA",
        "outputId": "c3311848-6450-4e43-8f99-b160fd716ca9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting pennylane\n",
            "  Downloading pennylane-0.42.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pennylane-lightning\n",
            "  Downloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray<0.8,>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.26.4)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning)\n",
            "  Downloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.8.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading pennylane-0.42.3-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.2-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.2 diastatic-malt-2.15.2 pennylane-0.42.3 pennylane-lightning-0.42.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.0.2\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, gensim\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "uruXqw_-Tac6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck62FRx6SbWC",
        "outputId": "4ccb12b1-ff12-4b24-d9f7-c128ad479a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import os\n",
        "import time\n",
        "import numpy as onp\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torchcrf import CRF\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "\n",
        "import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "onp.random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading and Preprocessing**"
      ],
      "metadata": {
        "id": "25_F1VSkTSdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    sentences, tags = [], []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        sent, tag_seq = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(\":\")\n",
        "                if len(parts) >= 2:\n",
        "                    word = parts[0].strip()\n",
        "                    tag = parts[-1].strip()\n",
        "                    if word in fasttext:\n",
        "                        sent.append(fasttext[word])\n",
        "                    else:\n",
        "                        sent.append(np.zeros(100))\n",
        "                    tag_seq.append(tag)\n",
        "            else:\n",
        "                if sent and tag_seq:\n",
        "                    sentences.append(sent)\n",
        "                    tags.append(tag_seq)\n",
        "                sent, tag_seq = [], []\n",
        "        if sent and tag_seq:\n",
        "            sentences.append(sent)\n",
        "            tags.append(tag_seq)\n",
        "    return sentences, tags"
      ],
      "metadata": {
        "id": "fSSNI3ACWJXF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext = KeyedVectors.load_word2vec_format(\n",
        "    \"/content/drive/MyDrive/QML-Research/Data/hi_Fasttext_vectors/wordVectors100.txt\",\n",
        "    binary=False\n",
        ")\n",
        "train_sentences, train_tags = load_data(\"/content/drive/MyDrive/QML-Research/Data/Data-POS_T/fb/fb_hi_cg_train2.txt\")\n",
        "\n",
        "train_val_sents, test_sents, train_val_tags, test_tags = train_test_split(\n",
        "    train_sentences, train_tags, test_size=0.2, random_state=SEED\n",
        ")\n",
        "\n",
        "train_sents, val_sents, train_tags, val_tags = train_test_split(\n",
        "    train_val_sents, train_val_tags, test_size=0.25, random_state=SEED\n",
        ")\n",
        "\n",
        "all_tags = sorted(set(tag for seq in train_tags for tag in seq))\n",
        "tag2id = {tag: idx for idx, tag in enumerate(all_tags)}\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}"
      ],
      "metadata": {
        "id": "Y-N4srmnSmWP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = [len(s) for s in train_sentences]\n",
        "print(\"Min:\", min(lengths), \"Max:\", max(lengths), \"Mean:\", onp.mean(lengths))\n",
        "print(\"95th percentile:\", int(onp.percentile(lengths, 95)))\n",
        "MAX_LEN = int(onp.percentile(lengths, 95))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIY9mm8tbuXP",
        "outputId": "cc5c3f7a-ddb0-486b-8ae1-a3bd4d6bcde1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min: 1 Max: 245 Mean: 18.376052385406922\n",
            "95th percentile: 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_collate(batch, pad_dim=100, max_len=MAX_LEN):\n",
        "    X_batch, y_batch = [], []\n",
        "    for x, y in batch:\n",
        "        if x.shape[0] > max_len:\n",
        "            x = x[:max_len]\n",
        "            y = y[:max_len]\n",
        "        pad_len = max_len - x.shape[0]\n",
        "        if pad_len > 0:\n",
        "            pad_x = torch.zeros(pad_len, pad_dim)\n",
        "            pad_y = torch.full((pad_len,), -100)\n",
        "            x = torch.cat([x, pad_x], dim=0)\n",
        "            y = torch.cat([y, pad_y], dim=0)\n",
        "        X_batch.append(x)\n",
        "        y_batch.append(y)\n",
        "    return torch.stack(X_batch), torch.stack(y_batch)"
      ],
      "metadata": {
        "id": "XJr9a3ZbbiVM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PosDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, tag2id):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.tag2id = tag2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = np.stack(self.sentences[idx])\n",
        "        x = torch.tensor(sent, dtype=torch.float32)\n",
        "        y = torch.tensor([self.tag2id[t] for t in self.tags[idx]], dtype=torch.long)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "5rxTyw0ublqU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PosDataset(train_sents, train_tags, tag2id)\n",
        "val_dataset   = PosDataset(val_sents, val_tags, tag2id)\n",
        "test_dataset  = PosDataset(test_sents, test_tags, tag2id)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32, collate_fn=pad_collate)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, collate_fn=pad_collate)"
      ],
      "metadata": {
        "id": "m0w-1LifWH7n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "print(len(val_dataset))\n",
        "print(len(test_dataset))\n",
        "print(len(train_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5EeKv2HUUs3",
        "outputId": "ce5a8a0b-01d2-46fd-deaf-1fb9ef61759b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "641\n",
            "214\n",
            "214\n",
            "1069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model & QNode Definition**"
      ],
      "metadata": {
        "id": "ciMogawiUOgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_qubits = 8\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# @qml.qnode(dev, interface=\"torch\")\n",
        "# def quantum_circuit(inputs, weights1, weights2):\n",
        "#     qml.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
        "#     qml.BasicEntanglerLayers(weights1, wires=range(n_qubits))\n",
        "#     qml.BasicEntanglerLayers(weights2, wires=range(n_qubits))\n",
        "#     return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# weight_shapes = {\n",
        "#     \"weights1\": (6, n_qubits),\n",
        "#     \"weights2\": (6, n_qubits)\n",
        "# }\n",
        "# quantum_layer = qml.qnn.TorchLayer(quantum_circuit, weight_shapes)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_circuit(inputs, weights):\n",
        "    qml.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
        "    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "weight_shapes = {\"weights\": (6, n_qubits),}\n",
        "quantum_layer = qml.qnn.TorchLayer(quantum_circuit, weight_shapes)"
      ],
      "metadata": {
        "id": "cs6p1beXSmRM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pure Classical Model\n",
        "\n",
        "class GRU_Classical(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=num_layers,\n",
        "                          batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "        self.crf = CRF(num_tags=num_classes, batch_first=True)\n",
        "\n",
        "    def forward(self, x, tags=None, mask=None):\n",
        "        out, _ = self.gru(x)\n",
        "        emissions = self.fc_layers(out)\n",
        "\n",
        "        if tags is not None:\n",
        "            return -self.crf(emissions, tags, mask=mask, reduction=\"mean\")\n",
        "        else:\n",
        "            return self.crf.decode(emissions, mask=mask)"
      ],
      "metadata": {
        "id": "Jf2aP2Re3F73"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantum classical Hybrid Model\n",
        "\n",
        "class HybridGRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=num_layers,\n",
        "                          batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc_reduce = nn.Linear(32, n_qubits)\n",
        "        self.quantum = quantum_layer\n",
        "        self.fc = nn.Linear(n_qubits, num_classes)\n",
        "        self.crf = CRF(num_classes, batch_first=True)\n",
        "\n",
        "    def forward(self, x, tags=None, mask=None):\n",
        "        out, _ = self.gru(x)\n",
        "        out = self.fc_reduce(out)\n",
        "        batch, seq_len, _ = out.shape\n",
        "        out = out.reshape(-1, n_qubits)\n",
        "        out = self.quantum(out)\n",
        "        out = self.fc(out)\n",
        "        out = out.view(batch, seq_len, -1)\n",
        "\n",
        "        if tags is not None and mask is not None:\n",
        "            log_likelihood = self.crf(out, tags, mask=mask, reduction=\"mean\")\n",
        "            return -log_likelihood\n",
        "        else:\n",
        "            return self.crf.decode(out, mask=mask)"
      ],
      "metadata": {
        "id": "4RNw7vUGe-gR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Paramter Initialization**"
      ],
      "metadata": {
        "id": "3uWILnqKWiL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_hybrid = HybridGRU(input_dim=100, hidden_dim=16, num_classes=len(tag2id), num_layers=2)\n",
        "\n",
        "model_classical = GRU_Classical(\n",
        "    input_dim=100,\n",
        "    hidden_dim=16,\n",
        "    num_classes=len(tag2id),\n",
        "    num_layers=2\n",
        ")\n",
        "\n",
        "optimizer_hybrid = optim.Adam(model_hybrid.parameters(), lr=0.001)\n",
        "optimizer_classical = optim.Adam(model_classical.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "num_epochs = 300"
      ],
      "metadata": {
        "id": "Cr07aFMxWhvG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parameter Count**"
      ],
      "metadata": {
        "id": "XemoZATc5XCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_named_parameter_breakdown(model):\n",
        "    total = 0\n",
        "    print(f\"{'Name':60} {'Shape':20} {'#params':>12}\")\n",
        "    print(\"-\" * 100)\n",
        "    for name, p in model.named_parameters():\n",
        "        cnt = p.numel()\n",
        "        print(f\"{name:60} {tuple(p.shape)!s:20} {cnt:12,}\")\n",
        "        total += cnt\n",
        "    print(\"-\" * 100)\n",
        "    print(f\"{'Total parameters':60} {total:12,}\")\n",
        "    return total\n",
        "\n",
        "def print_module_parameter_summary(model):\n",
        "    groups = defaultdict(int)\n",
        "    for name, p in model.named_parameters():\n",
        "        top = name.split('.')[0] if '.' in name else name\n",
        "        groups[top] += p.numel()\n",
        "    print(f\"{'Module':40} {'#params':>15}\")\n",
        "    print(\"-\" * 60)\n",
        "    total = 0\n",
        "    for module_name, cnt in sorted(groups.items(), key=lambda x: -x[1]):\n",
        "        print(f\"{module_name:40} {cnt:15,}\")\n",
        "        total += cnt\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Total':40} {total:15,}\")\n",
        "    return total\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Oe49crs25aYI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classical Model Parameter Count\n",
        "\n",
        "print(\"Classical named param breakdown\")\n",
        "print_named_parameter_breakdown(model_classical)\n",
        "print(\"\\nClassical module summary\")\n",
        "print_module_parameter_summary(model_classical)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rcyzEuD8saw",
        "outputId": "a43ad0e0-b821-423e-9960-87b19d4df594"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classical named param breakdown\n",
            "Name                                                         Shape                     #params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "gru.weight_ih_l0                                             (48, 100)                   4,800\n",
            "gru.weight_hh_l0                                             (48, 16)                      768\n",
            "gru.bias_ih_l0                                               (48,)                          48\n",
            "gru.bias_hh_l0                                               (48,)                          48\n",
            "gru.weight_ih_l0_reverse                                     (48, 100)                   4,800\n",
            "gru.weight_hh_l0_reverse                                     (48, 16)                      768\n",
            "gru.bias_ih_l0_reverse                                       (48,)                          48\n",
            "gru.bias_hh_l0_reverse                                       (48,)                          48\n",
            "gru.weight_ih_l1                                             (48, 32)                    1,536\n",
            "gru.weight_hh_l1                                             (48, 16)                      768\n",
            "gru.bias_ih_l1                                               (48,)                          48\n",
            "gru.bias_hh_l1                                               (48,)                          48\n",
            "gru.weight_ih_l1_reverse                                     (48, 32)                    1,536\n",
            "gru.weight_hh_l1_reverse                                     (48, 16)                      768\n",
            "gru.bias_ih_l1_reverse                                       (48,)                          48\n",
            "gru.bias_hh_l1_reverse                                       (48,)                          48\n",
            "fc_layers.0.weight                                           (32, 32)                    1,024\n",
            "fc_layers.0.bias                                             (32,)                          32\n",
            "fc_layers.2.weight                                           (11, 32)                      352\n",
            "fc_layers.2.bias                                             (11,)                          11\n",
            "crf.start_transitions                                        (11,)                          11\n",
            "crf.end_transitions                                          (11,)                          11\n",
            "crf.transitions                                              (11, 11)                      121\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Total parameters                                                   17,690\n",
            "\n",
            "Classical module summary\n",
            "Module                                           #params\n",
            "------------------------------------------------------------\n",
            "gru                                               16,128\n",
            "fc_layers                                          1,419\n",
            "crf                                                  143\n",
            "------------------------------------------------------------\n",
            "Total                                             17,690\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17690"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantum-Classical Hybrid Model Parameter Count\n",
        "\n",
        "print(\"Hybrid named param breakdown\")\n",
        "print_named_parameter_breakdown(model_hybrid)\n",
        "print(\"\\nHybrid module summary\")\n",
        "print_module_parameter_summary(model_hybrid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bYuSgnV8vTx",
        "outputId": "48016a62-88b5-40bf-ec7e-8952cde4271e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid named param breakdown\n",
            "Name                                                         Shape                     #params\n",
            "----------------------------------------------------------------------------------------------------\n",
            "gru.weight_ih_l0                                             (48, 100)                   4,800\n",
            "gru.weight_hh_l0                                             (48, 16)                      768\n",
            "gru.bias_ih_l0                                               (48,)                          48\n",
            "gru.bias_hh_l0                                               (48,)                          48\n",
            "gru.weight_ih_l0_reverse                                     (48, 100)                   4,800\n",
            "gru.weight_hh_l0_reverse                                     (48, 16)                      768\n",
            "gru.bias_ih_l0_reverse                                       (48,)                          48\n",
            "gru.bias_hh_l0_reverse                                       (48,)                          48\n",
            "gru.weight_ih_l1                                             (48, 32)                    1,536\n",
            "gru.weight_hh_l1                                             (48, 16)                      768\n",
            "gru.bias_ih_l1                                               (48,)                          48\n",
            "gru.bias_hh_l1                                               (48,)                          48\n",
            "gru.weight_ih_l1_reverse                                     (48, 32)                    1,536\n",
            "gru.weight_hh_l1_reverse                                     (48, 16)                      768\n",
            "gru.bias_ih_l1_reverse                                       (48,)                          48\n",
            "gru.bias_hh_l1_reverse                                       (48,)                          48\n",
            "fc_reduce.weight                                             (8, 32)                       256\n",
            "fc_reduce.bias                                               (8,)                            8\n",
            "quantum.weights                                              (6, 8)                         48\n",
            "fc.weight                                                    (11, 8)                        88\n",
            "fc.bias                                                      (11,)                          11\n",
            "crf.start_transitions                                        (11,)                          11\n",
            "crf.end_transitions                                          (11,)                          11\n",
            "crf.transitions                                              (11, 11)                      121\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Total parameters                                                   16,682\n",
            "\n",
            "Hybrid module summary\n",
            "Module                                           #params\n",
            "------------------------------------------------------------\n",
            "gru                                               16,128\n",
            "fc_reduce                                            264\n",
            "crf                                                  143\n",
            "fc                                                    99\n",
            "quantum                                               48\n",
            "------------------------------------------------------------\n",
            "Total                                             16,682\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16682"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Loop**"
      ],
      "metadata": {
        "id": "ICHQ_-a5UfKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classical Model Training Loop\n",
        "\n",
        "patience = 5\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs_no_improve = 0\n",
        "\n",
        "total_start = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_classical.train()\n",
        "    epoch_start = time.time()\n",
        "    correct, total, epoch_loss = 0, 0, 0\n",
        "\n",
        "    for X, y in train_loader:\n",
        "        mask = (y != -100)\n",
        "        optimizer_classical.zero_grad()\n",
        "\n",
        "        y_clamped = y.clone()\n",
        "        y_clamped[y_clamped == -100] = 0\n",
        "        loss = model_classical(X, tags=y_clamped, mask=mask)\n",
        "        loss.backward()\n",
        "        optimizer_classical.step()\n",
        "        epoch_loss += loss.item()\n",
        "        preds = model_classical(X, mask=mask)\n",
        "\n",
        "        for p_seq, y_seq, m_seq in zip(preds, y, mask):\n",
        "            for p, gold, m in zip(p_seq, y_seq, m_seq):\n",
        "                if m:\n",
        "                    correct += (p == gold.item())\n",
        "                    total += 1\n",
        "\n",
        "    acc = correct / total if total > 0 else 0\n",
        "    elapsed = time.time() - epoch_start\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss/len(train_loader):.4f} | \"\n",
        "          f\"Acc: {acc:.4f} | Time: {elapsed:.2f}s\")\n",
        "\n",
        "    # ---------------- Validation ----------------\n",
        "    model_classical.eval()\n",
        "    val_correct, val_total, val_loss = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            mask = (y != -100)\n",
        "            y_clamped = y.clone()\n",
        "            y_clamped[y_clamped == -100] = 0\n",
        "\n",
        "            loss = model_classical(X, tags=y_clamped, mask=mask)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = model_classical(X, mask=mask)\n",
        "            for p_seq, y_seq, m_seq in zip(preds, y, mask):\n",
        "                for p, gold, m in zip(p_seq, y_seq, m_seq):\n",
        "                    if m:\n",
        "                        val_correct += (p == gold.item())\n",
        "                        val_total += 1\n",
        "\n",
        "    val_acc = val_correct / val_total if val_total > 0 else 0\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"   >> Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "    model_classical.train()\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        # torch.save(model_classical.state_dict(),\n",
        "        #            \"/content/drive/MyDrive/QML-Research/Model Saves/Classical/POS_T/final_check_fb_hi_cg.pt\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nTotal Training Time: {time.time() - total_start:.2f}s\")"
      ],
      "metadata": {
        "id": "T7_015n3WfO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f4c3e26-1b07-428f-e2ef-e23264feb992"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300 | Loss: 36.5567 | Acc: 0.2193 | Time: 5.73s\n",
            "   >> Val Loss: 36.3103 | Val Acc: 0.2529\n",
            "Epoch 2/300 | Loss: 35.8941 | Acc: 0.3186 | Time: 5.05s\n",
            "   >> Val Loss: 34.3356 | Val Acc: 0.3695\n",
            "Epoch 3/300 | Loss: 31.2194 | Acc: 0.3924 | Time: 6.22s\n",
            "   >> Val Loss: 31.5890 | Val Acc: 0.3851\n",
            "Epoch 4/300 | Loss: 29.7737 | Acc: 0.4064 | Time: 4.62s\n",
            "   >> Val Loss: 28.8968 | Val Acc: 0.4062\n",
            "Epoch 5/300 | Loss: 25.8654 | Acc: 0.4388 | Time: 3.82s\n",
            "   >> Val Loss: 26.6613 | Val Acc: 0.4585\n",
            "Epoch 6/300 | Loss: 24.5569 | Acc: 0.5017 | Time: 4.76s\n",
            "   >> Val Loss: 24.5159 | Val Acc: 0.5367\n",
            "Epoch 7/300 | Loss: 21.7687 | Acc: 0.5834 | Time: 3.62s\n",
            "   >> Val Loss: 22.3107 | Val Acc: 0.5860\n",
            "Epoch 8/300 | Loss: 21.0025 | Acc: 0.6221 | Time: 3.60s\n",
            "   >> Val Loss: 20.2833 | Val Acc: 0.6244\n",
            "Epoch 9/300 | Loss: 20.0921 | Acc: 0.6562 | Time: 5.00s\n",
            "   >> Val Loss: 18.7701 | Val Acc: 0.6444\n",
            "Epoch 10/300 | Loss: 16.6188 | Acc: 0.6765 | Time: 3.67s\n",
            "   >> Val Loss: 17.5447 | Val Acc: 0.6639\n",
            "Epoch 11/300 | Loss: 15.9459 | Acc: 0.6905 | Time: 3.59s\n",
            "   >> Val Loss: 16.5881 | Val Acc: 0.6745\n",
            "Epoch 12/300 | Loss: 15.7396 | Acc: 0.7089 | Time: 4.94s\n",
            "   >> Val Loss: 15.7861 | Val Acc: 0.6939\n",
            "Epoch 13/300 | Loss: 14.2736 | Acc: 0.7206 | Time: 3.56s\n",
            "   >> Val Loss: 15.1500 | Val Acc: 0.7042\n",
            "Epoch 14/300 | Loss: 15.0868 | Acc: 0.7363 | Time: 3.59s\n",
            "   >> Val Loss: 14.6073 | Val Acc: 0.7181\n",
            "Epoch 15/300 | Loss: 14.2961 | Acc: 0.7403 | Time: 4.74s\n",
            "   >> Val Loss: 14.2747 | Val Acc: 0.7309\n",
            "Epoch 16/300 | Loss: 12.9552 | Acc: 0.7525 | Time: 3.59s\n",
            "   >> Val Loss: 13.7837 | Val Acc: 0.7385\n",
            "Epoch 17/300 | Loss: 13.3254 | Acc: 0.7567 | Time: 3.61s\n",
            "   >> Val Loss: 13.5931 | Val Acc: 0.7462\n",
            "Epoch 18/300 | Loss: 12.1319 | Acc: 0.7657 | Time: 4.44s\n",
            "   >> Val Loss: 13.1956 | Val Acc: 0.7546\n",
            "Epoch 19/300 | Loss: 12.3601 | Acc: 0.7716 | Time: 4.50s\n",
            "   >> Val Loss: 12.9427 | Val Acc: 0.7582\n",
            "Epoch 20/300 | Loss: 11.8768 | Acc: 0.7778 | Time: 3.64s\n",
            "   >> Val Loss: 12.8124 | Val Acc: 0.7607\n",
            "Epoch 21/300 | Loss: 11.1859 | Acc: 0.7825 | Time: 4.36s\n",
            "   >> Val Loss: 12.6533 | Val Acc: 0.7643\n",
            "Epoch 22/300 | Loss: 10.9365 | Acc: 0.7839 | Time: 3.97s\n",
            "   >> Val Loss: 12.4130 | Val Acc: 0.7677\n",
            "Epoch 23/300 | Loss: 10.7550 | Acc: 0.7906 | Time: 3.57s\n",
            "   >> Val Loss: 12.2483 | Val Acc: 0.7668\n",
            "Epoch 24/300 | Loss: 10.5071 | Acc: 0.7953 | Time: 4.05s\n",
            "   >> Val Loss: 12.1434 | Val Acc: 0.7718\n",
            "Epoch 25/300 | Loss: 10.7688 | Acc: 0.7981 | Time: 4.23s\n",
            "   >> Val Loss: 11.9795 | Val Acc: 0.7785\n",
            "Epoch 26/300 | Loss: 10.7794 | Acc: 0.7999 | Time: 3.99s\n",
            "   >> Val Loss: 11.9469 | Val Acc: 0.7746\n",
            "Epoch 27/300 | Loss: 11.0121 | Acc: 0.8020 | Time: 3.91s\n",
            "   >> Val Loss: 11.7872 | Val Acc: 0.7741\n",
            "Epoch 28/300 | Loss: 10.0033 | Acc: 0.8031 | Time: 4.73s\n",
            "   >> Val Loss: 11.7759 | Val Acc: 0.7780\n",
            "Epoch 29/300 | Loss: 10.4511 | Acc: 0.8067 | Time: 3.58s\n",
            "   >> Val Loss: 11.6008 | Val Acc: 0.7785\n",
            "Epoch 30/300 | Loss: 10.5035 | Acc: 0.8104 | Time: 3.80s\n",
            "   >> Val Loss: 11.5163 | Val Acc: 0.7799\n",
            "Epoch 31/300 | Loss: 9.5285 | Acc: 0.8125 | Time: 4.43s\n",
            "   >> Val Loss: 11.4332 | Val Acc: 0.7824\n",
            "Epoch 32/300 | Loss: 9.7200 | Acc: 0.8138 | Time: 3.61s\n",
            "   >> Val Loss: 11.3397 | Val Acc: 0.7838\n",
            "Epoch 33/300 | Loss: 9.2299 | Acc: 0.8146 | Time: 3.62s\n",
            "   >> Val Loss: 11.2538 | Val Acc: 0.7869\n",
            "Epoch 34/300 | Loss: 10.3990 | Acc: 0.8215 | Time: 4.77s\n",
            "   >> Val Loss: 11.2605 | Val Acc: 0.7807\n",
            "Epoch 35/300 | Loss: 9.3289 | Acc: 0.8156 | Time: 3.60s\n",
            "   >> Val Loss: 11.1665 | Val Acc: 0.7863\n",
            "Epoch 36/300 | Loss: 9.7654 | Acc: 0.8215 | Time: 3.60s\n",
            "   >> Val Loss: 11.0798 | Val Acc: 0.7880\n",
            "Epoch 37/300 | Loss: 9.0772 | Acc: 0.8238 | Time: 4.86s\n",
            "   >> Val Loss: 11.0102 | Val Acc: 0.7869\n",
            "Epoch 38/300 | Loss: 9.4364 | Acc: 0.8290 | Time: 3.58s\n",
            "   >> Val Loss: 10.9854 | Val Acc: 0.7899\n",
            "Epoch 39/300 | Loss: 8.9322 | Acc: 0.8285 | Time: 3.60s\n",
            "   >> Val Loss: 10.9463 | Val Acc: 0.7877\n",
            "Epoch 40/300 | Loss: 10.1105 | Acc: 0.8303 | Time: 4.85s\n",
            "   >> Val Loss: 10.9918 | Val Acc: 0.7860\n",
            "Epoch 41/300 | Loss: 8.8639 | Acc: 0.8293 | Time: 3.55s\n",
            "   >> Val Loss: 10.8908 | Val Acc: 0.7919\n",
            "Epoch 42/300 | Loss: 8.4388 | Acc: 0.8338 | Time: 3.62s\n",
            "   >> Val Loss: 10.9300 | Val Acc: 0.7858\n",
            "Epoch 43/300 | Loss: 8.2884 | Acc: 0.8363 | Time: 5.51s\n",
            "   >> Val Loss: 10.8091 | Val Acc: 0.7896\n",
            "Epoch 44/300 | Loss: 9.3388 | Acc: 0.8403 | Time: 3.64s\n",
            "   >> Val Loss: 10.7497 | Val Acc: 0.7924\n",
            "Epoch 45/300 | Loss: 8.1237 | Acc: 0.8390 | Time: 3.69s\n",
            "   >> Val Loss: 10.7788 | Val Acc: 0.7947\n",
            "Epoch 46/300 | Loss: 8.4138 | Acc: 0.8416 | Time: 4.59s\n",
            "   >> Val Loss: 10.6934 | Val Acc: 0.7938\n",
            "Epoch 47/300 | Loss: 8.2312 | Acc: 0.8456 | Time: 3.67s\n",
            "   >> Val Loss: 10.6755 | Val Acc: 0.7949\n",
            "Epoch 48/300 | Loss: 7.8771 | Acc: 0.8432 | Time: 3.56s\n",
            "   >> Val Loss: 10.6652 | Val Acc: 0.7947\n",
            "Epoch 49/300 | Loss: 7.7167 | Acc: 0.8491 | Time: 4.34s\n",
            "   >> Val Loss: 10.6020 | Val Acc: 0.7944\n",
            "Epoch 50/300 | Loss: 7.5990 | Acc: 0.8518 | Time: 4.03s\n",
            "   >> Val Loss: 10.5942 | Val Acc: 0.7955\n",
            "Epoch 51/300 | Loss: 7.7642 | Acc: 0.8528 | Time: 3.65s\n",
            "   >> Val Loss: 10.5546 | Val Acc: 0.7960\n",
            "Epoch 52/300 | Loss: 7.4723 | Acc: 0.8531 | Time: 4.12s\n",
            "   >> Val Loss: 10.5680 | Val Acc: 0.7974\n",
            "Epoch 53/300 | Loss: 7.3223 | Acc: 0.8551 | Time: 4.24s\n",
            "   >> Val Loss: 10.5302 | Val Acc: 0.7969\n",
            "Epoch 54/300 | Loss: 7.3186 | Acc: 0.8584 | Time: 3.57s\n",
            "   >> Val Loss: 10.5516 | Val Acc: 0.7969\n",
            "Epoch 55/300 | Loss: 7.7075 | Acc: 0.8597 | Time: 3.79s\n",
            "   >> Val Loss: 10.4746 | Val Acc: 0.7977\n",
            "Epoch 56/300 | Loss: 7.1499 | Acc: 0.8548 | Time: 4.55s\n",
            "   >> Val Loss: 10.5386 | Val Acc: 0.7958\n",
            "Epoch 57/300 | Loss: 7.5415 | Acc: 0.8603 | Time: 3.57s\n",
            "   >> Val Loss: 10.5598 | Val Acc: 0.7938\n",
            "Epoch 58/300 | Loss: 6.9417 | Acc: 0.8628 | Time: 3.63s\n",
            "   >> Val Loss: 10.4766 | Val Acc: 0.8019\n",
            "Epoch 59/300 | Loss: 6.9255 | Acc: 0.8655 | Time: 4.91s\n",
            "   >> Val Loss: 10.4482 | Val Acc: 0.8013\n",
            "Epoch 60/300 | Loss: 7.9564 | Acc: 0.8685 | Time: 3.62s\n",
            "   >> Val Loss: 10.5024 | Val Acc: 0.7952\n",
            "Epoch 61/300 | Loss: 7.6740 | Acc: 0.8564 | Time: 3.55s\n",
            "   >> Val Loss: 10.7168 | Val Acc: 0.7922\n",
            "Epoch 62/300 | Loss: 6.9259 | Acc: 0.8669 | Time: 4.91s\n",
            "   >> Val Loss: 10.6467 | Val Acc: 0.7930\n",
            "Epoch 63/300 | Loss: 6.9411 | Acc: 0.8691 | Time: 3.56s\n",
            "   >> Val Loss: 10.4933 | Val Acc: 0.7977\n",
            "Epoch 64/300 | Loss: 6.5251 | Acc: 0.8719 | Time: 3.56s\n",
            "   >> Val Loss: 10.4697 | Val Acc: 0.7999\n",
            "Early stopping at epoch 64\n",
            "\n",
            "Total Training Time: 293.34s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantum-Classical Model Training Loop\n",
        "\n",
        "patience = 5\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs_no_improve = 0\n",
        "\n",
        "total_start = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_hybrid.train()\n",
        "    epoch_start = time.time()\n",
        "    correct, total, epoch_loss = 0, 0, 0\n",
        "\n",
        "    for X, y in train_loader:\n",
        "        mask = (y != -100)\n",
        "        optimizer_hybrid.zero_grad()\n",
        "\n",
        "        y_clamped = y.clone()\n",
        "        y_clamped[y_clamped == -100] = 0\n",
        "        loss = model_hybrid(X, tags=y_clamped, mask=mask)\n",
        "        loss.backward()\n",
        "        optimizer_hybrid.step()\n",
        "        epoch_loss += loss.item()\n",
        "        preds = model_hybrid(X, mask=mask)\n",
        "\n",
        "        for p_seq, y_seq, m_seq in zip(preds, y, mask):\n",
        "            for p, gold, m in zip(p_seq, y_seq, m_seq):\n",
        "                if m:\n",
        "                    correct += (p == gold.item())\n",
        "                    total += 1\n",
        "\n",
        "    acc = correct / total if total > 0 else 0\n",
        "    elapsed = time.time() - epoch_start\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss/len(train_loader):.4f} | \"\n",
        "          f\"Acc: {acc:.4f} | Time: {elapsed:.2f}s\")\n",
        "\n",
        "    # ---------------- Validation ----------------\n",
        "    model_hybrid.eval()\n",
        "    val_correct, val_total, val_loss = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            mask = (y != -100)\n",
        "            y_clamped = y.clone()\n",
        "            y_clamped[y_clamped == -100] = 0\n",
        "\n",
        "            loss = model_hybrid(X, tags=y_clamped, mask=mask)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = model_hybrid(X, mask=mask)\n",
        "            for p_seq, y_seq, m_seq in zip(preds, y, mask):\n",
        "                for p, gold, m in zip(p_seq, y_seq, m_seq):\n",
        "                    if m:\n",
        "                        val_correct += (p == gold.item())\n",
        "                        val_total += 1\n",
        "\n",
        "    val_acc = val_correct / val_total if val_total > 0 else 0\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"   >> Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "    model_hybrid.train()\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model_hybrid.state_dict(),\n",
        "                   \"/content/drive/MyDrive/QML-Research/Model Saves/Quantum/POS_T/basicx2.pt\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nTotal Training Time: {time.time() - total_start:.2f}s\")"
      ],
      "metadata": {
        "id": "Yd4R8xL5-p78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb2442d-26ee-413e-ad27-86937fe0ad1f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300 | Loss: 39.4240 | Acc: 0.0605 | Time: 170.93s\n",
            "   >> Val Loss: 40.6804 | Val Acc: 0.0673\n",
            "Epoch 2/300 | Loss: 43.5487 | Acc: 0.0710 | Time: 172.06s\n",
            "   >> Val Loss: 39.5911 | Val Acc: 0.1169\n",
            "Epoch 3/300 | Loss: 37.2005 | Acc: 0.2807 | Time: 169.10s\n",
            "   >> Val Loss: 38.4639 | Val Acc: 0.3186\n",
            "Epoch 4/300 | Loss: 40.5042 | Acc: 0.3446 | Time: 174.40s\n",
            "   >> Val Loss: 37.4772 | Val Acc: 0.3152\n",
            "Epoch 5/300 | Loss: 35.6136 | Acc: 0.3337 | Time: 175.35s\n",
            "   >> Val Loss: 36.5986 | Val Acc: 0.3180\n",
            "Epoch 6/300 | Loss: 34.2893 | Acc: 0.3346 | Time: 175.44s\n",
            "   >> Val Loss: 35.8151 | Val Acc: 0.3203\n",
            "Epoch 7/300 | Loss: 34.2768 | Acc: 0.3371 | Time: 174.26s\n",
            "   >> Val Loss: 35.0815 | Val Acc: 0.3261\n",
            "Epoch 8/300 | Loss: 33.1255 | Acc: 0.3397 | Time: 173.47s\n",
            "   >> Val Loss: 34.3196 | Val Acc: 0.3322\n",
            "Epoch 9/300 | Loss: 32.2349 | Acc: 0.3531 | Time: 174.77s\n",
            "   >> Val Loss: 33.5885 | Val Acc: 0.3395\n",
            "Epoch 10/300 | Loss: 33.3452 | Acc: 0.3730 | Time: 173.35s\n",
            "   >> Val Loss: 32.7975 | Val Acc: 0.3673\n",
            "Epoch 11/300 | Loss: 30.4989 | Acc: 0.3821 | Time: 184.23s\n",
            "   >> Val Loss: 32.0724 | Val Acc: 0.3637\n",
            "Epoch 12/300 | Loss: 31.4286 | Acc: 0.3891 | Time: 181.42s\n",
            "   >> Val Loss: 31.3915 | Val Acc: 0.3776\n",
            "Epoch 13/300 | Loss: 30.6153 | Acc: 0.4014 | Time: 174.44s\n",
            "   >> Val Loss: 30.7692 | Val Acc: 0.3795\n",
            "Epoch 14/300 | Loss: 28.5248 | Acc: 0.4061 | Time: 166.92s\n",
            "   >> Val Loss: 30.1428 | Val Acc: 0.3815\n",
            "Epoch 15/300 | Loss: 29.0346 | Acc: 0.4156 | Time: 168.21s\n",
            "   >> Val Loss: 29.5761 | Val Acc: 0.3973\n",
            "Epoch 16/300 | Loss: 27.8719 | Acc: 0.4314 | Time: 176.75s\n",
            "   >> Val Loss: 28.9925 | Val Acc: 0.4160\n",
            "Epoch 17/300 | Loss: 27.8569 | Acc: 0.4515 | Time: 171.16s\n",
            "   >> Val Loss: 28.4157 | Val Acc: 0.4341\n",
            "Epoch 18/300 | Loss: 26.9289 | Acc: 0.4733 | Time: 160.80s\n",
            "   >> Val Loss: 27.8827 | Val Acc: 0.4572\n",
            "Epoch 19/300 | Loss: 25.8581 | Acc: 0.4868 | Time: 176.90s\n",
            "   >> Val Loss: 27.3729 | Val Acc: 0.4672\n",
            "Epoch 20/300 | Loss: 25.2012 | Acc: 0.5039 | Time: 175.31s\n",
            "   >> Val Loss: 26.8819 | Val Acc: 0.4866\n",
            "Epoch 21/300 | Loss: 24.8465 | Acc: 0.5211 | Time: 166.82s\n",
            "   >> Val Loss: 26.3705 | Val Acc: 0.5019\n",
            "Epoch 22/300 | Loss: 24.2312 | Acc: 0.5353 | Time: 176.12s\n",
            "   >> Val Loss: 25.9227 | Val Acc: 0.5206\n",
            "Epoch 23/300 | Loss: 23.7506 | Acc: 0.5552 | Time: 178.95s\n",
            "   >> Val Loss: 25.4713 | Val Acc: 0.5367\n",
            "Epoch 24/300 | Loss: 23.6700 | Acc: 0.5729 | Time: 179.88s\n",
            "   >> Val Loss: 25.1074 | Val Acc: 0.5490\n",
            "Epoch 25/300 | Loss: 24.4939 | Acc: 0.5814 | Time: 179.16s\n",
            "   >> Val Loss: 24.6740 | Val Acc: 0.5568\n",
            "Epoch 26/300 | Loss: 23.4669 | Acc: 0.5898 | Time: 174.12s\n",
            "   >> Val Loss: 24.3270 | Val Acc: 0.5587\n",
            "Epoch 27/300 | Loss: 22.4095 | Acc: 0.5922 | Time: 172.88s\n",
            "   >> Val Loss: 23.9758 | Val Acc: 0.5648\n",
            "Epoch 28/300 | Loss: 24.8311 | Acc: 0.5999 | Time: 169.50s\n",
            "   >> Val Loss: 23.6843 | Val Acc: 0.5698\n",
            "Epoch 29/300 | Loss: 22.2840 | Acc: 0.6052 | Time: 171.71s\n",
            "   >> Val Loss: 23.3299 | Val Acc: 0.5682\n",
            "Epoch 30/300 | Loss: 21.3110 | Acc: 0.6047 | Time: 169.85s\n",
            "   >> Val Loss: 23.0184 | Val Acc: 0.5757\n",
            "Epoch 31/300 | Loss: 20.8567 | Acc: 0.6086 | Time: 170.39s\n",
            "   >> Val Loss: 22.7555 | Val Acc: 0.5729\n",
            "Epoch 32/300 | Loss: 20.9053 | Acc: 0.6116 | Time: 167.06s\n",
            "   >> Val Loss: 22.4639 | Val Acc: 0.5762\n",
            "Epoch 33/300 | Loss: 21.9874 | Acc: 0.6117 | Time: 163.51s\n",
            "   >> Val Loss: 22.2720 | Val Acc: 0.5785\n",
            "Epoch 34/300 | Loss: 20.4051 | Acc: 0.6115 | Time: 173.41s\n",
            "   >> Val Loss: 22.0513 | Val Acc: 0.5787\n",
            "Epoch 35/300 | Loss: 20.5430 | Acc: 0.6149 | Time: 176.59s\n",
            "   >> Val Loss: 21.8435 | Val Acc: 0.5821\n",
            "Epoch 36/300 | Loss: 20.2203 | Acc: 0.6173 | Time: 189.05s\n",
            "   >> Val Loss: 21.5563 | Val Acc: 0.5868\n",
            "Epoch 37/300 | Loss: 19.3832 | Acc: 0.6210 | Time: 168.27s\n",
            "   >> Val Loss: 21.3621 | Val Acc: 0.5871\n",
            "Epoch 38/300 | Loss: 19.4686 | Acc: 0.6223 | Time: 169.17s\n",
            "   >> Val Loss: 21.1828 | Val Acc: 0.5851\n",
            "Epoch 39/300 | Loss: 19.5500 | Acc: 0.6257 | Time: 163.84s\n",
            "   >> Val Loss: 20.9634 | Val Acc: 0.5924\n",
            "Epoch 40/300 | Loss: 19.4144 | Acc: 0.6275 | Time: 162.43s\n",
            "   >> Val Loss: 20.7926 | Val Acc: 0.5960\n",
            "Epoch 41/300 | Loss: 19.1807 | Acc: 0.6291 | Time: 161.96s\n",
            "   >> Val Loss: 20.6008 | Val Acc: 0.5954\n",
            "Epoch 42/300 | Loss: 18.6038 | Acc: 0.6319 | Time: 159.41s\n",
            "   >> Val Loss: 20.4123 | Val Acc: 0.6013\n",
            "Epoch 43/300 | Loss: 19.2764 | Acc: 0.6370 | Time: 162.06s\n",
            "   >> Val Loss: 20.2385 | Val Acc: 0.6010\n",
            "Epoch 44/300 | Loss: 18.2962 | Acc: 0.6413 | Time: 161.11s\n",
            "   >> Val Loss: 20.0495 | Val Acc: 0.6063\n",
            "Epoch 45/300 | Loss: 18.3209 | Acc: 0.6451 | Time: 161.06s\n",
            "   >> Val Loss: 19.9273 | Val Acc: 0.6138\n",
            "Epoch 46/300 | Loss: 17.9321 | Acc: 0.6511 | Time: 160.29s\n",
            "   >> Val Loss: 19.7307 | Val Acc: 0.6149\n",
            "Epoch 47/300 | Loss: 19.5106 | Acc: 0.6564 | Time: 161.46s\n",
            "   >> Val Loss: 19.5826 | Val Acc: 0.6180\n",
            "Epoch 48/300 | Loss: 20.4452 | Acc: 0.6570 | Time: 160.31s\n",
            "   >> Val Loss: 19.3818 | Val Acc: 0.6249\n",
            "Epoch 49/300 | Loss: 17.0218 | Acc: 0.6667 | Time: 164.46s\n",
            "   >> Val Loss: 19.2461 | Val Acc: 0.6324\n",
            "Epoch 50/300 | Loss: 17.2161 | Acc: 0.6708 | Time: 170.34s\n",
            "   >> Val Loss: 19.0484 | Val Acc: 0.6322\n",
            "Epoch 51/300 | Loss: 16.5572 | Acc: 0.6742 | Time: 174.53s\n",
            "   >> Val Loss: 18.9107 | Val Acc: 0.6397\n",
            "Epoch 52/300 | Loss: 17.2406 | Acc: 0.6830 | Time: 167.16s\n",
            "   >> Val Loss: 18.7812 | Val Acc: 0.6436\n",
            "Epoch 53/300 | Loss: 16.7075 | Acc: 0.6839 | Time: 168.52s\n",
            "   >> Val Loss: 18.6012 | Val Acc: 0.6491\n",
            "Epoch 54/300 | Loss: 16.7486 | Acc: 0.6898 | Time: 164.93s\n",
            "   >> Val Loss: 18.4892 | Val Acc: 0.6519\n",
            "Epoch 55/300 | Loss: 16.4229 | Acc: 0.6905 | Time: 169.79s\n",
            "   >> Val Loss: 18.3575 | Val Acc: 0.6586\n",
            "Epoch 56/300 | Loss: 15.7177 | Acc: 0.6963 | Time: 169.23s\n",
            "   >> Val Loss: 18.2052 | Val Acc: 0.6619\n",
            "Epoch 57/300 | Loss: 16.0284 | Acc: 0.6997 | Time: 166.69s\n",
            "   >> Val Loss: 18.0566 | Val Acc: 0.6622\n",
            "Epoch 58/300 | Loss: 15.5321 | Acc: 0.7039 | Time: 165.91s\n",
            "   >> Val Loss: 17.9112 | Val Acc: 0.6708\n",
            "Epoch 59/300 | Loss: 15.6084 | Acc: 0.7096 | Time: 166.67s\n",
            "   >> Val Loss: 17.7977 | Val Acc: 0.6750\n",
            "Epoch 60/300 | Loss: 15.6341 | Acc: 0.7157 | Time: 167.91s\n",
            "   >> Val Loss: 17.6776 | Val Acc: 0.6770\n",
            "Epoch 61/300 | Loss: 15.0591 | Acc: 0.7156 | Time: 167.68s\n",
            "   >> Val Loss: 17.5263 | Val Acc: 0.6825\n",
            "Epoch 62/300 | Loss: 15.0672 | Acc: 0.7213 | Time: 161.33s\n",
            "   >> Val Loss: 17.4541 | Val Acc: 0.6864\n",
            "Epoch 63/300 | Loss: 15.6883 | Acc: 0.7234 | Time: 155.46s\n",
            "   >> Val Loss: 17.3034 | Val Acc: 0.6884\n",
            "Epoch 64/300 | Loss: 16.1547 | Acc: 0.7270 | Time: 159.48s\n",
            "   >> Val Loss: 17.3475 | Val Acc: 0.6859\n",
            "Epoch 65/300 | Loss: 16.5156 | Acc: 0.7205 | Time: 198.26s\n",
            "   >> Val Loss: 17.2649 | Val Acc: 0.6881\n",
            "Epoch 66/300 | Loss: 14.6650 | Acc: 0.7298 | Time: 170.02s\n",
            "   >> Val Loss: 17.0226 | Val Acc: 0.6912\n",
            "Epoch 67/300 | Loss: 14.2056 | Acc: 0.7327 | Time: 162.30s\n",
            "   >> Val Loss: 16.8709 | Val Acc: 0.6981\n",
            "Epoch 68/300 | Loss: 15.1515 | Acc: 0.7368 | Time: 153.82s\n",
            "   >> Val Loss: 16.7855 | Val Acc: 0.6967\n",
            "Epoch 69/300 | Loss: 15.0122 | Acc: 0.7383 | Time: 153.24s\n",
            "   >> Val Loss: 16.6445 | Val Acc: 0.7012\n",
            "Epoch 70/300 | Loss: 14.0252 | Acc: 0.7425 | Time: 152.55s\n",
            "   >> Val Loss: 16.5961 | Val Acc: 0.7028\n",
            "Epoch 71/300 | Loss: 14.3355 | Acc: 0.7457 | Time: 154.47s\n",
            "   >> Val Loss: 16.4687 | Val Acc: 0.7014\n",
            "Epoch 72/300 | Loss: 14.0953 | Acc: 0.7481 | Time: 156.20s\n",
            "   >> Val Loss: 16.4097 | Val Acc: 0.7034\n",
            "Epoch 73/300 | Loss: 13.4960 | Acc: 0.7512 | Time: 166.34s\n",
            "   >> Val Loss: 16.3207 | Val Acc: 0.7040\n",
            "Epoch 74/300 | Loss: 13.4644 | Acc: 0.7525 | Time: 168.80s\n",
            "   >> Val Loss: 16.2075 | Val Acc: 0.7084\n",
            "Epoch 75/300 | Loss: 13.1802 | Acc: 0.7581 | Time: 155.94s\n",
            "   >> Val Loss: 16.1253 | Val Acc: 0.7070\n",
            "Epoch 76/300 | Loss: 15.1258 | Acc: 0.7590 | Time: 167.59s\n",
            "   >> Val Loss: 16.2022 | Val Acc: 0.7048\n",
            "Epoch 77/300 | Loss: 13.3713 | Acc: 0.7513 | Time: 161.99s\n",
            "   >> Val Loss: 16.2316 | Val Acc: 0.7051\n",
            "Epoch 78/300 | Loss: 13.8408 | Acc: 0.7591 | Time: 163.86s\n",
            "   >> Val Loss: 15.9162 | Val Acc: 0.7123\n",
            "Epoch 79/300 | Loss: 12.8304 | Acc: 0.7643 | Time: 174.28s\n",
            "   >> Val Loss: 15.8464 | Val Acc: 0.7145\n",
            "Epoch 80/300 | Loss: 12.8617 | Acc: 0.7670 | Time: 173.85s\n",
            "   >> Val Loss: 15.7623 | Val Acc: 0.7156\n",
            "Epoch 81/300 | Loss: 13.1821 | Acc: 0.7694 | Time: 188.86s\n",
            "   >> Val Loss: 15.7098 | Val Acc: 0.7120\n",
            "Epoch 82/300 | Loss: 13.6443 | Acc: 0.7688 | Time: 186.12s\n",
            "   >> Val Loss: 15.6916 | Val Acc: 0.7104\n",
            "Epoch 83/300 | Loss: 12.4336 | Acc: 0.7696 | Time: 169.77s\n",
            "   >> Val Loss: 15.6131 | Val Acc: 0.7137\n",
            "Epoch 84/300 | Loss: 13.5951 | Acc: 0.7720 | Time: 169.29s\n",
            "   >> Val Loss: 15.5071 | Val Acc: 0.7176\n",
            "Epoch 85/300 | Loss: 12.3813 | Acc: 0.7749 | Time: 163.97s\n",
            "   >> Val Loss: 15.4419 | Val Acc: 0.7168\n",
            "Epoch 86/300 | Loss: 12.8686 | Acc: 0.7781 | Time: 164.99s\n",
            "   >> Val Loss: 15.4455 | Val Acc: 0.7162\n",
            "Epoch 87/300 | Loss: 12.0646 | Acc: 0.7799 | Time: 167.93s\n",
            "   >> Val Loss: 15.3733 | Val Acc: 0.7140\n",
            "Epoch 88/300 | Loss: 11.9096 | Acc: 0.7818 | Time: 162.94s\n",
            "   >> Val Loss: 15.2739 | Val Acc: 0.7204\n",
            "Epoch 89/300 | Loss: 12.0575 | Acc: 0.7836 | Time: 164.93s\n",
            "   >> Val Loss: 15.2586 | Val Acc: 0.7206\n",
            "Epoch 90/300 | Loss: 11.7715 | Acc: 0.7856 | Time: 164.63s\n",
            "   >> Val Loss: 15.1759 | Val Acc: 0.7212\n",
            "Epoch 91/300 | Loss: 13.5306 | Acc: 0.7883 | Time: 157.65s\n",
            "   >> Val Loss: 15.1750 | Val Acc: 0.7195\n",
            "Epoch 92/300 | Loss: 11.6988 | Acc: 0.7900 | Time: 162.27s\n",
            "   >> Val Loss: 15.2026 | Val Acc: 0.7223\n",
            "Epoch 93/300 | Loss: 11.6124 | Acc: 0.7918 | Time: 168.40s\n",
            "   >> Val Loss: 15.0964 | Val Acc: 0.7195\n",
            "Epoch 94/300 | Loss: 11.9709 | Acc: 0.7936 | Time: 168.89s\n",
            "   >> Val Loss: 15.0653 | Val Acc: 0.7204\n",
            "Epoch 95/300 | Loss: 11.7689 | Acc: 0.7957 | Time: 168.26s\n",
            "   >> Val Loss: 15.0299 | Val Acc: 0.7201\n",
            "Epoch 96/300 | Loss: 11.2969 | Acc: 0.7959 | Time: 165.26s\n",
            "   >> Val Loss: 14.9456 | Val Acc: 0.7243\n",
            "Epoch 97/300 | Loss: 11.2601 | Acc: 0.7991 | Time: 164.14s\n",
            "   >> Val Loss: 14.9195 | Val Acc: 0.7245\n",
            "Epoch 98/300 | Loss: 12.5884 | Acc: 0.8020 | Time: 165.29s\n",
            "   >> Val Loss: 14.9055 | Val Acc: 0.7265\n",
            "Epoch 99/300 | Loss: 11.5857 | Acc: 0.7978 | Time: 163.98s\n",
            "   >> Val Loss: 14.8378 | Val Acc: 0.7268\n",
            "Epoch 100/300 | Loss: 11.3611 | Acc: 0.8022 | Time: 159.44s\n",
            "   >> Val Loss: 14.7551 | Val Acc: 0.7270\n",
            "Epoch 101/300 | Loss: 11.9869 | Acc: 0.8047 | Time: 160.21s\n",
            "   >> Val Loss: 14.8261 | Val Acc: 0.7257\n",
            "Epoch 102/300 | Loss: 11.5539 | Acc: 0.8006 | Time: 168.17s\n",
            "   >> Val Loss: 14.7184 | Val Acc: 0.7293\n",
            "Epoch 103/300 | Loss: 11.9327 | Acc: 0.8033 | Time: 171.38s\n",
            "   >> Val Loss: 14.7236 | Val Acc: 0.7293\n",
            "Epoch 104/300 | Loss: 12.5112 | Acc: 0.8065 | Time: 171.13s\n",
            "   >> Val Loss: 14.6416 | Val Acc: 0.7243\n",
            "Epoch 105/300 | Loss: 10.6021 | Acc: 0.8060 | Time: 162.96s\n",
            "   >> Val Loss: 14.6000 | Val Acc: 0.7273\n",
            "Epoch 106/300 | Loss: 10.6466 | Acc: 0.8116 | Time: 163.85s\n",
            "   >> Val Loss: 14.5618 | Val Acc: 0.7295\n",
            "Epoch 107/300 | Loss: 10.5260 | Acc: 0.8137 | Time: 164.84s\n",
            "   >> Val Loss: 14.5558 | Val Acc: 0.7309\n",
            "Epoch 108/300 | Loss: 10.9673 | Acc: 0.8128 | Time: 162.19s\n",
            "   >> Val Loss: 14.5095 | Val Acc: 0.7315\n",
            "Epoch 109/300 | Loss: 10.2576 | Acc: 0.8157 | Time: 164.11s\n",
            "   >> Val Loss: 14.4518 | Val Acc: 0.7321\n",
            "Epoch 110/300 | Loss: 10.6950 | Acc: 0.8184 | Time: 173.36s\n",
            "   >> Val Loss: 14.4596 | Val Acc: 0.7340\n",
            "Epoch 111/300 | Loss: 10.1414 | Acc: 0.8159 | Time: 178.56s\n",
            "   >> Val Loss: 14.4357 | Val Acc: 0.7309\n",
            "Epoch 112/300 | Loss: 10.3872 | Acc: 0.8184 | Time: 179.07s\n",
            "   >> Val Loss: 14.5195 | Val Acc: 0.7321\n",
            "Epoch 113/300 | Loss: 10.4191 | Acc: 0.8191 | Time: 167.04s\n",
            "   >> Val Loss: 14.3725 | Val Acc: 0.7337\n",
            "Epoch 114/300 | Loss: 9.9492 | Acc: 0.8177 | Time: 167.24s\n",
            "   >> Val Loss: 14.3601 | Val Acc: 0.7343\n",
            "Epoch 115/300 | Loss: 11.5560 | Acc: 0.8222 | Time: 162.00s\n",
            "   >> Val Loss: 14.4024 | Val Acc: 0.7298\n",
            "Epoch 116/300 | Loss: 10.2306 | Acc: 0.8197 | Time: 156.38s\n",
            "   >> Val Loss: 14.4002 | Val Acc: 0.7321\n",
            "Epoch 117/300 | Loss: 9.9004 | Acc: 0.8241 | Time: 157.49s\n",
            "   >> Val Loss: 14.4202 | Val Acc: 0.7351\n",
            "Epoch 118/300 | Loss: 10.0499 | Acc: 0.8256 | Time: 160.31s\n",
            "   >> Val Loss: 14.3246 | Val Acc: 0.7323\n",
            "Epoch 119/300 | Loss: 10.0342 | Acc: 0.8283 | Time: 161.35s\n",
            "   >> Val Loss: 14.3426 | Val Acc: 0.7298\n",
            "Epoch 120/300 | Loss: 10.1769 | Acc: 0.8282 | Time: 159.10s\n",
            "   >> Val Loss: 14.3013 | Val Acc: 0.7323\n",
            "Epoch 121/300 | Loss: 10.1851 | Acc: 0.8286 | Time: 158.90s\n",
            "   >> Val Loss: 14.3241 | Val Acc: 0.7359\n",
            "Epoch 122/300 | Loss: 9.8691 | Acc: 0.8249 | Time: 158.18s\n",
            "   >> Val Loss: 14.3450 | Val Acc: 0.7362\n",
            "Epoch 123/300 | Loss: 9.8700 | Acc: 0.8321 | Time: 158.66s\n",
            "   >> Val Loss: 14.2700 | Val Acc: 0.7379\n",
            "Epoch 124/300 | Loss: 9.4119 | Acc: 0.8333 | Time: 158.34s\n",
            "   >> Val Loss: 14.1836 | Val Acc: 0.7348\n",
            "Epoch 125/300 | Loss: 9.4042 | Acc: 0.8347 | Time: 157.49s\n",
            "   >> Val Loss: 14.1843 | Val Acc: 0.7348\n",
            "Epoch 126/300 | Loss: 9.3101 | Acc: 0.8362 | Time: 163.17s\n",
            "   >> Val Loss: 14.1907 | Val Acc: 0.7329\n",
            "Epoch 127/300 | Loss: 9.6182 | Acc: 0.8373 | Time: 164.22s\n",
            "   >> Val Loss: 14.1325 | Val Acc: 0.7346\n",
            "Epoch 128/300 | Loss: 9.2977 | Acc: 0.8370 | Time: 164.78s\n",
            "   >> Val Loss: 14.2435 | Val Acc: 0.7359\n",
            "Epoch 129/300 | Loss: 9.9489 | Acc: 0.8385 | Time: 167.37s\n",
            "   >> Val Loss: 14.1508 | Val Acc: 0.7371\n",
            "Epoch 130/300 | Loss: 9.1075 | Acc: 0.8367 | Time: 160.56s\n",
            "   >> Val Loss: 14.2193 | Val Acc: 0.7418\n",
            "Epoch 131/300 | Loss: 8.8699 | Acc: 0.8397 | Time: 173.57s\n",
            "   >> Val Loss: 14.1228 | Val Acc: 0.7398\n",
            "Epoch 132/300 | Loss: 9.3698 | Acc: 0.8413 | Time: 177.97s\n",
            "   >> Val Loss: 14.1329 | Val Acc: 0.7387\n",
            "Epoch 133/300 | Loss: 8.8538 | Acc: 0.8407 | Time: 174.93s\n",
            "   >> Val Loss: 14.1090 | Val Acc: 0.7385\n",
            "Epoch 134/300 | Loss: 8.7581 | Acc: 0.8419 | Time: 173.18s\n",
            "   >> Val Loss: 14.1548 | Val Acc: 0.7371\n",
            "Epoch 135/300 | Loss: 9.0978 | Acc: 0.8439 | Time: 200.60s\n",
            "   >> Val Loss: 14.1022 | Val Acc: 0.7432\n",
            "Epoch 136/300 | Loss: 8.7843 | Acc: 0.8445 | Time: 170.41s\n",
            "   >> Val Loss: 14.1004 | Val Acc: 0.7412\n",
            "Epoch 137/300 | Loss: 8.9486 | Acc: 0.8452 | Time: 174.46s\n",
            "   >> Val Loss: 14.1283 | Val Acc: 0.7390\n",
            "Epoch 138/300 | Loss: 8.6006 | Acc: 0.8423 | Time: 169.00s\n",
            "   >> Val Loss: 14.1814 | Val Acc: 0.7418\n",
            "Epoch 139/300 | Loss: 8.4257 | Acc: 0.8461 | Time: 162.31s\n",
            "   >> Val Loss: 14.0970 | Val Acc: 0.7443\n",
            "Epoch 140/300 | Loss: 8.5242 | Acc: 0.8476 | Time: 168.98s\n",
            "   >> Val Loss: 14.0750 | Val Acc: 0.7410\n",
            "Epoch 141/300 | Loss: 8.5873 | Acc: 0.8495 | Time: 165.05s\n",
            "   >> Val Loss: 14.1219 | Val Acc: 0.7423\n",
            "Epoch 142/300 | Loss: 8.3670 | Acc: 0.8443 | Time: 167.74s\n",
            "   >> Val Loss: 14.1205 | Val Acc: 0.7385\n",
            "Epoch 143/300 | Loss: 8.2103 | Acc: 0.8491 | Time: 162.99s\n",
            "   >> Val Loss: 14.0970 | Val Acc: 0.7421\n",
            "Epoch 144/300 | Loss: 8.4909 | Acc: 0.8512 | Time: 167.06s\n",
            "   >> Val Loss: 14.0701 | Val Acc: 0.7437\n",
            "Epoch 145/300 | Loss: 9.2948 | Acc: 0.8514 | Time: 163.85s\n",
            "   >> Val Loss: 14.1485 | Val Acc: 0.7423\n",
            "Epoch 146/300 | Loss: 9.4513 | Acc: 0.8494 | Time: 167.63s\n",
            "   >> Val Loss: 14.1052 | Val Acc: 0.7418\n",
            "Epoch 147/300 | Loss: 9.3110 | Acc: 0.8505 | Time: 169.84s\n",
            "   >> Val Loss: 14.2557 | Val Acc: 0.7376\n",
            "Epoch 148/300 | Loss: 8.8461 | Acc: 0.8515 | Time: 164.00s\n",
            "   >> Val Loss: 14.2042 | Val Acc: 0.7368\n",
            "Epoch 149/300 | Loss: 8.1051 | Acc: 0.8529 | Time: 165.96s\n",
            "   >> Val Loss: 14.1476 | Val Acc: 0.7387\n",
            "Early stopping at epoch 149\n",
            "\n",
            "Total Training Time: 27889.98s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**"
      ],
      "metadata": {
        "id": "Ofu_mBn2W5AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_hybrid.load_state_dict(torch.load(\"/content/drive/MyDrive/QML-Research/Model Saves/Quantum/POS_T/fb-hi.pt\"))\n",
        "model_classical.load_state_dict(torch.load(\"/content/drive/MyDrive/QML-Research/Model Saves/Classical/POS_T/fb-hi.pt\"))"
      ],
      "metadata": {
        "id": "7afbs-bvfUA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f26013-d106-4857-e218-daacb087d67b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_classical.eval()\n",
        "correct, total, test_loss = 0, 0, 0\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        mask = (y != -100)\n",
        "        y_clamped = y.clone()\n",
        "        y_clamped[y_clamped == -100] = 0\n",
        "        loss = model_classical(X, tags=y_clamped, mask=mask)\n",
        "        test_loss += loss.item()\n",
        "        preds = model_classical(X, mask=mask)\n",
        "        for p_seq, y_seq, m_seq in zip(preds, y, mask):\n",
        "            for p, gold, m in zip(p_seq, y_seq, m_seq):\n",
        "                if m:\n",
        "                    correct += (p == gold.item())\n",
        "                    total += 1\n",
        "\n",
        "print(f\"Test Loss: {test_loss/len(test_loader):.4f} | Test Acc: {correct/total:.4f}\")"
      ],
      "metadata": {
        "id": "3e63vw1pW7B0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0915c0c-87b4-454b-b126-43aa56b58bc7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 9.9528 | Test Acc: 0.8029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_hybrid.eval()\n",
        "correct, total, test_loss = 0, 0, 0\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        mask = (y != -100)\n",
        "        y_clamped = y.clone()\n",
        "        y_clamped[y_clamped == -100] = 0\n",
        "        loss = model_hybrid(X, tags=y_clamped, mask=mask)\n",
        "        test_loss += loss.item()\n",
        "        preds = model_hybrid(X, mask=mask)\n",
        "        for p_seq, y_seq, m_seq in zip(preds, y, mask):\n",
        "            for p, gold, m in zip(p_seq, y_seq, m_seq):\n",
        "                if m:\n",
        "                    correct += (p == gold.item())\n",
        "                    total += 1\n",
        "\n",
        "print(f\"Test Loss: {test_loss/len(test_loader):.4f} | Test Acc: {correct/total:.4f}\")"
      ],
      "metadata": {
        "id": "Yh6Qlw4k_S3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d3b53a-437d-4926-ded4-5e7b00da068d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 12.2256 | Test Acc: 0.7813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_on_loader(model, loader, id2tag, description=\"Model\"):\n",
        "    model.eval()\n",
        "    all_true = []\n",
        "    all_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            mask = (y != -100)\n",
        "            y_clamped = y.clone()\n",
        "            y_clamped[y_clamped == -100] = 0\n",
        "            preds = model(X, mask=mask)\n",
        "\n",
        "            for p_seq, y_seq, m_seq in zip(preds, y, mask):\n",
        "                for p, gold, m in zip(p_seq, y_seq, m_seq):\n",
        "                    if m:\n",
        "                        all_true.append(gold.item())\n",
        "                        all_pred.append(int(p))\n",
        "\n",
        "    labels = sorted(set(all_true + all_pred))\n",
        "    target_names = [id2tag[i] for i in range(len(id2tag))]\n",
        "\n",
        "    print(f\"\\n=== {description} Evaluation ===\")\n",
        "    print(f\"Total tokens evaluated: {len(all_true)}\\n\")\n",
        "    print(\"Classification report (per-tag precision / recall / f1):\\n\")\n",
        "    print(classification_report(all_true, all_pred, labels=list(range(len(target_names))), target_names=target_names, zero_division=0))\n",
        "\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(all_true, all_pred, labels=list(range(len(target_names))), zero_division=0)\n",
        "    tag_total = {i: 0 for i in range(len(target_names))}\n",
        "    tag_correct = {i: 0 for i in range(len(target_names))}\n",
        "    for t_true, t_pred in zip(all_true, all_pred):\n",
        "        tag_total[t_true] += 1\n",
        "        if t_true == t_pred:\n",
        "            tag_correct[t_true] += 1\n",
        "\n",
        "    print(\"Per-tag counts and accuracy:\")\n",
        "    print(f\"{'Tag':20} {'Support':>8} {'Precision':>9} {'Recall':>8} {'F1':>8} {'Accuracy':>10}\")\n",
        "    print(\"-\" * 70)\n",
        "    for i, tag in enumerate(target_names):\n",
        "        sup = int(support[i])\n",
        "        prec = precision[i]\n",
        "        rec = recall[i]\n",
        "        f = f1[i]\n",
        "        acc = (tag_correct[i] / tag_total[i]) if tag_total[i] > 0 else 0.0\n",
        "        print(f\"{tag:20} {sup:8d} {prec:9.3f} {rec:8.3f} {f:8.3f} {acc:10.3f}\")\n",
        "    print(\"-\" * 70)\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "\n",
        "evaluate_model_on_loader(model_classical, test_loader, id2tag, description=\"Classical\")\n",
        "evaluate_model_on_loader(model_hybrid, test_loader, id2tag, description=\"Hybrid (Quantum-Classical)\")"
      ],
      "metadata": {
        "id": "mTSYjfLAxNTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19cacba2-a447-4967-cc4a-66b994951685"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Classical Evaluation ===\n",
            "Total tokens evaluated: 3521\n",
            "\n",
            "Classification report (per-tag precision / recall / f1):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          CC       0.59      0.47      0.52       118\n",
            "          DT       0.92      0.87      0.89       238\n",
            "         G_J       0.62      0.62      0.62       199\n",
            "         G_N       0.79      0.84      0.82       755\n",
            "       G_PRP       0.79      0.88      0.83       336\n",
            "       G_PRT       0.51      0.51      0.51       142\n",
            "         G_R       0.64      0.62      0.63       188\n",
            "       G_SYM       0.67      0.32      0.43        31\n",
            "         G_V       0.87      0.82      0.85       697\n",
            "         G_X       0.96      0.96      0.96       478\n",
            "         PSP       0.80      0.83      0.81       339\n",
            "\n",
            "    accuracy                           0.80      3521\n",
            "   macro avg       0.74      0.70      0.72      3521\n",
            "weighted avg       0.80      0.80      0.80      3521\n",
            "\n",
            "Per-tag counts and accuracy:\n",
            "Tag                   Support Precision   Recall       F1   Accuracy\n",
            "----------------------------------------------------------------------\n",
            "CC                        118     0.591    0.466    0.521      0.466\n",
            "DT                        238     0.916    0.866    0.890      0.866\n",
            "G_J                       199     0.620    0.623    0.622      0.623\n",
            "G_N                       755     0.792    0.841    0.816      0.841\n",
            "G_PRP                     336     0.789    0.878    0.831      0.878\n",
            "G_PRT                     142     0.514    0.507    0.511      0.507\n",
            "G_R                       188     0.639    0.622    0.631      0.622\n",
            "G_SYM                      31     0.667    0.323    0.435      0.323\n",
            "G_V                       697     0.871    0.824    0.847      0.824\n",
            "G_X                       478     0.960    0.958    0.959      0.958\n",
            "PSP                       339     0.796    0.829    0.812      0.829\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "=== Hybrid (Quantum-Classical) Evaluation ===\n",
            "Total tokens evaluated: 3521\n",
            "\n",
            "Classification report (per-tag precision / recall / f1):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          CC       0.47      0.12      0.19       118\n",
            "          DT       0.95      0.87      0.91       238\n",
            "         G_J       0.55      0.54      0.54       199\n",
            "         G_N       0.79      0.84      0.81       755\n",
            "       G_PRP       0.86      0.86      0.86       336\n",
            "       G_PRT       0.50      0.37      0.42       142\n",
            "         G_R       0.53      0.60      0.56       188\n",
            "       G_SYM       0.00      0.00      0.00        31\n",
            "         G_V       0.79      0.86      0.82       697\n",
            "         G_X       0.96      0.94      0.95       478\n",
            "         PSP       0.72      0.85      0.78       339\n",
            "\n",
            "    accuracy                           0.78      3521\n",
            "   macro avg       0.65      0.62      0.62      3521\n",
            "weighted avg       0.77      0.78      0.77      3521\n",
            "\n",
            "Per-tag counts and accuracy:\n",
            "Tag                   Support Precision   Recall       F1   Accuracy\n",
            "----------------------------------------------------------------------\n",
            "CC                        118     0.467    0.119    0.189      0.119\n",
            "DT                        238     0.949    0.866    0.905      0.866\n",
            "G_J                       199     0.545    0.543    0.544      0.543\n",
            "G_N                       755     0.790    0.837    0.813      0.837\n",
            "G_PRP                     336     0.858    0.863    0.861      0.863\n",
            "G_PRT                     142     0.500    0.366    0.423      0.366\n",
            "G_R                       188     0.533    0.601    0.565      0.601\n",
            "G_SYM                      31     0.000    0.000    0.000      0.000\n",
            "G_V                       697     0.792    0.858    0.824      0.858\n",
            "G_X                       478     0.964    0.941    0.952      0.941\n",
            "PSP                       339     0.720    0.850    0.779      0.850\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Inference (Synthetically Generated)**"
      ],
      "metadata": {
        "id": "xo8rHsibCJk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_random_samples(model, train_dataset, val_dataset, test_dataset, idx2tag, num_samples=10):\n",
        "    model.eval()\n",
        "    sets = [(\"Train\", train_dataset), (\"Validation\", val_dataset), (\"Test\", test_dataset)]\n",
        "\n",
        "    for set_name, dataset in sets:\n",
        "        print(f\"\\n=== Random Samples from {set_name} Set ===\")\n",
        "        samples = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
        "\n",
        "        for i, idx in enumerate(samples):\n",
        "            X, y = dataset[idx]\n",
        "            X = X.unsqueeze(0)\n",
        "            y = y.unsqueeze(0)\n",
        "\n",
        "            mask = (y != -100)\n",
        "            y_clamped = y.clone()\n",
        "            y_clamped[y_clamped == -100] = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                preds = model(X, mask=mask)\n",
        "                preds = preds[0]\n",
        "\n",
        "            gold_tags = [idx2tag[t.item()] for t in y.squeeze(0) if t.item() != -100]\n",
        "            pred_tags = [idx2tag[p] for p in preds[:len(gold_tags)]]\n",
        "            words = [f\"w{i}\" for i in range(len(gold_tags))]\n",
        "\n",
        "            print(\"\\nSentence prediction:\")\n",
        "            print(f\"{'Word':15}{'Gold':10}{'Pred':10}\")\n",
        "            print(\"-\" * 40)\n",
        "            correct = 0\n",
        "            for w, g, p in zip(words, gold_tags, pred_tags):\n",
        "                print(f\"{w:15}{g:10}{p:10}\")\n",
        "                if g == p:\n",
        "                    correct += 1\n",
        "            acc = correct / len(gold_tags)\n",
        "            print(f\"Sentence Accuracy: {acc:.2f}\")"
      ],
      "metadata": {
        "id": "0QVeHWxOcbjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_random_samples(model_classical, train_dataset, val_dataset, test_dataset, id2tag, num_samples=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a8nAEzvo3bA",
        "outputId": "8d1fff11-2a78-40e2-bb4f-d60b009528ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Random Samples from Train Set ===\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_X       G_X       \n",
            "Sentence Accuracy: 1.00\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_X       G_X       \n",
            "w1             G_V       G_V       \n",
            "w2             G_PRP     G_PRP     \n",
            "w3             G_V       G_V       \n",
            "w4             DT        DT        \n",
            "w5             G_N       G_N       \n",
            "w6             G_N       G_N       \n",
            "w7             PSP       PSP       \n",
            "w8             DT        DT        \n",
            "w9             G_N       G_N       \n",
            "w10            G_X       G_X       \n",
            "w11            G_PRP     G_PRP     \n",
            "w12            G_V       G_V       \n",
            "w13            G_V       G_V       \n",
            "w14            G_PRT     G_PRT     \n",
            "w15            G_V       G_V       \n",
            "w16            G_N       G_N       \n",
            "w17            G_R       G_R       \n",
            "w18            G_X       G_X       \n",
            "w19            G_PRP     G_PRP     \n",
            "w20            G_V       G_V       \n",
            "w21            G_PRT     G_R       \n",
            "w22            G_V       G_V       \n",
            "w23            DT        DT        \n",
            "w24            G_N       G_N       \n",
            "w25            G_X       G_X       \n",
            "Sentence Accuracy: 0.96\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_PRP     G_PRP     \n",
            "w1             G_PRT     G_PRT     \n",
            "w2             G_X       G_X       \n",
            "w3             G_PRP     G_PRP     \n",
            "w4             G_V       G_V       \n",
            "w5             G_R       G_R       \n",
            "w6             G_V       G_V       \n",
            "w7             G_J       G_J       \n",
            "w8             G_N       G_N       \n",
            "w9             G_X       G_X       \n",
            "w10            G_J       G_R       \n",
            "w11            G_V       G_V       \n",
            "w12            G_V       G_V       \n",
            "w13            G_V       G_V       \n",
            "w14            PSP       PSP       \n",
            "w15            G_R       G_R       \n",
            "w16            G_PRP     G_PRP     \n",
            "w17            G_V       G_V       \n",
            "w18            PSP       PSP       \n",
            "w19            G_N       G_N       \n",
            "Sentence Accuracy: 0.95\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_N       G_N       \n",
            "w1             G_N       G_N       \n",
            "w2             G_X       G_X       \n",
            "w3             G_PRP     G_PRP     \n",
            "w4             G_V       G_V       \n",
            "w5             DT        G_PRP     \n",
            "w6             G_N       G_N       \n",
            "w7             G_X       G_X       \n",
            "w8             G_V       G_V       \n",
            "w9             G_PRP     G_PRP     \n",
            "Sentence Accuracy: 0.90\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_PRP     G_PRP     \n",
            "w1             G_N       G_N       \n",
            "w2             PSP       PSP       \n",
            "w3             G_J       G_N       \n",
            "w4             PSP       PSP       \n",
            "w5             G_N       G_N       \n",
            "w6             PSP       PSP       \n",
            "w7             G_N       G_N       \n",
            "w8             G_V       G_V       \n",
            "w9             G_V       G_V       \n",
            "w10            PSP       G_N       \n",
            "w11            G_N       G_N       \n",
            "w12            G_N       G_N       \n",
            "w13            PSP       PSP       \n",
            "w14            G_J       G_J       \n",
            "w15            G_N       G_N       \n",
            "w16            G_PRT     G_PRT     \n",
            "w17            G_J       G_J       \n",
            "w18            G_N       G_N       \n",
            "w19            G_V       G_V       \n",
            "w20            G_V       G_V       \n",
            "w21            G_V       G_V       \n",
            "w22            G_N       G_N       \n",
            "w23            PSP       PSP       \n",
            "w24            G_J       G_N       \n",
            "w25            G_V       G_V       \n",
            "w26            G_V       G_V       \n",
            "w27            G_N       G_N       \n",
            "w28            G_N       G_N       \n",
            "Sentence Accuracy: 0.90\n",
            "\n",
            "=== Random Samples from Validation Set ===\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             PSP       PSP       \n",
            "w1             G_N       G_N       \n",
            "w2             G_PRT     G_PRT     \n",
            "w3             G_N       G_N       \n",
            "w4             G_N       G_N       \n",
            "w5             G_V       G_V       \n",
            "w6             CC        PSP       \n",
            "w7             G_N       G_N       \n",
            "w8             G_N       G_V       \n",
            "w9             G_V       G_V       \n",
            "Sentence Accuracy: 0.80\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_N       G_N       \n",
            "w1             G_X       G_X       \n",
            "w2             G_N       G_N       \n",
            "w3             G_N       G_N       \n",
            "w4             G_X       G_X       \n",
            "w5             G_J       G_J       \n",
            "w6             G_N       G_N       \n",
            "w7             G_N       G_N       \n",
            "w8             G_R       G_N       \n",
            "w9             PSP       PSP       \n",
            "w10            G_J       G_J       \n",
            "w11            G_N       G_N       \n",
            "w12            CC        CC        \n",
            "w13            G_N       G_N       \n",
            "w14            DT        DT        \n",
            "w15            PSP       PSP       \n",
            "w16            G_N       G_N       \n",
            "w17            G_X       G_X       \n",
            "w18            CC        PSP       \n",
            "w19            G_PRP     G_N       \n",
            "w20            G_R       G_R       \n",
            "w21            G_V       G_V       \n",
            "w22            G_R       G_R       \n",
            "w23            G_PRP     G_PRP     \n",
            "w24            G_V       G_V       \n",
            "w25            G_R       G_R       \n",
            "w26            G_SYM     G_N       \n",
            "w27            G_X       G_X       \n",
            "Sentence Accuracy: 0.86\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_N       G_X       \n",
            "w1             G_N       G_N       \n",
            "w2             G_N       G_N       \n",
            "Sentence Accuracy: 0.67\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_PRT     G_N       \n",
            "w1             G_PRP     G_J       \n",
            "w2             G_N       G_N       \n",
            "w3             PSP       PSP       \n",
            "w4             G_PRP     G_SYM     \n",
            "w5             G_N       G_N       \n",
            "w6             PSP       PSP       \n",
            "w7             G_PRP     G_PRP     \n",
            "w8             G_N       G_N       \n",
            "w9             PSP       G_V       \n",
            "w10            G_PRP     G_PRP     \n",
            "w11            G_J       G_J       \n",
            "w12            G_N       G_N       \n",
            "w13            G_V       G_V       \n",
            "w14            G_V       G_V       \n",
            "w15            G_X       G_X       \n",
            "Sentence Accuracy: 0.75\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_N       G_N       \n",
            "w1             G_N       G_N       \n",
            "Sentence Accuracy: 1.00\n",
            "\n",
            "=== Random Samples from Test Set ===\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_PRP     G_PRP     \n",
            "w1             G_V       G_V       \n",
            "w2             CC        PSP       \n",
            "w3             G_N       G_N       \n",
            "Sentence Accuracy: 0.75\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_N       G_N       \n",
            "w1             G_PRP     G_N       \n",
            "w2             G_V       G_V       \n",
            "w3             DT        G_PRP     \n",
            "w4             G_J       G_SYM     \n",
            "w5             CC        PSP       \n",
            "w6             G_PRP     G_PRP     \n",
            "w7             G_V       G_V       \n",
            "w8             G_V       G_V       \n",
            "w9             G_X       G_X       \n",
            "w10            G_R       G_N       \n",
            "w11            CC        CC        \n",
            "w12            G_V       G_V       \n",
            "w13            G_R       G_PRT     \n",
            "w14            CC        PSP       \n",
            "w15            CC        G_PRT     \n",
            "w16            G_R       G_R       \n",
            "w17            G_V       G_V       \n",
            "w18            G_X       G_X       \n",
            "w19            G_X       G_X       \n",
            "w20            G_X       G_X       \n",
            "Sentence Accuracy: 0.62\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_N       G_N       \n",
            "w1             CC        PSP       \n",
            "w2             G_N       G_N       \n",
            "w3             G_PRP     G_PRP     \n",
            "w4             G_V       G_V       \n",
            "w5             G_N       G_PRP     \n",
            "w6             G_PRP     G_N       \n",
            "w7             G_R       G_R       \n",
            "w8             G_J       G_J       \n",
            "w9             CC        PSP       \n",
            "w10            G_X       G_X       \n",
            "w11            G_PRP     G_PRP     \n",
            "w12            G_V       G_V       \n",
            "w13            CC        G_PRT     \n",
            "w14            G_V       G_V       \n",
            "w15            G_V       G_V       \n",
            "w16            DT        DT        \n",
            "w17            G_N       G_N       \n",
            "w18            CC        PSP       \n",
            "w19            DT        DT        \n",
            "w20            G_N       G_N       \n",
            "w21            G_R       G_R       \n",
            "w22            G_PRP     G_PRP     \n",
            "w23            G_V       G_V       \n",
            "w24            G_N       G_N       \n",
            "w25            CC        PSP       \n",
            "w26            DT        DT        \n",
            "w27            G_V       G_J       \n",
            "w28            G_X       G_X       \n",
            "w29            G_PRP     G_N       \n",
            "w30            G_R       G_PRP     \n",
            "w31            G_V       G_V       \n",
            "w32            G_N       G_N       \n",
            "w33            G_R       G_R       \n",
            "w34            G_X       G_X       \n",
            "w35            G_N       G_N       \n",
            "w36            PSP       G_N       \n",
            "w37            G_X       G_X       \n",
            "w38            PSP       PSP       \n",
            "w39            G_X       G_X       \n",
            "w40            G_N       G_N       \n",
            "w41            PSP       G_PRT     \n",
            "w42            G_X       G_X       \n",
            "w43            PSP       PSP       \n",
            "w44            G_PRP     G_PRP     \n",
            "w45            G_V       G_V       \n",
            "w46            G_V       G_V       \n",
            "w47            DT        DT        \n",
            "w48            G_N       G_V       \n",
            "w49            G_X       G_X       \n",
            "w50            G_J       G_PRP     \n",
            "w51            CC        G_PRT     \n",
            "w52            DT        DT        \n",
            "w53            G_PRP     G_PRP     \n",
            "w54            G_V       G_V       \n",
            "w55            G_J       G_V       \n",
            "w56            G_R       G_R       \n",
            "w57            CC        G_PRT     \n",
            "w58            G_V       G_V       \n",
            "w59            CC        PSP       \n",
            "w60            G_PRP     G_PRP     \n",
            "w61            G_V       G_R       \n",
            "w62            G_X       G_X       \n",
            "Sentence Accuracy: 0.70\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_V       G_PRT     \n",
            "w1             G_N       G_N       \n",
            "w2             G_R       PSP       \n",
            "w3             G_N       G_N       \n",
            "w4             PSP       PSP       \n",
            "w5             G_V       G_N       \n",
            "w6             G_V       G_V       \n",
            "w7             G_V       G_V       \n",
            "Sentence Accuracy: 0.62\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_X       G_X       \n",
            "w1             G_PRP     G_PRP     \n",
            "w2             G_V       G_V       \n",
            "w3             PSP       PSP       \n",
            "w4             G_N       G_N       \n",
            "w5             PSP       PSP       \n",
            "Sentence Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_random_samples(model_hybrid, train_dataset, val_dataset, test_dataset, id2tag, num_samples=5)"
      ],
      "metadata": {
        "id": "dyJCTLG-aFfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c65cd49-d83a-4215-dae2-5fa5f420c0fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Random Samples from Train Set ===\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_N       G_N       \n",
            "w1             G_V       G_V       \n",
            "w2             PSP       PSP       \n",
            "w3             G_N       G_N       \n",
            "w4             G_R       G_R       \n",
            "w5             G_X       G_X       \n",
            "Sentence Accuracy: 1.00\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_PRP     G_PRP     \n",
            "w1             G_N       G_N       \n",
            "w2             G_V       G_V       \n",
            "w3             G_V       G_V       \n",
            "w4             G_N       G_N       \n",
            "w5             G_N       G_N       \n",
            "w6             PSP       PSP       \n",
            "w7             G_N       G_N       \n",
            "Sentence Accuracy: 1.00\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_N       G_N       \n",
            "w1             G_X       G_X       \n",
            "w2             G_X       G_X       \n",
            "w3             G_N       G_N       \n",
            "w4             G_X       G_X       \n",
            "w5             G_J       G_J       \n",
            "w6             G_N       G_N       \n",
            "w7             G_PRT     G_V       \n",
            "Sentence Accuracy: 0.88\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_PRT     G_PRT     \n",
            "w1             G_J       G_R       \n",
            "w2             G_X       G_X       \n",
            "w3             CC        PSP       \n",
            "w4             G_N       G_N       \n",
            "w5             G_N       G_N       \n",
            "w6             G_N       G_V       \n",
            "w7             G_V       G_V       \n",
            "w8             G_V       G_V       \n",
            "w9             G_V       G_V       \n",
            "w10            G_X       G_X       \n",
            "w11            CC        PSP       \n",
            "w12            G_N       G_N       \n",
            "w13            G_N       G_N       \n",
            "w14            G_N       G_N       \n",
            "w15            PSP       PSP       \n",
            "w16            G_J       G_N       \n",
            "w17            G_V       G_V       \n",
            "w18            G_X       G_X       \n",
            "w19            G_N       G_N       \n",
            "w20            G_N       G_N       \n",
            "w21            G_N       G_N       \n",
            "w22            G_N       G_N       \n",
            "Sentence Accuracy: 0.78\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_N       G_N       \n",
            "w1             G_V       G_V       \n",
            "w2             G_V       G_V       \n",
            "w3             G_PRT     G_R       \n",
            "w4             PSP       PSP       \n",
            "w5             G_V       G_N       \n",
            "w6             G_V       G_V       \n",
            "w7             G_N       G_N       \n",
            "w8             PSP       PSP       \n",
            "w9             G_V       G_V       \n",
            "w10            G_R       G_X       \n",
            "w11            G_PRP     G_PRP     \n",
            "w12            G_N       G_N       \n",
            "w13            G_V       G_V       \n",
            "w14            G_N       G_N       \n",
            "w15            PSP       PSP       \n",
            "w16            G_N       G_N       \n",
            "w17            G_N       G_N       \n",
            "w18            G_J       G_R       \n",
            "w19            G_PRP     G_PRP     \n",
            "w20            G_V       G_V       \n",
            "w21            G_V       G_V       \n",
            "w22            G_X       G_X       \n",
            "Sentence Accuracy: 0.83\n",
            "\n",
            "=== Random Samples from Validation Set ===\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             PSP       PSP       \n",
            "w1             G_R       G_J       \n",
            "w2             G_N       G_N       \n",
            "w3             G_N       G_N       \n",
            "w4             G_V       G_V       \n",
            "w5             G_N       G_N       \n",
            "w6             G_N       G_N       \n",
            "w7             G_N       G_N       \n",
            "w8             G_X       G_X       \n",
            "w9             G_N       G_N       \n",
            "w10            G_R       G_PRT     \n",
            "w11            G_V       G_V       \n",
            "w12            G_V       G_V       \n",
            "w13            G_PRT     G_PRT     \n",
            "w14            CC        PSP       \n",
            "w15            G_N       G_N       \n",
            "w16            PSP       PSP       \n",
            "w17            G_R       G_PRT     \n",
            "w18            G_V       G_V       \n",
            "w19            G_V       G_V       \n",
            "w20            G_X       G_X       \n",
            "w21            G_PRT     G_R       \n",
            "Sentence Accuracy: 0.77\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_PRP     G_PRP     \n",
            "w1             G_J       G_R       \n",
            "w2             G_V       G_V       \n",
            "w3             G_V       G_V       \n",
            "w4             G_V       G_PRP     \n",
            "w5             G_PRP     G_PRP     \n",
            "w6             G_SYM     G_R       \n",
            "w7             G_J       G_J       \n",
            "w8             G_N       G_N       \n",
            "w9             G_V       G_V       \n",
            "Sentence Accuracy: 0.70\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_N       G_N       \n",
            "w1             G_SYM     G_R       \n",
            "w2             G_J       G_J       \n",
            "w3             G_N       G_N       \n",
            "w4             G_V       G_R       \n",
            "w5             G_V       G_V       \n",
            "w6             G_V       G_V       \n",
            "w7             PSP       PSP       \n",
            "w8             CC        G_PRT     \n",
            "w9             G_PRP     G_V       \n",
            "w10            G_N       G_N       \n",
            "w11            G_PRT     PSP       \n",
            "w12            G_V       G_N       \n",
            "w13            G_V       G_V       \n",
            "w14            PSP       G_PRT     \n",
            "w15            G_N       G_N       \n",
            "w16            PSP       G_R       \n",
            "w17            PSP       PSP       \n",
            "w18            G_PRP     G_V       \n",
            "w19            G_N       G_J       \n",
            "w20            G_N       G_N       \n",
            "w21            G_V       G_V       \n",
            "w22            G_V       G_V       \n",
            "w23            G_V       G_V       \n",
            "Sentence Accuracy: 0.58\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             PSP       PSP       \n",
            "w1             G_N       G_N       \n",
            "w2             G_N       G_N       \n",
            "w3             PSP       PSP       \n",
            "w4             G_N       G_N       \n",
            "w5             G_X       G_X       \n",
            "Sentence Accuracy: 1.00\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_PRT     G_PRT     \n",
            "w1             G_PRT     G_V       \n",
            "w2             G_PRT     G_V       \n",
            "Sentence Accuracy: 0.33\n",
            "\n",
            "=== Random Samples from Test Set ===\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_PRT     G_PRT     \n",
            "w1             G_X       G_X       \n",
            "w2             G_N       G_N       \n",
            "w3             G_V       G_V       \n",
            "w4             G_J       G_J       \n",
            "w5             PSP       PSP       \n",
            "w6             G_X       G_N       \n",
            "w7             G_N       G_N       \n",
            "w8             G_X       G_X       \n",
            "w9             G_V       G_V       \n",
            "w10            G_PRP     G_PRP     \n",
            "w11            DT        DT        \n",
            "w12            G_N       G_J       \n",
            "w13            PSP       PSP       \n",
            "w14            G_J       G_N       \n",
            "w15            G_N       G_V       \n",
            "w16            G_N       G_N       \n",
            "Sentence Accuracy: 0.76\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_PRP     G_PRP     \n",
            "w1             G_V       G_V       \n",
            "w2             DT        DT        \n",
            "w3             G_J       G_J       \n",
            "w4             G_N       G_N       \n",
            "w5             G_V       G_V       \n",
            "w6             DT        DT        \n",
            "w7             G_J       G_J       \n",
            "w8             G_N       G_N       \n",
            "w9             G_X       G_X       \n",
            "w10            DT        DT        \n",
            "w11            G_N       G_N       \n",
            "w12            G_V       G_V       \n",
            "w13            G_R       G_R       \n",
            "w14            G_V       G_V       \n",
            "w15            DT        DT        \n",
            "w16            G_J       G_J       \n",
            "w17            G_X       G_X       \n",
            "w18            G_PRP     G_PRP     \n",
            "w19            G_V       G_V       \n",
            "w20            PSP       G_PRT     \n",
            "w21            G_V       G_N       \n",
            "w22            G_R       G_R       \n",
            "w23            PSP       CC        \n",
            "w24            G_V       G_V       \n",
            "w25            G_R       G_R       \n",
            "w26            G_R       G_V       \n",
            "w27            G_R       G_R       \n",
            "w28            G_R       G_V       \n",
            "w29            G_V       G_V       \n",
            "w30            G_PRP     G_PRP     \n",
            "w31            G_V       G_V       \n",
            "w32            G_X       G_X       \n",
            "Sentence Accuracy: 0.85\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_PRP     G_PRP     \n",
            "w1             G_V       G_V       \n",
            "w2             CC        PSP       \n",
            "w3             G_N       G_X       \n",
            "Sentence Accuracy: 0.50\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_V       G_V       \n",
            "w1             G_V       G_V       \n",
            "w2             DT        DT        \n",
            "w3             G_J       G_N       \n",
            "w4             G_V       G_V       \n",
            "w5             G_V       G_PRP     \n",
            "w6             G_V       G_V       \n",
            "w7             G_V       G_V       \n",
            "w8             PSP       PSP       \n",
            "w9             G_PRP     G_PRP     \n",
            "w10            G_N       G_N       \n",
            "w11            G_X       G_X       \n",
            "Sentence Accuracy: 0.83\n",
            "\n",
            "Sentence prediction:\n",
            "Word           Gold      Pred      \n",
            "----------------------------------------\n",
            "w0             G_X       G_X       \n",
            "w1             G_N       G_N       \n",
            "Sentence Accuracy: 1.00\n"
          ]
        }
      ]
    }
  ]
}