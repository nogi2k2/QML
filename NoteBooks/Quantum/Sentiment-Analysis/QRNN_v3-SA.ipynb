{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BpMy_EPElfv"
      },
      "source": [
        "# **Imports and Library Installations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXg-ZhRs8yGf",
        "outputId": "329c2ddb-971d-47ed-d543-db03baf3e85c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4 --upgrade --force-reinstall --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n13WNiIK_U4W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8rBRK-w_Fr8",
        "outputId": "2402658f-adf8-4bc6-d722-d96e323a461b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX22YoA96Rx_",
        "outputId": "888637d1-5fb7-407a-b0de-a670a55d30cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.41.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pennylane-lightning\n",
            "  Downloading pennylane_lightning-0.41.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.15.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.8.0)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.3)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.14.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (24.2)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning)\n",
            "  Downloading scipy_openblas32-0.3.30.0.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.6.15)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading PennyLane-0.41.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.41.1-cp311-cp311-manylinux_2_28_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.1-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.1 diastatic-malt-2.15.2 pennylane-0.41.1 pennylane-lightning-0.41.1 rustworkx-0.16.0 scipy-openblas32-0.3.30.0.1\n"
          ]
        }
      ],
      "source": [
        "#Install dependencies\n",
        "!pip install pennylane pennylane-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Coau8lq5dVs",
        "outputId": "4c224e13-ca53-4062-ddc2-30419d1e1924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wujDBeIWVfJB",
        "outputId": "9d698ea2-104b-4fdb-9487-7414f9d03dc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pennylane/capture/capture_operators.py:33: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.4.28. You have version 0.5.2 installed. Please downgrade JAX to <=0.4.28 to avoid runtime errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "import math\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "import matplotlib.pyplot as plt\n",
        "import pennylane as qml\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQACW6Glibop",
        "outputId": "7ffd41b2-27ad-4fe7-9497-38a9318291b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "198\n",
            "205\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "print(len(stop_words))\n",
        "domain_neutral_words = {\n",
        "    \"phone\", \"product\", \"battery\", \"headset\", \"quality\", \"one\", \"use\"\n",
        "}\n",
        "stop_words.update(domain_neutral_words)\n",
        "print(len(stop_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-JvfuE2iluD"
      },
      "source": [
        "# **Data Loading and Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pFavMV2CVq48"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/QML-Research/Data/sentiment labelled sentences/amazon_cells_labelled.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "sentences = [line.split(\"\\t\")[0] for line in lines]\n",
        "labels = [int(line.split(\"\\t\")[1]) for line in lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeVFuLNuw6QY",
        "outputId": "19b31c45-043c-4583-ab2f-1e97297f6711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 Labels: 500\n",
            "1 Labels: 500\n"
          ]
        }
      ],
      "source": [
        "print(f\"0 Labels: {labels.count(0)}\")\n",
        "print(f\"1 Labels: {labels.count(1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7tw7RTHtFYv",
        "outputId": "5ce9fb66-fd30-4b77-f7d9-d05d0aefc085"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wXOxpa2nWuqS",
        "outputId": "682b757e-8a8b-46b4-9d93-fb4ee58a027a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence Lengths:\n",
            "min = 11\n",
            "max = 149\n",
            "mean = 55.23\n",
            "95th percentile = 125.04999999999995\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "lengths = [len(s) for s in sentences]\n",
        "print(f\"Sentence Lengths:\\nmin = {np.min(lengths)}\\nmax = {np.max(lengths)}\\nmean = {np.mean(lengths):.2f}\\n95th percentile = {np.percentile(lengths, 95)}\")\n",
        "print()\n",
        "plt.hist(lengths, bins = 30)\n",
        "plt.title(\"Sentence Length Distribution Before Stop Word Removal\")\n",
        "plt.xlabel(\"Sentence Length\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nARd4RZaik6n"
      },
      "outputs": [],
      "source": [
        "def clean_and_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "cleaned_sents = [clean_and_tokenize(sentence) for sentence in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOrMaAVspzbr"
      },
      "source": [
        "# **Padding and Truncation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rt3Wu3pApwci"
      },
      "outputs": [],
      "source": [
        "max_len = 10\n",
        "for i in range(len(cleaned_sents)):\n",
        "  if (len(cleaned_sents[i]) < max_len):\n",
        "    cleaned_sents[i] += [\"<PAD>\"] * (max_len - len(cleaned_sents[i]))\n",
        "  else:\n",
        "    cleaned_sents[i] = cleaned_sents[i][:max_len]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zANXoZ7r5P1Z"
      },
      "source": [
        "# **Manual GloVe Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gXPD5oWx5PQg"
      },
      "outputs": [],
      "source": [
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4T9ySygT5vCf"
      },
      "outputs": [],
      "source": [
        "def sentence_to_vec(sentence, embeddings, dim):\n",
        "    vectors = []\n",
        "    for word in sentence:\n",
        "        if word in embeddings:\n",
        "            vectors.append(embeddings[word])\n",
        "        else:\n",
        "            vectors.append(np.zeros(dim))\n",
        "    return vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JwH8ts1o5vAO"
      },
      "outputs": [],
      "source": [
        "def embed_sentences(cleaned_sents, embeddings, dim=5):\n",
        "    return np.array([sentence_to_vec(tokens, embeddings, dim) for tokens in cleaned_sents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "E8gSQcWF5u9l"
      },
      "outputs": [],
      "source": [
        "glove_path = '/content/drive/MyDrive/QML-Research/Data/glove.6B.100d.txt'\n",
        "glove = load_glove_embeddings(glove_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UNpScLuCJpT"
      },
      "source": [
        "# **PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5Y2dkcDCCInS"
      },
      "outputs": [],
      "source": [
        "all_words = list(glove.keys())\n",
        "all_vectors = np.array([glove[word] for word in all_words])\n",
        "# pca = PCA(n_components=15, random_state=42)\n",
        "pca = PCA(n_components=5, random_state=42)\n",
        "reduced_vectors = pca.fit_transform(all_vectors)\n",
        "reduced_embeddings = {word: reduced_vectors[i] for i, word in enumerate(all_words)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K7EODX6CPup"
      },
      "source": [
        "# **Word Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "88dYvXhz6ZE-"
      },
      "outputs": [],
      "source": [
        "# X_embed_np = embed_sentences(cleaned_sents, reduced_embeddings, dim=15)\n",
        "X_embed_np = embed_sentences(cleaned_sents, reduced_embeddings, dim=5)\n",
        "X_embed = torch.tensor(X_embed_np).float()\n",
        "y_embed = torch.tensor(labels).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoIII4KCdxJL"
      },
      "source": [
        "# **DataLoader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Qa4_XeG2Vqz0"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_embed, y_embed, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O7DmHla6VqxF"
      },
      "outputs": [],
      "source": [
        "class AmazonDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1yNozjtYVqui"
      },
      "outputs": [],
      "source": [
        "train_dataset = AmazonDataset(X_train, y_train)\n",
        "train_loader_small = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(AmazonDataset(X_test, y_test), batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4g3qDLgfInk"
      },
      "source": [
        "# **Config - 1 (Dense Angle Encoding)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hi-3cG-fOJL"
      },
      "outputs": [],
      "source": [
        "n_qubits = 4\n",
        "dev = qml.device(\"lightning.qubit\", wires=n_qubits, shots=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJVTugSjfOiu"
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_step(inputs, hidden, weights_in, weights_h):\n",
        "    for i in range(5):\n",
        "        qml.RY(inputs[i], wires=0)\n",
        "        qml.RZ(inputs[i] * 0.5, wires=0)\n",
        "\n",
        "    for i in range(5, 10):\n",
        "        qml.RY(inputs[i], wires=1)\n",
        "        qml.RZ(inputs[i] * 0.5, wires=1)\n",
        "\n",
        "    for i in range(10, 15):\n",
        "        qml.RY(inputs[i], wires=2)\n",
        "        qml.RZ(inputs[i] * 0.5, wires=2)\n",
        "\n",
        "    qml.RY(hidden, wires=3)\n",
        "\n",
        "    qml.CNOT(wires=[0, 1])\n",
        "    qml.CNOT(wires=[1, 2])\n",
        "    qml.CNOT(wires=[2, 3])\n",
        "    qml.CNOT(wires=[3, 0])\n",
        "\n",
        "    for i in range(3):\n",
        "        qml.RY(weights_in[i], wires=i)\n",
        "        qml.RZ(weights_in[i + 3], wires=i)\n",
        "\n",
        "    qml.RY(weights_h[0], wires=3)\n",
        "    return qml.expval(qml.PauliZ(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipvyWU4hfOqr"
      },
      "outputs": [],
      "source": [
        "class QuantumRNN(nn.Module):\n",
        "    def __init__(self, n_repeats=3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Linear(15, 15)\n",
        "        self.weights_in = nn.Parameter(torch.randn(6) * 0.1)\n",
        "        self.weights_h = nn.Parameter(torch.randn(1) * 0.1)\n",
        "        self.classifier = nn.Linear(1, 1)\n",
        "        self.n_repeats = n_repeats\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[0]\n",
        "        hidden = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            input_t = self.embedding(x[t])\n",
        "            for _ in range(self.n_repeats):\n",
        "                hidden = quantum_step(input_t, hidden, self.weights_in, self.weights_h).float()\n",
        "\n",
        "        out = self.classifier(hidden.unsqueeze(0))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ueg6Y0nFJxxe"
      },
      "source": [
        "# **Config - 2 (Angle Encoding)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KzMuxsmEJ3Bg"
      },
      "outputs": [],
      "source": [
        "n_qubits = 6\n",
        "dev = qml.device(\"lightning.qubit\", wires=n_qubits, shots=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "It-qqbbPJ27G"
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_step(inputs, hidden, weights_in, weights_h):\n",
        "    inputs_norm = inputs / (torch.norm(inputs) + 1e-8)\n",
        "\n",
        "    for i in range(5):\n",
        "        qml.RY(inputs_norm[i], wires=i)\n",
        "\n",
        "    qml.RY(hidden, wires=5)\n",
        "\n",
        "    for i in range(5):\n",
        "        qml.CNOT(wires=[i, (i + 1) % 5])\n",
        "\n",
        "    for i in range(5):\n",
        "        qml.RY(weights_in[i], wires=i)\n",
        "\n",
        "    qml.RY(weights_h[0], wires=5)\n",
        "    return qml.expval(qml.PauliZ(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry05rp_jJ2wd"
      },
      "outputs": [],
      "source": [
        "class QuantumRNN(nn.Module):\n",
        "    def __init__(self, n_repeats=3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Linear(5, 5)\n",
        "        self.weights_in = nn.Parameter(torch.randn(5) * 0.1)\n",
        "        self.weights_h = nn.Parameter(torch.randn(1) * 0.1)\n",
        "        self.classifier = nn.Linear(1, 1)\n",
        "        self.n_repeats = n_repeats\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[0]\n",
        "        hidden = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            input_t = self.embedding(x[t])\n",
        "            for _ in range(self.n_repeats):\n",
        "                hidden = quantum_step(input_t, hidden, self.weights_in, self.weights_h).float()\n",
        "\n",
        "        out = self.classifier(hidden.unsqueeze(0))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFJPnC_oxRHq",
        "outputId": "7f41aad5-cfd1-4cab-d30f-a99a65801fb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.7121, Accuracy: 48.50%\n",
            "Time for epoch: 161.49 seconds\n",
            "Epoch 2, Loss: 0.6959, Accuracy: 46.88%\n",
            "Time for epoch: 162.12 seconds\n",
            "Epoch 3, Loss: 0.6962, Accuracy: 48.38%\n",
            "Time for epoch: 160.49 seconds\n",
            "Epoch 4, Loss: 0.6962, Accuracy: 50.62%\n",
            "Time for epoch: 163.64 seconds\n",
            "Epoch 5, Loss: 0.6957, Accuracy: 48.62%\n",
            "Time for epoch: 161.13 seconds\n",
            "Epoch 6, Loss: 0.6958, Accuracy: 48.12%\n",
            "Time for epoch: 171.93 seconds\n",
            "Epoch 7, Loss: 0.6937, Accuracy: 52.25%\n",
            "Time for epoch: 166.20 seconds\n",
            "Epoch 8, Loss: 0.6956, Accuracy: 46.38%\n",
            "Time for epoch: 168.20 seconds\n",
            "Epoch 9, Loss: 0.6960, Accuracy: 47.38%\n",
            "Time for epoch: 165.76 seconds\n",
            "Epoch 10, Loss: 0.6939, Accuracy: 51.50%\n",
            "Time for epoch: 164.85 seconds\n",
            "Epoch 11, Loss: 0.6956, Accuracy: 44.88%\n",
            "Time for epoch: 169.14 seconds\n",
            "Epoch 12, Loss: 0.6953, Accuracy: 49.38%\n",
            "Time for epoch: 167.35 seconds\n",
            "Epoch 13, Loss: 0.6948, Accuracy: 49.25%\n",
            "Time for epoch: 167.70 seconds\n",
            "Epoch 14, Loss: 0.6948, Accuracy: 47.75%\n",
            "Time for epoch: 164.39 seconds\n",
            "Epoch 15, Loss: 0.6944, Accuracy: 51.62%\n",
            "Time for epoch: 166.68 seconds\n",
            "Epoch 16, Loss: 0.6948, Accuracy: 50.62%\n",
            "Time for epoch: 167.27 seconds\n",
            "Epoch 17, Loss: 0.6943, Accuracy: 49.75%\n",
            "Time for epoch: 166.11 seconds\n",
            "Epoch 18, Loss: 0.6951, Accuracy: 46.88%\n",
            "Time for epoch: 163.59 seconds\n",
            "Epoch 19, Loss: 0.6948, Accuracy: 50.25%\n",
            "Time for epoch: 162.76 seconds\n",
            "Epoch 20, Loss: 0.6948, Accuracy: 47.50%\n",
            "Time for epoch: 159.68 seconds\n",
            "Epoch 21, Loss: 0.6944, Accuracy: 48.88%\n",
            "Time for epoch: 161.40 seconds\n",
            "Epoch 22, Loss: 0.6946, Accuracy: 49.75%\n",
            "Time for epoch: 162.11 seconds\n",
            "Epoch 23, Loss: 0.6945, Accuracy: 52.00%\n",
            "Time for epoch: 160.04 seconds\n",
            "Epoch 24, Loss: 0.6934, Accuracy: 51.25%\n",
            "Time for epoch: 160.45 seconds\n",
            "Epoch 25, Loss: 0.6945, Accuracy: 48.75%\n",
            "Time for epoch: 160.66 seconds\n",
            "Epoch 26, Loss: 0.6948, Accuracy: 50.00%\n",
            "Time for epoch: 157.71 seconds\n",
            "Epoch 27, Loss: 0.6948, Accuracy: 50.00%\n",
            "Time for epoch: 160.07 seconds\n",
            "Epoch 28, Loss: 0.6946, Accuracy: 49.50%\n",
            "Time for epoch: 158.42 seconds\n",
            "Epoch 29, Loss: 0.6952, Accuracy: 48.75%\n",
            "Time for epoch: 161.08 seconds\n",
            "Epoch 30, Loss: 0.6947, Accuracy: 50.00%\n",
            "Time for epoch: 163.94 seconds\n",
            "Epoch 31, Loss: 0.6951, Accuracy: 48.62%\n",
            "Time for epoch: 162.56 seconds\n",
            "Epoch 32, Loss: 0.6949, Accuracy: 48.00%\n",
            "Time for epoch: 164.07 seconds\n",
            "Epoch 33, Loss: 0.6939, Accuracy: 50.12%\n",
            "Time for epoch: 164.50 seconds\n",
            "Epoch 34, Loss: 0.6954, Accuracy: 46.75%\n",
            "Time for epoch: 162.44 seconds\n",
            "Epoch 35, Loss: 0.6952, Accuracy: 48.88%\n",
            "Time for epoch: 161.22 seconds\n",
            "Epoch 36, Loss: 0.6948, Accuracy: 47.75%\n",
            "Time for epoch: 162.03 seconds\n",
            "Epoch 37, Loss: 0.6936, Accuracy: 51.38%\n",
            "Time for epoch: 161.23 seconds\n",
            "Epoch 38, Loss: 0.6937, Accuracy: 51.25%\n",
            "Time for epoch: 163.11 seconds\n",
            "Epoch 39, Loss: 0.6951, Accuracy: 47.75%\n",
            "Time for epoch: 162.74 seconds\n",
            "Epoch 40, Loss: 0.6944, Accuracy: 50.12%\n",
            "Time for epoch: 159.60 seconds\n",
            "Epoch 41, Loss: 0.6942, Accuracy: 50.25%\n",
            "Time for epoch: 162.37 seconds\n",
            "Epoch 42, Loss: 0.6953, Accuracy: 49.75%\n",
            "Time for epoch: 163.68 seconds\n",
            "Epoch 43, Loss: 0.6950, Accuracy: 50.12%\n",
            "Time for epoch: 160.67 seconds\n",
            "Epoch 44, Loss: 0.6944, Accuracy: 50.88%\n",
            "Time for epoch: 162.61 seconds\n",
            "Epoch 45, Loss: 0.6949, Accuracy: 48.38%\n",
            "Time for epoch: 160.77 seconds\n",
            "Epoch 46, Loss: 0.6944, Accuracy: 50.88%\n",
            "Time for epoch: 158.05 seconds\n",
            "Epoch 47, Loss: 0.6951, Accuracy: 49.38%\n",
            "Time for epoch: 160.74 seconds\n",
            "Epoch 48, Loss: 0.6953, Accuracy: 50.88%\n",
            "Time for epoch: 160.88 seconds\n",
            "Epoch 49, Loss: 0.6948, Accuracy: 50.12%\n",
            "Time for epoch: 161.39 seconds\n",
            "Epoch 50, Loss: 0.6945, Accuracy: 50.00%\n",
            "Time for epoch: 161.12 seconds\n",
            "\n",
            "Total training time: 8142.09 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "epochs = 50\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    for xb, yb in train_loader_small:\n",
        "        xb = xb.squeeze(0)\n",
        "        yb = yb.squeeze()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(xb)\n",
        "\n",
        "        output_flat = output.squeeze()\n",
        "        yb_flat = yb.float()\n",
        "\n",
        "        loss = loss_fn(output_flat, yb_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = (torch.sigmoid(output_flat) > 0.5).float()\n",
        "        correct += (predicted == yb_flat).sum().item()\n",
        "        total += 1\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader_small)\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Time for epoch: {epoch_time:.2f} seconds\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal training time: {total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXMPLWCDxO3O",
        "outputId": "9b1a7a06-3ad9-43aa-9fe0-a680187445c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logit: -0.032 | Probability: 0.492 | Pred: 0.0, Actual: 1.0\n",
            "Logit: -0.032 | Probability: 0.492 | Pred: 0.0, Actual: 1.0\n",
            "Logit: -0.032 | Probability: 0.492 | Pred: 0.0, Actual: 1.0\n",
            "Logit: -0.032 | Probability: 0.492 | Pred: 0.0, Actual: 0.0\n",
            "Logit: -0.032 | Probability: 0.492 | Pred: 0.0, Actual: 1.0\n",
            "Logit: -0.032 | Probability: 0.492 | Pred: 0.0, Actual: 1.0\n",
            "Logit: -0.032 | Probability: 0.492 | Pred: 0.0, Actual: 0.0\n",
            "Logit: -0.032 | Probability: 0.492 | Pred: 0.0, Actual: 1.0\n",
            "Logit: -0.032 | Probability: 0.492 | Pred: 0.0, Actual: 0.0\n",
            "Logit: -0.032 | Probability: 0.492 | Pred: 0.0, Actual: 1.0\n",
            "Test Accuracy: 46.50%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.squeeze(0)\n",
        "        yb = yb.squeeze()\n",
        "\n",
        "        logits = model(xb)\n",
        "        logits_flat = logits.squeeze()\n",
        "        prob = torch.sigmoid(logits_flat)\n",
        "        pred = (prob > 0.5).float()\n",
        "        actual = yb.float()\n",
        "\n",
        "        if pred == actual:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "        if total <= 10:\n",
        "            print(f\"Logit: {logits_flat.item():.3f} | Probability: {prob.item():.3f} | Pred: {pred.item()}, Actual: {actual.item()}\")\n",
        "\n",
        "print(f\"Test Accuracy: {correct / total * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlPHS2t6EQB7"
      },
      "source": [
        "# **Model Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zl1tzMJfOxZ"
      },
      "outputs": [],
      "source": [
        "# Dense Angle Encoding\n",
        "model = QuantumRNN(n_repeats=2)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lYm3Kn49TwQU"
      },
      "outputs": [],
      "source": [
        "# Angle Encoding\n",
        "model = QuantumRNN(n_repeats=3)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw_pSzbLfO4s",
        "outputId": "fd2b1d84-a626-4a69-c25e-60e4d2c79a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "weights_in: torch.Size([5])\n",
            "weights_h: torch.Size([1])\n",
            "embedding.weight: torch.Size([5, 5])\n",
            "embedding.bias: torch.Size([5])\n",
            "classifier.weight: torch.Size([1, 1])\n",
            "classifier.bias: torch.Size([1])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV1LA8RHEbJd"
      },
      "source": [
        "# **Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qC162v4hQGX",
        "outputId": "365f16e1-9e0a-4b3a-cd89-8fb27c46e650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.7015, Accuracy: 48.25%\n",
            "Time for epoch: 197.81 seconds\n",
            "Epoch 2, Loss: 0.6986, Accuracy: 49.88%\n",
            "Time for epoch: 195.32 seconds\n",
            "Epoch 3, Loss: 0.6981, Accuracy: 46.75%\n",
            "Time for epoch: 200.33 seconds\n",
            "Epoch 4, Loss: 0.6964, Accuracy: 50.88%\n",
            "Time for epoch: 200.70 seconds\n",
            "Epoch 5, Loss: 0.6965, Accuracy: 50.50%\n",
            "Time for epoch: 202.60 seconds\n",
            "Epoch 6, Loss: 0.6957, Accuracy: 48.12%\n",
            "Time for epoch: 198.51 seconds\n",
            "Epoch 7, Loss: 0.6942, Accuracy: 51.88%\n",
            "Time for epoch: 198.62 seconds\n",
            "Epoch 8, Loss: 0.6944, Accuracy: 50.50%\n",
            "Time for epoch: 196.29 seconds\n",
            "Epoch 9, Loss: 0.6955, Accuracy: 47.75%\n",
            "Time for epoch: 197.40 seconds\n",
            "Epoch 10, Loss: 0.6947, Accuracy: 48.12%\n",
            "Time for epoch: 197.67 seconds\n",
            "Epoch 11, Loss: 0.6952, Accuracy: 48.25%\n",
            "Time for epoch: 198.07 seconds\n",
            "Epoch 12, Loss: 0.6940, Accuracy: 48.38%\n",
            "Time for epoch: 200.97 seconds\n",
            "Epoch 13, Loss: 0.6956, Accuracy: 48.62%\n",
            "Time for epoch: 198.37 seconds\n",
            "Epoch 14, Loss: 0.6949, Accuracy: 48.00%\n",
            "Time for epoch: 197.99 seconds\n",
            "Epoch 15, Loss: 0.6953, Accuracy: 47.38%\n",
            "Time for epoch: 196.41 seconds\n",
            "Epoch 16, Loss: 0.6950, Accuracy: 48.62%\n",
            "Time for epoch: 197.98 seconds\n",
            "Epoch 17, Loss: 0.6943, Accuracy: 50.25%\n",
            "Time for epoch: 195.01 seconds\n",
            "Epoch 18, Loss: 0.6957, Accuracy: 47.12%\n",
            "Time for epoch: 195.76 seconds\n",
            "Epoch 19, Loss: 0.6955, Accuracy: 47.62%\n",
            "Time for epoch: 193.70 seconds\n",
            "Epoch 20, Loss: 0.6953, Accuracy: 45.88%\n",
            "Time for epoch: 195.41 seconds\n",
            "Epoch 21, Loss: 0.6947, Accuracy: 47.12%\n",
            "Time for epoch: 192.73 seconds\n",
            "Epoch 22, Loss: 0.6947, Accuracy: 50.62%\n",
            "Time for epoch: 197.68 seconds\n",
            "Epoch 23, Loss: 0.6956, Accuracy: 48.88%\n",
            "Time for epoch: 193.43 seconds\n",
            "Epoch 24, Loss: 0.6953, Accuracy: 47.88%\n",
            "Time for epoch: 192.02 seconds\n",
            "Epoch 25, Loss: 0.6948, Accuracy: 49.62%\n",
            "Time for epoch: 192.05 seconds\n",
            "Epoch 26, Loss: 0.6951, Accuracy: 48.38%\n",
            "Time for epoch: 194.06 seconds\n",
            "Epoch 27, Loss: 0.6945, Accuracy: 49.88%\n",
            "Time for epoch: 194.61 seconds\n",
            "Epoch 28, Loss: 0.6945, Accuracy: 51.12%\n",
            "Time for epoch: 193.95 seconds\n",
            "Epoch 29, Loss: 0.6948, Accuracy: 50.00%\n",
            "Time for epoch: 204.50 seconds\n",
            "Epoch 30, Loss: 0.6949, Accuracy: 49.62%\n",
            "Time for epoch: 194.66 seconds\n",
            "Epoch 31, Loss: 0.6957, Accuracy: 49.00%\n",
            "Time for epoch: 196.30 seconds\n",
            "Epoch 32, Loss: 0.6947, Accuracy: 48.88%\n",
            "Time for epoch: 194.04 seconds\n",
            "Epoch 33, Loss: 0.6943, Accuracy: 51.12%\n",
            "Time for epoch: 194.77 seconds\n",
            "Epoch 34, Loss: 0.6949, Accuracy: 49.12%\n",
            "Time for epoch: 192.02 seconds\n",
            "Epoch 35, Loss: 0.6946, Accuracy: 49.25%\n",
            "Time for epoch: 193.99 seconds\n",
            "Epoch 36, Loss: 0.6944, Accuracy: 50.75%\n",
            "Time for epoch: 192.94 seconds\n",
            "Epoch 37, Loss: 0.6949, Accuracy: 50.12%\n",
            "Time for epoch: 211.33 seconds\n",
            "Epoch 38, Loss: 0.6943, Accuracy: 48.62%\n",
            "Time for epoch: 193.86 seconds\n",
            "Epoch 39, Loss: 0.6943, Accuracy: 49.75%\n",
            "Time for epoch: 193.87 seconds\n",
            "Epoch 40, Loss: 0.6948, Accuracy: 50.62%\n",
            "Time for epoch: 197.26 seconds\n",
            "Epoch 41, Loss: 0.6943, Accuracy: 48.75%\n",
            "Time for epoch: 192.90 seconds\n",
            "Epoch 42, Loss: 0.6954, Accuracy: 47.62%\n",
            "Time for epoch: 217.91 seconds\n",
            "Epoch 43, Loss: 0.6947, Accuracy: 50.88%\n",
            "Time for epoch: 193.98 seconds\n",
            "Epoch 44, Loss: 0.6936, Accuracy: 52.50%\n",
            "Time for epoch: 199.74 seconds\n",
            "Epoch 45, Loss: 0.6950, Accuracy: 49.25%\n",
            "Time for epoch: 193.94 seconds\n",
            "Epoch 46, Loss: 0.6952, Accuracy: 50.62%\n",
            "Time for epoch: 193.52 seconds\n",
            "Epoch 47, Loss: 0.6954, Accuracy: 47.88%\n",
            "Time for epoch: 194.34 seconds\n",
            "Epoch 48, Loss: 0.6931, Accuracy: 52.75%\n",
            "Time for epoch: 195.74 seconds\n",
            "Epoch 49, Loss: 0.6955, Accuracy: 50.00%\n",
            "Time for epoch: 197.88 seconds\n",
            "Epoch 50, Loss: 0.6938, Accuracy: 51.38%\n",
            "Time for epoch: 201.32 seconds\n",
            "\n",
            "Total training time: 9846.27 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "epochs = 50\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    for xb, yb in train_loader_small:\n",
        "        xb = xb.squeeze(0)\n",
        "        yb = yb.squeeze()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(xb)\n",
        "\n",
        "        output_flat = output.squeeze()\n",
        "        yb_flat = yb.float()\n",
        "\n",
        "        loss = loss_fn(output_flat, yb_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = (torch.sigmoid(output_flat) > 0.5).float()\n",
        "        correct += (predicted == yb_flat).sum().item()\n",
        "        total += 1\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader_small)\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Time for epoch: {epoch_time:.2f} seconds\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal training time: {total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt1g-08qEeuZ"
      },
      "source": [
        "# **Testing Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi5_G1yehQA4",
        "outputId": "34a9d14f-89c5-4f62-cb5f-a1e06b0f5a0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logit: 0.105 | Probability: 0.526 | Pred: 1.0, Actual: 1.0\n",
            "Logit: 0.108 | Probability: 0.527 | Pred: 1.0, Actual: 1.0\n",
            "Logit: 0.108 | Probability: 0.527 | Pred: 1.0, Actual: 1.0\n",
            "Logit: 0.085 | Probability: 0.521 | Pred: 1.0, Actual: 0.0\n",
            "Logit: 0.106 | Probability: 0.527 | Pred: 1.0, Actual: 1.0\n",
            "Logit: 0.105 | Probability: 0.526 | Pred: 1.0, Actual: 1.0\n",
            "Logit: 0.099 | Probability: 0.525 | Pred: 1.0, Actual: 0.0\n",
            "Logit: 0.098 | Probability: 0.525 | Pred: 1.0, Actual: 1.0\n",
            "Logit: 0.107 | Probability: 0.527 | Pred: 1.0, Actual: 0.0\n",
            "Logit: 0.107 | Probability: 0.527 | Pred: 1.0, Actual: 1.0\n",
            "Test Accuracy: 53.50%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.squeeze(0)\n",
        "        yb = yb.squeeze()\n",
        "\n",
        "        logits = model(xb)\n",
        "        logits_flat = logits.squeeze()\n",
        "        prob = torch.sigmoid(logits_flat)\n",
        "        pred = (prob > 0.5).float()\n",
        "        actual = yb.float()\n",
        "\n",
        "        if pred == actual:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "        if total <= 10:\n",
        "            print(f\"Logit: {logits_flat.item():.3f} | Probability: {prob.item():.3f} | Pred: {pred.item()}, Actual: {actual.item()}\")\n",
        "\n",
        "print(f\"Test Accuracy: {correct / total * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5BpMy_EPElfv",
        "W-JvfuE2iluD",
        "eOrMaAVspzbr",
        "zANXoZ7r5P1Z",
        "1UNpScLuCJpT",
        "2K7EODX6CPup",
        "BoIII4KCdxJL",
        "s4g3qDLgfInk",
        "mV1LA8RHEbJd",
        "Gt1g-08qEeuZ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
