{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xBSoJuk-YX2-",
        "L0JVY7_xYTua",
        "DvGPyfvsY4FH",
        "kMko8N6ha-19",
        "tyGflm5x0H48",
        "ax1GmPGn3hnl",
        "7jzjSqXhn5Vp",
        "W1_BRSusZZMX",
        "vTirzz1ArZ1R",
        "5wHc_nu3ITLq",
        "qvRycYz9rG81",
        "MjYGEijbc7yy",
        "VODeSsL-4dSr",
        "aJUnYQXIhCuw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports and Downloads**"
      ],
      "metadata": {
        "id": "xBSoJuk-YX2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nind1MPQVDFp",
        "outputId": "d6992b18-22f0-4bfc-9c23-def0ca16ab6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting pennylane\n",
            "  Downloading pennylane-0.42.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pennylane-lightning\n",
            "  Downloading pennylane_lightning-0.42.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.15.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (24.2)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.26.4)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning)\n",
            "  Downloading scipy_openblas32-0.3.30.0.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.7.14)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading pennylane-0.42.0-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.42.0-cp311-cp311-manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.2-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.2 diastatic-malt-2.15.2 pennylane-0.42.0 pennylane-lightning-0.42.0 rustworkx-0.16.0 scipy-openblas32-0.3.30.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4 --upgrade --force-reinstall --quiet\n",
        "!pip install --upgrade pennylane pennylane-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "7SIA-uaBVEDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqZ-dh-NVFkq",
        "outputId": "00ba8912-dfb0-432c-849f-98b07e4f45a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import math\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Quantum computing\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "\n",
        "# NLP and preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Sklearn tools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "pnp.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "emjwDqgTVFhi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading and Preprocessing**"
      ],
      "metadata": {
        "id": "L0JVY7_xYTua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/QML-Research/Data/sentiment labelled sentences/amazon_cells_labelled.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "sentences = [line.split(\"\\t\")[0] for line in lines]\n",
        "labels = [int(line.split(\"\\t\")[1]) for line in lines]"
      ],
      "metadata": {
        "id": "IDJ9A_o7VFbu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "domain_neutral_words = {\n",
        "    \"phone\", \"product\", \"battery\", \"headset\", \"quality\", \"one\", \"use\"\n",
        "}\n",
        "stop_words.update(domain_neutral_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwR5eTa0YlFp",
        "outputId": "ba356a61-c902-4a1c-cab8-6e0064bd4b5f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "cleaned_sents = [clean_and_tokenize(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "IbJX9uXlVFZE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 10\n",
        "for i in range(len(cleaned_sents)):\n",
        "  if (len(cleaned_sents[i]) < max_len):\n",
        "    cleaned_sents[i] += [\"<PAD>\"] * (max_len - len(cleaned_sents[i]))\n",
        "  else:\n",
        "    cleaned_sents[i] = cleaned_sents[i][:max_len]"
      ],
      "metadata": {
        "id": "jommbUzPVFTx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GloVE Word Embeddings**"
      ],
      "metadata": {
        "id": "DvGPyfvsY4FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "KJfUcJaRVFQ1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_path = '/content/drive/MyDrive/QML-Research/Data/glove.6B.100d.txt'\n",
        "glove = load_glove_embeddings(glove_path)"
      ],
      "metadata": {
        "id": "Z40kR5fWZ_0Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AutoEncoder**"
      ],
      "metadata": {
        "id": "kMko8N6ha-19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GloVeAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(GloVeAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "zxYOHIgrVFIh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = list(glove.keys())\n",
        "all_vectors = np.array([glove[word] for word in all_words])\n",
        "all_vectors = normalize(all_vectors)\n",
        "word_tensor = torch.tensor(all_vectors).float()"
      ],
      "metadata": {
        "id": "FKWwxltMVFF6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 8\n",
        "epochs = 100\n",
        "save_path = '/content/drive/MyDrive/QML-Research/Autoencoder/glove_autoencoder_normalized_8.pth'\n",
        "# save_path = '/content/drive/MyDrive/QML-Research/Autoencoder/glove_autoencoder_normalized_32.pth'"
      ],
      "metadata": {
        "id": "HfGzHVQP0h6B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(save_path):\n",
        "    print(f\"Loading Autoencoder from {save_path}\")\n",
        "    autoencoder = GloVeAutoencoder(input_dim=100, latent_dim=latent_dim)\n",
        "    autoencoder.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
        "else:\n",
        "    print(\"Training Autoencoder\")\n",
        "    autoencoder = GloVeAutoencoder(input_dim=100, latent_dim=latent_dim)\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed = autoencoder(word_tensor)\n",
        "        loss = criterion(reconstructed, word_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "    torch.save(autoencoder.state_dict(), save_path)\n",
        "    print(f\"Saved Autoencoder serialized model in drive @ {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUDeGZHIVFC2",
        "outputId": "cb02a3f1-bdd6-4707-a6f6-8e18cf243c50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Autoencoder from /content/drive/MyDrive/QML-Research/Autoencoder/glove_autoencoder_normalized_8.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.eval()\n",
        "with torch.no_grad():\n",
        "    compressed_vectors = autoencoder.encoder(word_tensor).numpy()\n",
        "\n",
        "reduced_embeddings = {\n",
        "    word: compressed_vectors[i]\n",
        "    for i, word in enumerate(all_words)\n",
        "}"
      ],
      "metadata": {
        "id": "eZYs04ccVE_6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding**"
      ],
      "metadata": {
        "id": "tyGflm5x0H48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_vec(sentence, embeddings, dim):\n",
        "    vectors = []\n",
        "    for word in sentence:\n",
        "        if word in embeddings:\n",
        "            vectors.append(embeddings[word])\n",
        "        else:\n",
        "            vectors.append(np.zeros(dim))\n",
        "    return vectors\n",
        "\n",
        "def embed_sentences(cleaned_sents, embeddings, dim):\n",
        "    return np.array([sentence_to_vec(tokens, embeddings, dim) for tokens in cleaned_sents])"
      ],
      "metadata": {
        "id": "LHyTknfpVhiI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_embed_np = embed_sentences(cleaned_sents, reduced_embeddings, dim=8)\n",
        "X_embed_np = (X_embed_np - X_embed_np.min()) * (np.pi / (X_embed_np.max() - X_embed_np.min()))\n",
        "\n",
        "X_embed = torch.tensor(X_embed_np).long()\n",
        "y_embed = torch.tensor(labels).long()"
      ],
      "metadata": {
        "id": "PtDZbVTTVhfS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset and DataLoader**"
      ],
      "metadata": {
        "id": "ax1GmPGn3hnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AmazonDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "KZP80femFVRA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_embed, y_embed, test_size=0.2, stratify=y_embed.numpy(), random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = AmazonDataset(X_train, y_train)\n",
        "test_dataset = AmazonDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)"
      ],
      "metadata": {
        "id": "tLSMr6nta7Ki"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utility Functions**"
      ],
      "metadata": {
        "id": "7jzjSqXhn5Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "    logits = qml.numpy.array(logits)\n",
        "    logits = logits - qml.numpy.max(logits)\n",
        "    exps = qml.numpy.exp(logits)\n",
        "    return exps / qml.numpy.sum(exps)\n",
        "\n",
        "def cross_entropy(label, probs):\n",
        "    p = qml.numpy.clip(probs[label], 1e-10, 1.0)\n",
        "    return -qml.numpy.log(p)\n",
        "\n",
        "# def reshape_params(flat_params):\n",
        "#     wpw = flat_params[:80].reshape((10, 8))\n",
        "#     clf = flat_params[80:].reshape((8,))\n",
        "#     return wpw, clf"
      ],
      "metadata": {
        "id": "o_qhLqOXn5GC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QRNN Model Layers**"
      ],
      "metadata": {
        "id": "W1_BRSusZZMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "First Variation of QRNN that showed learning during training, Achieving a training accuracy of 67% (Highest)\n",
        "and a peak testing accuracy of 55.5% with a training duration of 50-60 epochs. The layers have been split\n",
        "into Unitary and entanglement and have been implemnted as functions. This model processes the whole sequence\n",
        "(All Timesteps) at once in contrast to the previous iterations that processes one time-step/word at a time.\n",
        "\"\"\"\n",
        "\n",
        "# def dense_angle_embedding(x, wires):\n",
        "#     # For 32 dim:\n",
        "#     # chunk_size = 8\n",
        "#     chunk_size = 2\n",
        "\n",
        "#     for i, wire in enumerate(wires):\n",
        "#         for j in range(chunk_size):\n",
        "#             idx = i * chunk_size + j\n",
        "#             qml.RX(x[idx], wires=wire)\n",
        "#             qml.RY(x[idx], wires=wire)\n",
        "#             qml.RZ(x[idx], wires=wire)\n",
        "\n",
        "# def variational_block(weights, wires):\n",
        "#     for i, wire in enumerate(wires):\n",
        "#         qml.RY(weights[i], wires=wire)\n",
        "#         qml.RZ(weights[i + len(wires)], wires=wire)\n",
        "\n",
        "# def entangle(wires):\n",
        "#     for i in range(len(wires) - 1):\n",
        "#         qml.CNOT(wires=[wires[i], wires[i + 1]])\n",
        "#     qml.CNOT(wires=[wires[-1], wires[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OhwchF8tZgac",
        "outputId": "9013bca6-18d2-4b6b-acb2-9018f27cf76a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFirst Variation of QRNN that showed learning during training, Achieving a training accuracy of 67% (Highest)\\nand a peak testing accuracy of 55.5% with a training duration of 50-60 epochs. The layers have been split\\ninto Unitary and entanglement and have been implemnted as functions. This model processes the whole sequence\\n(All Timesteps) at once in contrast to the previous iterations that processes one time-step/word at a time.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The second iteration of the previous model introducing major architectural changes (All layers are implemented as functions):\n",
        "a. Stacked QRNN (2 layered QRNN), with each QRB having seperate weights (No weight sharing)\n",
        "b. Interaction layers implemented seperate from the QRBs (Quantum Recurrent Block)\n",
        "c. Introduced an additional interaction layer (classification layer) before quantum measurement\n",
        "\"\"\"\n",
        "\n",
        "def input_layer(x_sequence, wires):\n",
        "    for word_vec in x_sequence:\n",
        "        for i, wire in enumerate(wires):\n",
        "            qml.RX(word_vec[2 * i], wires=wire)\n",
        "            qml.RY(word_vec[2 * i + 1], wires=wire)\n",
        "            qml.RZ(word_vec[2 * i], wires=wire)\n",
        "\n",
        "# def qrnn_layer(weights, wires):\n",
        "#     for i, wire in enumerate(wires):\n",
        "#         qml.RY(weights[i], wires=wire)\n",
        "#         qml.RZ(weights[i + len(wires)], wires=wire)\n",
        "\n",
        "def qrnn_layer(weights, wires):\n",
        "    for i, wire in enumerate(wires):\n",
        "        qml.Rot(weights[i, 0], weights[i, 1], weights[i, 2], wires=wire)\n",
        "\n",
        "def interaction_layer(wires):\n",
        "    for i in range(len(wires)):\n",
        "        qml.CNOT(wires=[wires[i], wires[(i + 1) % len(wires)]])\n",
        "\n",
        "# def final_interaction_layer(weights, wires):\n",
        "#     for i, wire in enumerate(wires):\n",
        "#         qml.RY(weights[i], wires=wire)\n",
        "#         qml.RZ(weights[i + len(wires)], wires=wire)\n",
        "\n",
        "def final_interaction_layer(weights, wires):\n",
        "    for i, wire in enumerate(wires):\n",
        "        qml.Rot(*weights[i], wires=wire)"
      ],
      "metadata": {
        "id": "283pXFYTZhxS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "vTirzz1ArZ1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def forward(input_sequence, weights_in, weights_h, hidden_init=None):\n",
        "#     \"\"\"\n",
        "#     input_sequence: (10, 32) numpy array\n",
        "#     weights_in: (6,) numpy array\n",
        "#     weights_h: (2,) numpy array\n",
        "#     hidden_init: optional scalar float, default is small random value\n",
        "#     \"\"\"\n",
        "#     if hidden_init is None:\n",
        "#         hidden = np.random.uniform(-0.1, 0.1)\n",
        "#     else:\n",
        "#         hidden = hidden_init\n",
        "\n",
        "#     for word_vec in input_sequence:\n",
        "#         logits = quantum_step(word_vec, hidden, weights_in, weights_h)\n",
        "#         hidden = np.tanh(logits[0] + logits[1])\n",
        "\n",
        "#     return logits"
      ],
      "metadata": {
        "id": "j4KRWY_MZhof"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def qrnn_model(x_sequence, flat_params, wires):\n",
        "    # w_qrnn1 = flat_params[:8]\n",
        "    # w_qrnn2 = flat_params[8:16]\n",
        "    # w_classifier = flat_params[16:32]\n",
        "\n",
        "    w_qrb1 = flat_params[:12].reshape((4, 3))\n",
        "    w_qrb2 = flat_params[12:24].reshape((4, 3))\n",
        "    # w_classifier = flat_params[24:]\n",
        "    w_classifier = flat_params[24:].reshape((4, 3))\n",
        "\n",
        "    input_layer(x_sequence, wires)\n",
        "    qrnn_layer(w_qrb1, wires)\n",
        "    interaction_layer(wires)\n",
        "\n",
        "    qrnn_layer(w_qrb2, wires)\n",
        "    interaction_layer(wires)\n",
        "\n",
        "    final_interaction_layer(w_classifier, wires)"
      ],
      "metadata": {
        "id": "_BQXE4pJrfhJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QNode**"
      ],
      "metadata": {
        "id": "5wHc_nu3ITLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_qubits = 4\n",
        "# dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "dev = qml.device(\"lightning.qubit\", wires=n_qubits)"
      ],
      "metadata": {
        "id": "E1badQSvISf9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simple Qnode with Dense Angle Encoding for the 8 dimensional inputs,\n",
        "A simple Ring entanglement and parameteried training of input weights\n",
        "and hidden weights. Returns 2 quantum measurables.\n",
        "\"\"\"\n",
        "\n",
        "# @qml.qnode(dev, interface=\"torch\")\n",
        "# def quantum_step(inputs, hidden, weights_in, weights_h):\n",
        "#     #Encoding\n",
        "#     dense_angle_embedding(inputs, wires=[0, 1, 2, 3])\n",
        "#     qml.RY(hidden, wires=3)\n",
        "\n",
        "#     # Interaction Layer/Entanglement\n",
        "#     qml.CNOT(wires=[0, 1])\n",
        "#     qml.CNOT(wires=[1, 2])\n",
        "#     qml.CNOT(wires=[2, 3])\n",
        "#     qml.CNOT(wires=[3, 0])\n",
        "\n",
        "#     # Parametrized trainable unitaries on input wires\n",
        "#     for i in range(3):\n",
        "#         qml.RY(weights_in[i], wires=i)\n",
        "#         qml.RZ(weights_in[i + 3], wires=i)\n",
        "\n",
        "#     # Parametrized unitaries on hidden wire\n",
        "#     qml.RY(weights_h[0], wires=3)\n",
        "#     qml.RZ(weights_h[1], wires=3)\n",
        "\n",
        "#     return qml.expval(qml.PauliZ(2)), qml.expval(qml.PauliZ(3))"
      ],
      "metadata": {
        "id": "MyH4XOByISdZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2c77c3cf-b870-4578-bbe3-3b45acb8be69"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSimple Qnode with Dense Angle Encoding for the 8 dimensional inputs,\\nA simple Ring entanglement and parameteried training of input weights\\nand hidden weights. Returns 2 quantum measurables.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Qnode to process a whole sequence at a time in contrast to the initial attempts at\n",
        "processing a time step at once. It still uses 4 qubit dense angle encoding, but also\n",
        "preserves the the hidden wire's quantum state throughout the sequence processing,\n",
        "which helps avoid re-encoding the hidden state at each time-step.\n",
        "\"\"\"\n",
        "\n",
        "# @qml.qnode(dev, interface=\"autograd\")\n",
        "# def full_sentence_qnode(sentence_embeds, weights_per_word, classifier_weights):\n",
        "#     wires = list(range(n_qubits))\n",
        "\n",
        "#     # Processes a whole sequence (10 words * 8 Dimensional vector embedding)\n",
        "#     for t in range(len(sentence_embeds)):\n",
        "#         dense_angle_embedding(sentence_embeds[t], wires)\n",
        "#         entangle(wires)\n",
        "#         variational_block(weights_per_word[t], wires)\n",
        "\n",
        "#     # Final classifier\n",
        "#     entangle(wires)\n",
        "#     variational_block(classifier_weights, wires)\n",
        "\n",
        "#     return qml.expval(qml.PauliZ(wires[2])), qml.expval(qml.PauliZ(wires[3]))"
      ],
      "metadata": {
        "id": "JE2FDMqB9dwL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2fe2ec29-de8a-419c-b4eb-c01eb2c0de48"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nQnode to process a whole sequence at a time in contrast to the initial attempts at\\nprocessing a time step at once. It still uses 4 qubit dense angle encoding, but also\\npreserves the the hidden wire's quantum state throughout the sequence processing,\\nwhich helps avoid re-encoding the hidden state at each time-step.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@qml.qnode(dev, interface=\"autograd\")\n",
        "def stacked_qrnn(x_sequence, flat_params):\n",
        "    wires = list(range(n_qubits))\n",
        "    qrnn_model(x_sequence, flat_params, wires)\n",
        "    return qml.expval(qml.PauliZ(wires[2])), qml.expval(qml.PauliZ(wires[3]))"
      ],
      "metadata": {
        "id": "FdPfadWXrhEi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parameter Initialization**"
      ],
      "metadata": {
        "id": "qvRycYz9rG81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weights_in = np.random.uniform(0, 2 * np.pi, size=6)\n",
        "# weights_h  = np.random.uniform(0, 2 * np.pi, size=2)\n",
        "# optimizer = AdamOptimizer(stepsize=0.01)\n",
        "# epochs = 20"
      ],
      "metadata": {
        "id": "Zvh3XimdrK9W"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# weights_per_word = qml.numpy.array(np.random.uniform(0, 2*np.pi, (10, 8)), requires_grad=True)\n",
        "# classifier_weights = qml.numpy.array(np.random.uniform(0, 2*np.pi, (8,)), requires_grad=True)\n",
        "\n",
        "# flat_params = qml.numpy.concatenate([\n",
        "#     weights_per_word.flatten(),\n",
        "#     classifier_weights.flatten()\n",
        "# ])\n",
        "\n",
        "# optimizer = AdamOptimizer(stepsize=0.01)\n",
        "# epochs = 60"
      ],
      "metadata": {
        "id": "ikln0pq9rK60"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_params = qml.numpy.array(\n",
        "    # np.random.uniform(0, 2 * np.pi, size=(32,)),\n",
        "    # np.random.normal(0, 0.01, size=(32,)),\n",
        "    np.random.normal(0, 0.01, size=(36,)),\n",
        "    requires_grad=True\n",
        ")\n",
        "\n",
        "optimizer = AdamOptimizer(stepsize=0.005)\n",
        "epochs = 80"
      ],
      "metadata": {
        "id": "TYgq2DYYth7G"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Directory Creation**"
      ],
      "metadata": {
        "id": "MjYGEijbc7yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/MyDrive/QML-Research/Analysis\"\n",
        "\n",
        "folders_to_create = [\n",
        "    \"logs/qrnn_outputs\",\n",
        "    \"plots/loss_trend\",\n",
        "    \"plots/accuracy_trend\",\n",
        "    \"plots/test_boxplots\",\n",
        "]\n",
        "\n",
        "for folder in folders_to_create:\n",
        "    path = os.path.join(base_dir, folder)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f\"Created (or already exists): {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSUjeQ-BdAYy",
        "outputId": "fc71562e-19c0-41d3-a775-7de00172ae24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created (or already exists): /content/drive/MyDrive/QML-Research/Analysis/logs/qrnn_outputs\n",
            "Created (or already exists): /content/drive/MyDrive/QML-Research/Analysis/plots/loss_trend\n",
            "Created (or already exists): /content/drive/MyDrive/QML-Research/Analysis/plots/accuracy_trend\n",
            "Created (or already exists): /content/drive/MyDrive/QML-Research/Analysis/plots/test_boxplots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "Ce4aLIso4byH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "print(\"Training Loop\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        x_np = xb.squeeze(0).numpy()\n",
        "        y_np = yb.item()\n",
        "        x_qml = qml.numpy.array(x_np, requires_grad=False)\n",
        "\n",
        "        def cost(flat_params):\n",
        "          # wpw, clf = reshape_params(flat_params)\n",
        "          # logits = full_sentence_qnode(x_qml, wpw, clf)\n",
        "          logits = stacked_qrnn(x_qml, flat_params)\n",
        "          logits = qml.numpy.array(logits)\n",
        "          probs = softmax(logits)\n",
        "          return cross_entropy(y_np, probs)\n",
        "\n",
        "\n",
        "\n",
        "        # weights_in, weights_h = optimizer.step(cost, (weights_in, weights_h))\n",
        "        # logits = forward(x_np, weights_in, weights_h)\n",
        "        flat_params = optimizer.step(cost, flat_params)\n",
        "        # weights_per_word, classifier_weights = reshape_params(flat_params)\n",
        "        # logits = full_sentence_qnode(x_qml, weights_per_word, classifier_weights)\n",
        "        logits = stacked_qrnn(x_qml, flat_params)\n",
        "        probs = softmax(logits)\n",
        "        loss = cross_entropy(y_np, probs)\n",
        "\n",
        "        pred_label = qml.numpy.argmax(probs)\n",
        "\n",
        "        if pred_label == y_np:\n",
        "            correct += 1\n",
        "        epoch_loss += loss\n",
        "        total += 1\n",
        "\n",
        "    avg_loss = epoch_loss / total\n",
        "    acc = correct / total\n",
        "\n",
        "    train_losses.append(avg_loss)\n",
        "    train_accuracies.append(acc)\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    print(f\"Epoch {epoch:02d} | Loss: {avg_loss:.4f} | Acc: {acc:.4f} | Time: {epoch_time:.2f}s\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTraining complete in {total_time:.2f} seconds.\")"
      ],
      "metadata": {
        "id": "qOG0mTOZa7B4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595b5148-7937-41af-cac2-1baacda5a745"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loop\n",
            "\n",
            "Epoch 01 | Loss: 0.7178 | Acc: 0.5062 | Time: 48.89s\n",
            "Epoch 02 | Loss: 0.6904 | Acc: 0.5500 | Time: 46.75s\n",
            "Epoch 03 | Loss: 0.6821 | Acc: 0.5563 | Time: 46.36s\n",
            "Epoch 04 | Loss: 0.6779 | Acc: 0.5713 | Time: 46.74s\n",
            "Epoch 05 | Loss: 0.6744 | Acc: 0.5900 | Time: 46.57s\n",
            "Epoch 06 | Loss: 0.6708 | Acc: 0.6038 | Time: 46.28s\n",
            "Epoch 07 | Loss: 0.6703 | Acc: 0.6012 | Time: 50.52s\n",
            "Epoch 08 | Loss: 0.6693 | Acc: 0.5962 | Time: 49.04s\n",
            "Epoch 09 | Loss: 0.6671 | Acc: 0.6075 | Time: 48.55s\n",
            "Epoch 10 | Loss: 0.6674 | Acc: 0.6062 | Time: 47.12s\n",
            "Epoch 11 | Loss: 0.6662 | Acc: 0.6112 | Time: 47.28s\n",
            "Epoch 12 | Loss: 0.6672 | Acc: 0.6088 | Time: 47.22s\n",
            "Epoch 13 | Loss: 0.6671 | Acc: 0.6150 | Time: 47.74s\n",
            "Epoch 14 | Loss: 0.6672 | Acc: 0.6025 | Time: 47.49s\n",
            "Epoch 15 | Loss: 0.6679 | Acc: 0.6138 | Time: 46.87s\n",
            "Epoch 16 | Loss: 0.6664 | Acc: 0.6150 | Time: 47.58s\n",
            "Epoch 17 | Loss: 0.6658 | Acc: 0.6112 | Time: 47.20s\n",
            "Epoch 18 | Loss: 0.6670 | Acc: 0.6075 | Time: 47.15s\n",
            "Epoch 19 | Loss: 0.6666 | Acc: 0.6138 | Time: 46.97s\n",
            "Epoch 20 | Loss: 0.6650 | Acc: 0.6125 | Time: 51.56s\n",
            "Epoch 21 | Loss: 0.6659 | Acc: 0.6175 | Time: 48.75s\n",
            "Epoch 22 | Loss: 0.6665 | Acc: 0.6262 | Time: 47.73s\n",
            "Epoch 23 | Loss: 0.6656 | Acc: 0.6200 | Time: 47.60s\n",
            "Epoch 24 | Loss: 0.6668 | Acc: 0.6125 | Time: 47.14s\n",
            "Epoch 25 | Loss: 0.6660 | Acc: 0.6212 | Time: 47.45s\n",
            "Epoch 26 | Loss: 0.6664 | Acc: 0.6250 | Time: 47.79s\n",
            "Epoch 27 | Loss: 0.6667 | Acc: 0.6112 | Time: 47.89s\n",
            "Epoch 28 | Loss: 0.6657 | Acc: 0.6112 | Time: 46.97s\n",
            "Epoch 29 | Loss: 0.6647 | Acc: 0.6125 | Time: 47.02s\n",
            "Epoch 30 | Loss: 0.6663 | Acc: 0.6138 | Time: 47.14s\n",
            "Epoch 31 | Loss: 0.6633 | Acc: 0.6150 | Time: 48.71s\n",
            "Epoch 32 | Loss: 0.6650 | Acc: 0.6088 | Time: 47.30s\n",
            "Epoch 33 | Loss: 0.6665 | Acc: 0.6162 | Time: 47.65s\n",
            "Epoch 34 | Loss: 0.6658 | Acc: 0.5988 | Time: 46.55s\n",
            "Epoch 35 | Loss: 0.6649 | Acc: 0.6175 | Time: 46.55s\n",
            "Epoch 36 | Loss: 0.6660 | Acc: 0.6088 | Time: 46.27s\n",
            "Epoch 37 | Loss: 0.6664 | Acc: 0.6225 | Time: 46.71s\n",
            "Epoch 38 | Loss: 0.6653 | Acc: 0.6038 | Time: 46.62s\n",
            "Epoch 39 | Loss: 0.6657 | Acc: 0.6225 | Time: 46.26s\n",
            "Epoch 40 | Loss: 0.6654 | Acc: 0.6175 | Time: 46.34s\n",
            "Epoch 41 | Loss: 0.6665 | Acc: 0.6112 | Time: 46.34s\n",
            "Epoch 42 | Loss: 0.6657 | Acc: 0.6000 | Time: 46.42s\n",
            "Epoch 43 | Loss: 0.6633 | Acc: 0.6150 | Time: 46.51s\n",
            "Epoch 44 | Loss: 0.6659 | Acc: 0.6012 | Time: 46.39s\n",
            "Epoch 45 | Loss: 0.6658 | Acc: 0.6262 | Time: 46.47s\n",
            "Epoch 46 | Loss: 0.6656 | Acc: 0.6212 | Time: 46.48s\n",
            "Epoch 47 | Loss: 0.6651 | Acc: 0.6150 | Time: 46.52s\n",
            "Epoch 48 | Loss: 0.6651 | Acc: 0.6175 | Time: 46.03s\n",
            "Epoch 49 | Loss: 0.6659 | Acc: 0.5988 | Time: 46.59s\n",
            "Epoch 50 | Loss: 0.6659 | Acc: 0.6225 | Time: 46.93s\n",
            "Epoch 51 | Loss: 0.6657 | Acc: 0.6138 | Time: 47.40s\n",
            "Epoch 52 | Loss: 0.6656 | Acc: 0.6162 | Time: 47.41s\n",
            "Epoch 53 | Loss: 0.6661 | Acc: 0.6175 | Time: 46.62s\n",
            "Epoch 54 | Loss: 0.6643 | Acc: 0.6212 | Time: 45.98s\n",
            "Epoch 55 | Loss: 0.6652 | Acc: 0.6075 | Time: 46.67s\n",
            "Epoch 56 | Loss: 0.6661 | Acc: 0.6288 | Time: 46.05s\n",
            "Epoch 57 | Loss: 0.6642 | Acc: 0.6012 | Time: 46.27s\n",
            "Epoch 58 | Loss: 0.6637 | Acc: 0.6100 | Time: 46.48s\n",
            "Epoch 59 | Loss: 0.6657 | Acc: 0.6138 | Time: 46.91s\n",
            "Epoch 60 | Loss: 0.6665 | Acc: 0.6038 | Time: 46.73s\n",
            "Epoch 61 | Loss: 0.6657 | Acc: 0.5988 | Time: 46.84s\n",
            "Epoch 62 | Loss: 0.6654 | Acc: 0.6025 | Time: 46.99s\n",
            "Epoch 63 | Loss: 0.6663 | Acc: 0.6188 | Time: 46.95s\n",
            "Epoch 64 | Loss: 0.6651 | Acc: 0.6188 | Time: 46.88s\n",
            "Epoch 65 | Loss: 0.6650 | Acc: 0.6100 | Time: 46.98s\n",
            "Epoch 66 | Loss: 0.6657 | Acc: 0.6112 | Time: 47.05s\n",
            "Epoch 67 | Loss: 0.6643 | Acc: 0.6100 | Time: 47.93s\n",
            "Epoch 68 | Loss: 0.6662 | Acc: 0.6212 | Time: 46.98s\n",
            "Epoch 69 | Loss: 0.6654 | Acc: 0.6188 | Time: 47.16s\n",
            "Epoch 70 | Loss: 0.6653 | Acc: 0.6250 | Time: 46.22s\n",
            "Epoch 71 | Loss: 0.6665 | Acc: 0.6175 | Time: 47.23s\n",
            "Epoch 72 | Loss: 0.6658 | Acc: 0.6188 | Time: 47.45s\n",
            "Epoch 73 | Loss: 0.6654 | Acc: 0.6112 | Time: 46.80s\n",
            "Epoch 74 | Loss: 0.6658 | Acc: 0.6350 | Time: 46.70s\n",
            "Epoch 75 | Loss: 0.6655 | Acc: 0.6038 | Time: 46.93s\n",
            "Epoch 76 | Loss: 0.6649 | Acc: 0.6212 | Time: 46.39s\n",
            "Epoch 77 | Loss: 0.6654 | Acc: 0.6138 | Time: 46.83s\n",
            "Epoch 78 | Loss: 0.6658 | Acc: 0.6288 | Time: 46.48s\n",
            "Epoch 79 | Loss: 0.6657 | Acc: 0.6262 | Time: 46.48s\n",
            "Epoch 80 | Loss: 0.6640 | Acc: 0.6088 | Time: 47.23s\n",
            "\n",
            "Training complete in 3768.04 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**"
      ],
      "metadata": {
        "id": "VODeSsL-4dSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluate_test(weights_in, weights_h):\n",
        "# def evaluate_test(weights_per_word, classifier_weights):\n",
        "def evaluate_test(flat_params):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0\n",
        "\n",
        "    for xb, yb in test_loader:\n",
        "        x_np = xb.squeeze(0).numpy()\n",
        "        y_np = yb.item()\n",
        "        x_qml = qml.numpy.array(x_np, requires_grad=False)\n",
        "\n",
        "        # logits = full_sentence_qnode(x_qml, weights_per_word, classifier_weights)\n",
        "        logits = stacked_qrnn(x_qml, flat_params)\n",
        "        probs = softmax(logits)\n",
        "        loss = cross_entropy(y_np, probs)\n",
        "\n",
        "        pred = qml.numpy.argmax(probs)\n",
        "\n",
        "        if pred == y_np:\n",
        "            correct += 1\n",
        "        test_loss += loss\n",
        "        total += 1\n",
        "\n",
        "    return test_loss / total, correct / total"
      ],
      "metadata": {
        "id": "chAp3V1Ba692"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logging**"
      ],
      "metadata": {
        "id": "aJUnYQXIhCuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_test_results(model_name, test_acc, test_loss):\n",
        "    log_path = \"/content/drive/MyDrive/QML-Research/Analysis/logs/qrnn_outputs/accuracy_logs.txt\"\n",
        "\n",
        "    with open(log_path, \"a\") as f:\n",
        "        f.write(f\"[Model: {model_name}]\\n\")\n",
        "        f.write(f\"Test Accuracy: {test_acc:.4f}\\n\")\n",
        "        f.write(f\"Test Loss: {test_loss:.4f}\\n\\n\")\n",
        "\n",
        "    print(f\"Logged test results to: {log_path}\")"
      ],
      "metadata": {
        "id": "veWHFuY3fVo7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_training_plots(model_name, train_losses, train_accuracies):\n",
        "    base_path = \"/content/drive/MyDrive/QML-Research/Analysis/plots\"\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(train_losses, label=\"Training Loss\")\n",
        "    plt.title(f\"{model_name} - Loss Trend\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{base_path}/loss_trend/loss_plot_{model_name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(train_accuracies, label=\"Training Accuracy\", color='green')\n",
        "    plt.title(f\"{model_name} - Accuracy Trend\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{base_path}/accuracy_trend/acc_plot_{model_name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Plots saved to: {base_path}/loss_trend/ and /accuracy_trend/\")"
      ],
      "metadata": {
        "id": "x-bwkU_FfVmR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_loss, test_acc = evaluate_test(weights_in, weights_h)\n",
        "# test_loss, test_acc = evaluate_test(weights_per_word, classifier_weights)\n",
        "test_loss, test_acc = evaluate_test(flat_params)\n",
        "log_test_results(\"Stacked_QRNN_v6\", test_acc, test_loss)\n",
        "save_training_plots(\"Stacked_QRNN_v6\", train_losses, train_accuracies)"
      ],
      "metadata": {
        "id": "niKaK4AJfVjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab0fbf1a-dfe9-428f-ee5f-49cea9857d67"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged test results to: /content/drive/MyDrive/QML-Research/Analysis/logs/qrnn_outputs/accuracy_logs.txt\n",
            "Plots saved to: /content/drive/MyDrive/QML-Research/Analysis/plots/loss_trend/ and /accuracy_trend/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Saving and Loading**"
      ],
      "metadata": {
        "id": "Stu8Vrh9hL9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/QML-Research/Model Saves/stacked_qrnn_v6_params.pkl', 'wb') as f:\n",
        "    pickle.dump(flat_params, f)\n",
        "\n",
        "print(\"Saved Model parameters\")"
      ],
      "metadata": {
        "id": "VbBL3qvshQ63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874830f0-93f4-4572-f68b-f8f3c83d9184"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Model parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/QML-Research/Models/stacked_qrnn_v6_params.pkl', 'rb') as f:\n",
        "    flat_params = pickle.load(f)\n",
        "\n",
        "weights_per_word, classifier_weights = reshape_params(flat_params)\n",
        "print(\"Model parameters loaded\")"
      ],
      "metadata": {
        "id": "Yl65GX_UiGTU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}