{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xBSoJuk-YX2-",
        "L0JVY7_xYTua",
        "DvGPyfvsY4FH",
        "kMko8N6ha-19",
        "tyGflm5x0H48",
        "ax1GmPGn3hnl",
        "7jzjSqXhn5Vp",
        "5wHc_nu3ITLq",
        "h21yTxS64OsA",
        "MjYGEijbc7yy",
        "Ce4aLIso4byH",
        "VODeSsL-4dSr"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports and Downloads**"
      ],
      "metadata": {
        "id": "xBSoJuk-YX2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nind1MPQVDFp",
        "outputId": "745ca54e-483c-4e23-a38a-cf1766670413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pennylane in /usr/local/lib/python3.11/dist-packages (0.41.1)\n",
            "Requirement already satisfied: pennylane-lightning in /usr/local/lib/python3.11/dist-packages (0.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.15.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.5)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.16.0)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.8.0)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray>=0.6.11 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.7.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (24.2)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning) (0.3.30.0.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.7.9)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4 --upgrade --force-reinstall --quiet\n",
        "!pip install pennylane pennylane-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "7SIA-uaBVEDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqZ-dh-NVFkq",
        "outputId": "f7150415-46e7-45f7-c347-f6dabc2dc3c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Quantum computing\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "\n",
        "# NLP and preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Sklearn tools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "pnp.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "emjwDqgTVFhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1db4959-928c-49e2-bcab-2304c01a7138"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pennylane/capture/capture_operators.py:33: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.4.28. You have version 0.5.2 installed. Please downgrade JAX to <=0.4.28 to avoid runtime errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading and Preprocessing**"
      ],
      "metadata": {
        "id": "L0JVY7_xYTua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/QML-Research/Data/sentiment labelled sentences/amazon_cells_labelled.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "sentences = [line.split(\"\\t\")[0] for line in lines]\n",
        "labels = [int(line.split(\"\\t\")[1]) for line in lines]"
      ],
      "metadata": {
        "id": "IDJ9A_o7VFbu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "domain_neutral_words = {\n",
        "    \"phone\", \"product\", \"battery\", \"headset\", \"quality\", \"one\", \"use\"\n",
        "}\n",
        "stop_words.update(domain_neutral_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwR5eTa0YlFp",
        "outputId": "bc8ddced-6a38-4091-e1ed-91c12b3a1a00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "cleaned_sents = [clean_and_tokenize(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "IbJX9uXlVFZE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 10\n",
        "for i in range(len(cleaned_sents)):\n",
        "  if (len(cleaned_sents[i]) < max_len):\n",
        "    cleaned_sents[i] += [\"<PAD>\"] * (max_len - len(cleaned_sents[i]))\n",
        "  else:\n",
        "    cleaned_sents[i] = cleaned_sents[i][:max_len]"
      ],
      "metadata": {
        "id": "jommbUzPVFTx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GloVE Word Embeddings**"
      ],
      "metadata": {
        "id": "DvGPyfvsY4FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "KJfUcJaRVFQ1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_path = '/content/drive/MyDrive/QML-Research/Data/glove.6B.100d.txt'\n",
        "glove = load_glove_embeddings(glove_path)"
      ],
      "metadata": {
        "id": "Z40kR5fWZ_0Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AutoEncoder**"
      ],
      "metadata": {
        "id": "kMko8N6ha-19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GloVeAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(GloVeAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "zxYOHIgrVFIh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = list(glove.keys())\n",
        "all_vectors = np.array([glove[word] for word in all_words])\n",
        "all_vectors = normalize(all_vectors)\n",
        "word_tensor = torch.tensor(all_vectors).float()"
      ],
      "metadata": {
        "id": "FKWwxltMVFF6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 8\n",
        "epochs = 100\n",
        "save_path = '/content/drive/MyDrive/QML-Research/Autoencoder/glove_autoencoder_normalized_8.pth'\n",
        "# save_path = '/content/drive/MyDrive/QML-Research/Autoencoder/glove_autoencoder_normalized_32.pth'"
      ],
      "metadata": {
        "id": "HfGzHVQP0h6B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(save_path):\n",
        "    print(f\"Loading Autoencoder from {save_path}\")\n",
        "    autoencoder = GloVeAutoencoder(input_dim=100, latent_dim=latent_dim)\n",
        "    autoencoder.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
        "else:\n",
        "    print(\"Training Autoencoder\")\n",
        "    autoencoder = GloVeAutoencoder(input_dim=100, latent_dim=latent_dim)\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed = autoencoder(word_tensor)\n",
        "        loss = criterion(reconstructed, word_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "    torch.save(autoencoder.state_dict(), save_path)\n",
        "    print(f\"Saved Autoencoder serialized model in drive @ {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUDeGZHIVFC2",
        "outputId": "06ae721a-4a57-46a6-e82b-dd1633ca9127"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Autoencoder from /content/drive/MyDrive/QML-Research/Autoencoder/glove_autoencoder_normalized_8.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.eval()\n",
        "with torch.no_grad():\n",
        "    compressed_vectors = autoencoder.encoder(word_tensor).numpy()\n",
        "\n",
        "reduced_embeddings = {\n",
        "    word: compressed_vectors[i]\n",
        "    for i, word in enumerate(all_words)\n",
        "}"
      ],
      "metadata": {
        "id": "eZYs04ccVE_6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding**"
      ],
      "metadata": {
        "id": "tyGflm5x0H48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_vec(sentence, embeddings, dim):\n",
        "    vectors = []\n",
        "    for word in sentence:\n",
        "        if word in embeddings:\n",
        "            vectors.append(embeddings[word])\n",
        "        else:\n",
        "            vectors.append(np.zeros(dim))\n",
        "    return vectors\n",
        "\n",
        "def embed_sentences(cleaned_sents, embeddings, dim):\n",
        "    return np.array([sentence_to_vec(tokens, embeddings, dim) for tokens in cleaned_sents])"
      ],
      "metadata": {
        "id": "LHyTknfpVhiI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_embed_np = embed_sentences(cleaned_sents, reduced_embeddings, dim=8)\n",
        "X_embed_np = (X_embed_np - X_embed_np.min()) * (np.pi / (X_embed_np.max() - X_embed_np.min()))\n",
        "\n",
        "X_embed = torch.tensor(X_embed_np).long()\n",
        "y_embed = torch.tensor(labels).long()"
      ],
      "metadata": {
        "id": "PtDZbVTTVhfS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset and DataLoader**"
      ],
      "metadata": {
        "id": "ax1GmPGn3hnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AmazonDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "KZP80femFVRA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_embed, y_embed, test_size=0.2, stratify=y_embed.numpy(), random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = AmazonDataset(X_train, y_train)\n",
        "test_dataset = AmazonDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)"
      ],
      "metadata": {
        "id": "tLSMr6nta7Ki"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utility Functions**"
      ],
      "metadata": {
        "id": "7jzjSqXhn5Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "    e_x = np.exp(logits - np.max(logits))\n",
        "    return e_x / e_x.sum()"
      ],
      "metadata": {
        "id": "o_qhLqOXn5GC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(true_label, predictions):\n",
        "    j = np.clip(predictions[true_label], 1e-10, 1.0)\n",
        "    return -np.log(j)"
      ],
      "metadata": {
        "id": "o6m_Y06SoBn3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_angle_embedding(x, wires):\n",
        "    # For 32 dim:\n",
        "    # chunk_size = 8\n",
        "    chunk_size = 2\n",
        "\n",
        "    for i, wire in enumerate(wires):\n",
        "        for j in range(chunk_size):\n",
        "            idx = i * chunk_size + j\n",
        "            qml.RX(x[idx], wires=wire)\n",
        "            qml.RY(x[idx], wires=wire)\n",
        "            qml.RZ(x[idx], wires=wire)"
      ],
      "metadata": {
        "id": "2gB1FbT8oBjr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ansatz and QNode**"
      ],
      "metadata": {
        "id": "5wHc_nu3ITLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_qubits = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)"
      ],
      "metadata": {
        "id": "E1badQSvISf9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_step(inputs, hidden, weights_in, weights_h):\n",
        "    #Encoding\n",
        "    dense_angle_embedding(inputs, wires=[0, 1, 2, 3])\n",
        "    qml.RY(hidden, wires=3)\n",
        "\n",
        "    # Interaction Layer/Entanglement\n",
        "    qml.CNOT(wires=[0, 1])\n",
        "    qml.CNOT(wires=[1, 2])\n",
        "    qml.CNOT(wires=[2, 3])\n",
        "    qml.CNOT(wires=[3, 0])\n",
        "\n",
        "    # Parametrized trainable unitaries on input wires\n",
        "    for i in range(3):\n",
        "        qml.RY(weights_in[i], wires=i)\n",
        "        qml.RZ(weights_in[i + 3], wires=i)\n",
        "\n",
        "    # Parametrized unitaries on hidden wire\n",
        "    qml.RY(weights_h[0], wires=3)\n",
        "    qml.RZ(weights_h[1], wires=3)\n",
        "\n",
        "    return qml.expval(qml.PauliZ(2)), qml.expval(qml.PauliZ(3))"
      ],
      "metadata": {
        "id": "MyH4XOByISdZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Class**"
      ],
      "metadata": {
        "id": "h21yTxS64OsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(input_sequence, weights_in, weights_h, hidden_init=None):\n",
        "    \"\"\"\n",
        "    input_sequence: (10, 32) numpy array\n",
        "    weights_in: (6,) numpy array\n",
        "    weights_h: (2,) numpy array\n",
        "    hidden_init: optional scalar float, default is small random value\n",
        "    \"\"\"\n",
        "    if hidden_init is None:\n",
        "        hidden = np.random.uniform(-0.1, 0.1)\n",
        "    else:\n",
        "        hidden = hidden_init\n",
        "\n",
        "    for word_vec in input_sequence:\n",
        "        logits = quantum_step(word_vec, hidden, weights_in, weights_h)\n",
        "        hidden = np.tanh(logits[0] + logits[1])\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "eg6eRsHba7Ek"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_in = np.random.uniform(0, 2 * np.pi, size=6)\n",
        "weights_h  = np.random.uniform(0, 2 * np.pi, size=2)\n",
        "optimizer = AdamOptimizer(stepsize=0.01)\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "z4gZXW0t4jat"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Directory Creation**"
      ],
      "metadata": {
        "id": "MjYGEijbc7yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/MyDrive/QML-Research/Analysis\"\n",
        "\n",
        "folders_to_create = [\n",
        "    \"logs/qrnn_outputs\",\n",
        "    \"plots/loss_trend\",\n",
        "    \"plots/accuracy_trend\",\n",
        "    \"plots/test_boxplots\",\n",
        "]\n",
        "\n",
        "for folder in folders_to_create:\n",
        "    path = os.path.join(base_dir, folder)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f\"Created (or already exists): {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSUjeQ-BdAYy",
        "outputId": "fc71562e-19c0-41d3-a775-7de00172ae24"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created (or already exists): /content/drive/MyDrive/QML-Research/Analysis/logs/qrnn_outputs\n",
            "Created (or already exists): /content/drive/MyDrive/QML-Research/Analysis/plots/loss_trend\n",
            "Created (or already exists): /content/drive/MyDrive/QML-Research/Analysis/plots/accuracy_trend\n",
            "Created (or already exists): /content/drive/MyDrive/QML-Research/Analysis/plots/test_boxplots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "Ce4aLIso4byH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "print(\"Training Loop\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        x_np = xb.squeeze(0).numpy()\n",
        "        y_np = yb.item()\n",
        "\n",
        "        def cost(params):\n",
        "            w_in, w_h = params\n",
        "            logits = forward(x_np, w_in, w_h)\n",
        "            probs = softmax(np.array(logits))\n",
        "            return cross_entropy(y_np, probs)\n",
        "\n",
        "        weights_in, weights_h = optimizer.step(cost, (weights_in, weights_h))\n",
        "        logits = forward(x_np, weights_in, weights_h)\n",
        "        probs = softmax(np.array(logits))\n",
        "        loss = cross_entropy(y_np, probs)\n",
        "\n",
        "        pred_label = np.argmax(probs)\n",
        "\n",
        "        if pred_label == y_np:\n",
        "            correct += 1\n",
        "        epoch_loss += loss\n",
        "        total += 1\n",
        "\n",
        "    avg_loss = epoch_loss / total\n",
        "    acc = correct / total\n",
        "\n",
        "    train_losses.append(avg_loss)\n",
        "    train_accuracies.append(acc)\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    print(f\"Epoch {epoch:02d} | Loss: {avg_loss:.4f} | Acc: {acc:.4f} | Time: {epoch_time:.2f}s\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTraining complete in {total_time:.2f} seconds.\")"
      ],
      "metadata": {
        "id": "qOG0mTOZa7B4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e7724d-592f-4951-9aa4-3d49ecafe7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loop\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pennylane/_grad.py:216: UserWarning: Attempted to differentiate a function with no trainable parameters. If this is unintended, please add trainable parameters via the 'requires_grad' attribute or 'argnum' keyword.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Loss: 0.7626 | Acc: 0.5012 | Time: 161.84s\n",
            "Epoch 02 | Loss: 0.7626 | Acc: 0.5012 | Time: 159.10s\n",
            "Epoch 03 | Loss: 0.7626 | Acc: 0.5012 | Time: 159.77s\n",
            "Epoch 04 | Loss: 0.7626 | Acc: 0.5012 | Time: 170.14s\n",
            "Epoch 05 | Loss: 0.7626 | Acc: 0.5012 | Time: 161.22s\n",
            "Epoch 06 | Loss: 0.7626 | Acc: 0.5012 | Time: 158.76s\n",
            "Epoch 07 | Loss: 0.7626 | Acc: 0.5012 | Time: 158.39s\n",
            "Epoch 08 | Loss: 0.7626 | Acc: 0.5012 | Time: 157.65s\n",
            "Epoch 09 | Loss: 0.7626 | Acc: 0.5012 | Time: 158.04s\n",
            "Epoch 10 | Loss: 0.7626 | Acc: 0.5012 | Time: 157.93s\n",
            "Epoch 11 | Loss: 0.7626 | Acc: 0.5012 | Time: 157.85s\n",
            "Epoch 12 | Loss: 0.7626 | Acc: 0.5012 | Time: 160.06s\n",
            "Epoch 13 | Loss: 0.7626 | Acc: 0.5012 | Time: 157.08s\n",
            "Epoch 14 | Loss: 0.7626 | Acc: 0.5012 | Time: 157.37s\n",
            "Epoch 15 | Loss: 0.7626 | Acc: 0.5012 | Time: 156.76s\n",
            "Epoch 16 | Loss: 0.7626 | Acc: 0.5012 | Time: 157.20s\n",
            "Epoch 17 | Loss: 0.7626 | Acc: 0.5012 | Time: 156.77s\n",
            "Epoch 18 | Loss: 0.7626 | Acc: 0.5012 | Time: 155.87s\n",
            "Epoch 19 | Loss: 0.7626 | Acc: 0.5012 | Time: 156.85s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**"
      ],
      "metadata": {
        "id": "VODeSsL-4dSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_test(weights_in, weights_h):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0\n",
        "\n",
        "    for xb, yb in test_loader:\n",
        "        x_np = xb.squeeze(0).numpy()\n",
        "        y_np = yb.item()  # correct scalar label\n",
        "\n",
        "        logits = forward(x_np, weights_in, weights_h)\n",
        "        probs = softmax(np.array(logits))\n",
        "        loss = cross_entropy(y_np, probs)\n",
        "\n",
        "        pred = np.argmax(probs)\n",
        "        true = y_np\n",
        "\n",
        "        if pred == true:\n",
        "            correct += 1\n",
        "        test_loss += loss\n",
        "        total += 1\n",
        "\n",
        "    return test_loss / total, correct / total"
      ],
      "metadata": {
        "id": "chAp3V1Ba692"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logging**"
      ],
      "metadata": {
        "id": "aJUnYQXIhCuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_test_results(model_name, test_acc, test_loss):\n",
        "    log_path = \"/content/drive/MyDrive/QML-Research/Analysis/logs/qrnn_outputs/accuracy_logs.txt\"\n",
        "\n",
        "    with open(log_path, \"a\") as f:\n",
        "        f.write(f\"[Model: {model_name}]\\n\")\n",
        "        f.write(f\"Test Accuracy: {test_acc:.4f}\\n\")\n",
        "        f.write(f\"Test Loss: {test_loss:.4f}\\n\\n\")\n",
        "\n",
        "    print(f\"Logged test results to: {log_path}\")"
      ],
      "metadata": {
        "id": "veWHFuY3fVo7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_training_plots(model_name, train_losses, train_accuracies):\n",
        "    base_path = \"/content/drive/MyDrive/QML-Research/Analysis/plots\"\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(train_losses, label=\"Training Loss\")\n",
        "    plt.title(f\"{model_name} - Loss Trend\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{base_path}/loss_trend/loss_plot_{model_name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(train_accuracies, label=\"Training Accuracy\", color='green')\n",
        "    plt.title(f\"{model_name} - Accuracy Trend\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{base_path}/accuracy_trend/acc_plot_{model_name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Plots saved to: {base_path}/loss_trend/ and /accuracy_trend/\")"
      ],
      "metadata": {
        "id": "x-bwkU_FfVmR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate_test(weights_in, weights_h)\n",
        "log_test_results(\"QRNN_v2\", test_acc, test_loss)\n",
        "save_training_plots(\"QRNN_v2\", train_losses, train_accuracies)"
      ],
      "metadata": {
        "id": "niKaK4AJfVjB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}