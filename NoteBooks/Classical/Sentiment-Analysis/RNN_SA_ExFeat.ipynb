{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBSoJuk-YX2-"
      },
      "source": [
        "# **Imports and Downloads**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqZ-dh-NVFkq",
        "outputId": "edf40a07-f849-48f7-9153-63179d46d1bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emjwDqgTVFhi"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import math\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# NLP and preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, opinion_lexicon\n",
        "from nltk.util import ngrams\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Sklearn tools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize, StandardScaler\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0JVY7_xYTua"
      },
      "source": [
        "# **Data Loading and Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDJ9A_o7VFbu"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/QML-Research/Data/sentiment labelled sentences/amazon_cells_labelled.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "sentences = [line.split(\"\\t\")[0] for line in lines]\n",
        "labels = [int(line.split(\"\\t\")[1]) for line in lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwR5eTa0YlFp",
        "outputId": "cae35be5-8a62-4551-e2d6-b0c06a11bd7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "domain_neutral_words = {\n",
        "    \"phone\", \"product\", \"battery\", \"headset\", \"quality\", \"one\", \"use\"\n",
        "}\n",
        "stop_words.update(domain_neutral_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbJX9uXlVFZE"
      },
      "outputs": [],
      "source": [
        "def clean_and_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "cleaned_sents = [clean_and_tokenize(sentence) for sentence in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jommbUzPVFTx"
      },
      "outputs": [],
      "source": [
        "max_len = 10\n",
        "for i in range(len(cleaned_sents)):\n",
        "  if (len(cleaned_sents[i]) < max_len):\n",
        "    cleaned_sents[i] += [\"<PAD>\"] * (max_len - len(cleaned_sents[i]))\n",
        "  else:\n",
        "    cleaned_sents[i] = cleaned_sents[i][:max_len]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvGPyfvsY4FH"
      },
      "source": [
        "# **GloVE Word Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJfUcJaRVFQ1"
      },
      "outputs": [],
      "source": [
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z40kR5fWZ_0Z"
      },
      "outputs": [],
      "source": [
        "glove_path = '/content/drive/MyDrive/QML-Research/Data/glove.6B.100d.txt'\n",
        "glove = load_glove_embeddings(glove_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMko8N6ha-19"
      },
      "source": [
        "# **AutoEncoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxYOHIgrVFIh"
      },
      "outputs": [],
      "source": [
        "class GloVeAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(GloVeAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKWwxltMVFF6"
      },
      "outputs": [],
      "source": [
        "all_words = list(glove.keys())\n",
        "all_vectors = np.array([glove[word] for word in all_words])\n",
        "all_vectors = normalize(all_vectors)\n",
        "word_tensor = torch.tensor(all_vectors).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfGzHVQP0h6B"
      },
      "outputs": [],
      "source": [
        "latent_dim = 8\n",
        "epochs = 100\n",
        "save_path = f\"/content/drive/MyDrive/QML-Research/Autoencoder-weights/glove_autoencoder_normalized_{latent_dim}.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUDeGZHIVFC2",
        "outputId": "515ce5ce-32d6-4b6d-f400-3c033ef7234c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Autoencoder from /content/drive/MyDrive/QML-Research/Autoencoder-weights/glove_autoencoder_normalized_8.pth\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(save_path):\n",
        "    print(f\"Loading Autoencoder from {save_path}\")\n",
        "    autoencoder = GloVeAutoencoder(input_dim=100, latent_dim=latent_dim)\n",
        "    autoencoder.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
        "else:\n",
        "    print(\"Training Autoencoder\")\n",
        "    autoencoder = GloVeAutoencoder(input_dim=100, latent_dim=latent_dim)\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed = autoencoder(word_tensor)\n",
        "        loss = criterion(reconstructed, word_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "    torch.save(autoencoder.state_dict(), save_path)\n",
        "    print(f\"Saved Autoencoder serialized model in drive @ {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZYs04ccVE_6"
      },
      "outputs": [],
      "source": [
        "autoencoder.eval()\n",
        "with torch.no_grad():\n",
        "    compressed_vectors = autoencoder.encoder(word_tensor).numpy()\n",
        "\n",
        "reduced_embeddings = {\n",
        "    word: compressed_vectors[i]\n",
        "    for i, word in enumerate(all_words)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Opinion Lexicons, Negators & Vader Compound**"
      ],
      "metadata": {
        "id": "wTN0vRiv2PJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"opinion_lexicon\")\n",
        "POS_SET = set(opinion_lexicon.positive())\n",
        "NEG_SET = set(opinion_lexicon.negative())\n",
        "print(f\"Positive Lexicons length: {len(POS_SET)}\")\n",
        "print(f\"Negative Lexicons length: {len(NEG_SET)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQk2A_B_2Ouq",
        "outputId": "6a171560-6c2d-4bec-f093-0b77b96e74e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Lexicons length: 2006\n",
            "Negative Lexicons length: 4783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"vader_lexicon\")\n",
        "SIA = SentimentIntensityAnalyzer()\n",
        "\n",
        "NEGATORS = {\n",
        "    \"no\",\"not\",\"never\",\"hardly\",\"scarcely\",\"barely\",\"cannot\",\n",
        "    \"cant\",\"isnt\",\"arent\",\"werent\",\"wasnt\",\"dont\",\"doesnt\",\n",
        "    \"didnt\",\"won't\",\"wont\",\"can't\",\"isn't\",\"aren't\",\"weren't\",\n",
        "    \"wasn't\",\"don't\",\"doesn't\",\"didn't\"\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dagECluF9Ne",
        "outputId": "a7d894b3-5a94-4d5b-9149-e0cbcfdb9bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_feats_from_tokens(tokens, pos_set=POS_SET, neg_set=NEG_SET, raw_text=None):\n",
        "    toks = [t for t in tokens if t.isalpha()]\n",
        "    total = len(toks) if toks else 1\n",
        "    pos_cnt = sum(1 for t in toks if t in pos_set)\n",
        "    neg_cnt = sum(1 for t in toks if t in neg_set)\n",
        "    pos_ratio = pos_cnt / total\n",
        "    neg_ratio = neg_cnt / total\n",
        "    negation_count = float(sum(1 for t in toks if t in NEGATORS))\n",
        "\n",
        "    if raw_text is None:\n",
        "        raw_text = \" \".join(toks)\n",
        "    vader_compound = float(SIA.polarity_scores(raw_text)[\"compound\"])\n",
        "\n",
        "    return [pos_ratio, neg_ratio, negation_count, vader_compound]"
      ],
      "metadata": {
        "id": "e3WoE8W72VSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyGflm5x0H48"
      },
      "source": [
        "# **Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHyTknfpVhiI"
      },
      "outputs": [],
      "source": [
        "def sentence_to_vec(sentence, embeddings, dim):\n",
        "    vectors = []\n",
        "    for word in sentence:\n",
        "        if word in embeddings:\n",
        "            vectors.append(embeddings[word])\n",
        "        else:\n",
        "            vectors.append(np.zeros(dim))\n",
        "    return vectors\n",
        "\n",
        "def embed_sentences(cleaned_sents, embeddings, dim):\n",
        "    return np.array([sentence_to_vec(tokens, embeddings, dim) for tokens in cleaned_sents])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_embed_np = embed_sentences(cleaned_sents, reduced_embeddings, dim=8)\n",
        "X_feats_np = np.array(\n",
        "    [simple_feats_from_tokens(tokens, raw_text=sentences[i]) for i, tokens in enumerate(cleaned_sents)],\n",
        "    dtype=\"float32\"\n",
        ")\n",
        "labels_np  = np.array(labels, dtype=np.int64)\n",
        "\n",
        "idx_all = np.arange(len(labels_np))\n",
        "idx_tr, idx_te = train_test_split(\n",
        "    idx_all, test_size=0.2, random_state=42, stratify=labels_np\n",
        ")\n",
        "\n",
        "mu = X_embed_np[idx_tr].mean(axis=(0, 1), keepdims=True)\n",
        "sd = X_embed_np[idx_tr].std(axis=(0, 1), keepdims=True) + 1e-8\n",
        "X_embed_std = (X_embed_np - mu) / sd\n",
        "\n",
        "feat_scaler = StandardScaler().fit(X_feats_np[idx_tr])\n",
        "X_feats_std = feat_scaler.transform(X_feats_np)"
      ],
      "metadata": {
        "id": "W_MmhNyi_lA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax1GmPGn3hnl"
      },
      "source": [
        "# **Dataset and DataLoader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZP80femFVRA"
      },
      "outputs": [],
      "source": [
        "class AmazonDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X_seq, X_feats, y):\n",
        "        self.X_seq = X_seq\n",
        "        self.X_feats = X_feats\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X_seq[idx], self.X_feats[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLSMr6nta7Ki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada1c5db-4a0a-4ff5-8a44-da13b17a8750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings (std) train shape: torch.Size([800, 10, 8])\n",
            "Extra features (std) train shape: torch.Size([800, 4])\n"
          ]
        }
      ],
      "source": [
        "X_seq_train   = torch.tensor(X_embed_std[idx_tr]).float()\n",
        "X_seq_test    = torch.tensor(X_embed_std[idx_te]).float()\n",
        "X_feats_train = torch.tensor(X_feats_std[idx_tr]).float()\n",
        "X_feats_test  = torch.tensor(X_feats_std[idx_te]).float()\n",
        "y_train       = torch.tensor(labels_np[idx_tr]).long()\n",
        "y_test        = torch.tensor(labels_np[idx_te]).long()\n",
        "\n",
        "print(\"Embeddings (std) train shape:\", X_seq_train.shape)\n",
        "print(\"Extra features (std) train shape:\", X_feats_train.shape)\n",
        "\n",
        "train_dataset = AmazonDataset(X_seq_train, X_feats_train, y_train)\n",
        "test_dataset  = AmazonDataset(X_seq_test,  X_feats_test,  y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTirzz1ArZ1R"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDEO8TIRy9UF"
      },
      "outputs": [],
      "source": [
        "class SimpleSeqFeatClassifier(nn.Module):\n",
        "    def __init__(self, emb_dim=8, feat_dim=4, hidden=64, num_classes=2, p_drop=0.2):\n",
        "        super().__init__()\n",
        "        self.token_proj = nn.Sequential(\n",
        "            nn.Linear(emb_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden + feat_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p_drop),\n",
        "            nn.Linear(64, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_seq, x_feats):\n",
        "        H = self.token_proj(x_seq)\n",
        "        h_sent = H.mean(dim=1)\n",
        "        fused = torch.cat([h_sent, x_feats], dim=-1)\n",
        "        logits = self.classifier(fused)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvRycYz9rG81"
      },
      "source": [
        "# **Parameter Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej6BAEBKyrb3"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SimpleSeqFeatClassifier(\n",
        "    emb_dim=8,\n",
        "    feat_dim=4,\n",
        "    hidden=64,\n",
        "    num_classes=2\n",
        ").to(device)\n",
        "\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce4aLIso4byH"
      },
      "source": [
        "# **Training & Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOG0mTOZa7B4"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for x_seq, x_feats, y in loader:\n",
        "        x_seq, x_feats, y = x_seq.to(device), x_feats.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x_seq, x_feats)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return total_loss / total, correct / total"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for x_seq, x_feats, y in loader:\n",
        "        x_seq, x_feats, y = x_seq.to(device), x_feats.to(device), y.to(device)\n",
        "        logits = model(x_seq, x_feats)\n",
        "        loss = criterion(logits, y)\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return total_loss / total, correct / total"
      ],
      "metadata": {
        "id": "u_m1-OAtsRLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_state = None\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    te_loss, te_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    if te_acc > best_acc:\n",
        "        best_acc = te_acc\n",
        "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Train loss {tr_loss:.4f} acc {tr_acc:.3f} \"\n",
        "          f\"| Test loss {te_loss:.4f} acc {te_acc:.3f}\")\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/QML-Research/Model Saves/Classical/simple_seq_feat_v1.pt\")\n",
        "\n",
        "final_loss, final_acc = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"Final Test — loss: {final_loss:.4f}, acc: {final_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXC6dV-NeoEX",
        "outputId": "a7badad0-62f8-4683-9506-cc329d83bb7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train loss 0.6014 acc 0.811 | Test loss 0.5436 acc 0.810\n",
            "Epoch 02 | Train loss 0.4661 acc 0.845 | Test loss 0.4577 acc 0.815\n",
            "Epoch 03 | Train loss 0.3888 acc 0.873 | Test loss 0.4076 acc 0.810\n",
            "Epoch 04 | Train loss 0.3463 acc 0.870 | Test loss 0.3987 acc 0.815\n",
            "Epoch 05 | Train loss 0.3263 acc 0.868 | Test loss 0.3839 acc 0.825\n",
            "Epoch 06 | Train loss 0.3167 acc 0.873 | Test loss 0.3775 acc 0.825\n",
            "Epoch 07 | Train loss 0.3179 acc 0.861 | Test loss 0.3737 acc 0.820\n",
            "Epoch 08 | Train loss 0.3096 acc 0.870 | Test loss 0.3775 acc 0.835\n",
            "Epoch 09 | Train loss 0.3029 acc 0.874 | Test loss 0.3751 acc 0.840\n",
            "Epoch 10 | Train loss 0.3006 acc 0.864 | Test loss 0.3762 acc 0.840\n",
            "Epoch 11 | Train loss 0.2908 acc 0.881 | Test loss 0.3741 acc 0.830\n",
            "Epoch 12 | Train loss 0.2886 acc 0.881 | Test loss 0.3745 acc 0.835\n",
            "Epoch 13 | Train loss 0.2918 acc 0.879 | Test loss 0.3722 acc 0.825\n",
            "Epoch 14 | Train loss 0.2919 acc 0.870 | Test loss 0.3748 acc 0.830\n",
            "Epoch 15 | Train loss 0.2871 acc 0.875 | Test loss 0.3753 acc 0.820\n",
            "Epoch 16 | Train loss 0.2912 acc 0.877 | Test loss 0.3747 acc 0.830\n",
            "Epoch 17 | Train loss 0.2839 acc 0.875 | Test loss 0.3792 acc 0.835\n",
            "Epoch 18 | Train loss 0.2829 acc 0.881 | Test loss 0.3763 acc 0.830\n",
            "Epoch 19 | Train loss 0.2828 acc 0.884 | Test loss 0.3724 acc 0.820\n",
            "Epoch 20 | Train loss 0.2893 acc 0.870 | Test loss 0.3717 acc 0.825\n",
            "Epoch 21 | Train loss 0.2816 acc 0.881 | Test loss 0.3801 acc 0.840\n",
            "Epoch 22 | Train loss 0.2785 acc 0.881 | Test loss 0.3707 acc 0.820\n",
            "Epoch 23 | Train loss 0.2746 acc 0.887 | Test loss 0.3687 acc 0.820\n",
            "Epoch 24 | Train loss 0.2700 acc 0.892 | Test loss 0.3705 acc 0.820\n",
            "Epoch 25 | Train loss 0.2773 acc 0.880 | Test loss 0.3731 acc 0.820\n",
            "Epoch 26 | Train loss 0.2657 acc 0.889 | Test loss 0.3694 acc 0.825\n",
            "Epoch 27 | Train loss 0.2716 acc 0.887 | Test loss 0.3769 acc 0.820\n",
            "Epoch 28 | Train loss 0.2727 acc 0.886 | Test loss 0.3719 acc 0.820\n",
            "Epoch 29 | Train loss 0.2671 acc 0.880 | Test loss 0.3733 acc 0.820\n",
            "Epoch 30 | Train loss 0.2661 acc 0.890 | Test loss 0.3864 acc 0.845\n",
            "Epoch 31 | Train loss 0.2688 acc 0.886 | Test loss 0.3726 acc 0.820\n",
            "Epoch 32 | Train loss 0.2608 acc 0.887 | Test loss 0.3778 acc 0.820\n",
            "Epoch 33 | Train loss 0.2615 acc 0.887 | Test loss 0.3709 acc 0.825\n",
            "Epoch 34 | Train loss 0.2630 acc 0.887 | Test loss 0.3791 acc 0.825\n",
            "Epoch 35 | Train loss 0.2603 acc 0.880 | Test loss 0.3738 acc 0.810\n",
            "Epoch 36 | Train loss 0.2547 acc 0.881 | Test loss 0.3826 acc 0.830\n",
            "Epoch 37 | Train loss 0.2550 acc 0.890 | Test loss 0.3747 acc 0.830\n",
            "Epoch 38 | Train loss 0.2566 acc 0.890 | Test loss 0.3780 acc 0.825\n",
            "Epoch 39 | Train loss 0.2596 acc 0.880 | Test loss 0.3878 acc 0.835\n",
            "Epoch 40 | Train loss 0.2587 acc 0.882 | Test loss 0.3824 acc 0.830\n",
            "Epoch 41 | Train loss 0.2560 acc 0.885 | Test loss 0.3746 acc 0.825\n",
            "Epoch 42 | Train loss 0.2479 acc 0.891 | Test loss 0.3862 acc 0.805\n",
            "Epoch 43 | Train loss 0.2664 acc 0.885 | Test loss 0.3998 acc 0.820\n",
            "Epoch 44 | Train loss 0.2469 acc 0.886 | Test loss 0.3856 acc 0.835\n",
            "Epoch 45 | Train loss 0.2472 acc 0.891 | Test loss 0.3867 acc 0.820\n",
            "Epoch 46 | Train loss 0.2489 acc 0.889 | Test loss 0.3907 acc 0.830\n",
            "Epoch 47 | Train loss 0.2459 acc 0.895 | Test loss 0.3812 acc 0.825\n",
            "Epoch 48 | Train loss 0.2486 acc 0.890 | Test loss 0.3942 acc 0.820\n",
            "Epoch 49 | Train loss 0.2419 acc 0.892 | Test loss 0.3916 acc 0.830\n",
            "Epoch 50 | Train loss 0.2474 acc 0.886 | Test loss 0.3955 acc 0.820\n",
            "Epoch 51 | Train loss 0.2453 acc 0.889 | Test loss 0.3901 acc 0.810\n",
            "Epoch 52 | Train loss 0.2367 acc 0.896 | Test loss 0.4007 acc 0.800\n",
            "Epoch 53 | Train loss 0.2445 acc 0.887 | Test loss 0.3948 acc 0.815\n",
            "Epoch 54 | Train loss 0.2379 acc 0.891 | Test loss 0.4048 acc 0.800\n",
            "Epoch 55 | Train loss 0.2380 acc 0.890 | Test loss 0.4046 acc 0.815\n",
            "Epoch 56 | Train loss 0.2389 acc 0.886 | Test loss 0.3919 acc 0.815\n",
            "Epoch 57 | Train loss 0.2354 acc 0.902 | Test loss 0.4031 acc 0.810\n",
            "Epoch 58 | Train loss 0.2357 acc 0.890 | Test loss 0.3947 acc 0.810\n",
            "Epoch 59 | Train loss 0.2282 acc 0.891 | Test loss 0.4023 acc 0.810\n",
            "Epoch 60 | Train loss 0.2268 acc 0.892 | Test loss 0.4060 acc 0.800\n",
            "Epoch 61 | Train loss 0.2394 acc 0.891 | Test loss 0.4088 acc 0.800\n",
            "Epoch 62 | Train loss 0.2299 acc 0.896 | Test loss 0.4133 acc 0.805\n",
            "Epoch 63 | Train loss 0.2264 acc 0.894 | Test loss 0.4069 acc 0.805\n",
            "Epoch 64 | Train loss 0.2214 acc 0.897 | Test loss 0.4095 acc 0.805\n",
            "Epoch 65 | Train loss 0.2249 acc 0.890 | Test loss 0.4085 acc 0.800\n",
            "Epoch 66 | Train loss 0.2318 acc 0.884 | Test loss 0.4051 acc 0.800\n",
            "Epoch 67 | Train loss 0.2252 acc 0.896 | Test loss 0.4110 acc 0.800\n",
            "Epoch 68 | Train loss 0.2216 acc 0.896 | Test loss 0.4116 acc 0.810\n",
            "Epoch 69 | Train loss 0.2194 acc 0.902 | Test loss 0.4148 acc 0.800\n",
            "Epoch 70 | Train loss 0.2274 acc 0.899 | Test loss 0.4247 acc 0.810\n",
            "Epoch 71 | Train loss 0.2264 acc 0.902 | Test loss 0.4229 acc 0.800\n",
            "Epoch 72 | Train loss 0.2233 acc 0.897 | Test loss 0.4234 acc 0.800\n",
            "Epoch 73 | Train loss 0.2178 acc 0.900 | Test loss 0.4186 acc 0.800\n",
            "Epoch 74 | Train loss 0.2184 acc 0.906 | Test loss 0.4241 acc 0.800\n",
            "Epoch 75 | Train loss 0.2207 acc 0.907 | Test loss 0.4291 acc 0.800\n",
            "Epoch 76 | Train loss 0.2125 acc 0.904 | Test loss 0.4288 acc 0.800\n",
            "Epoch 77 | Train loss 0.2130 acc 0.902 | Test loss 0.4259 acc 0.795\n",
            "Epoch 78 | Train loss 0.2102 acc 0.902 | Test loss 0.4298 acc 0.795\n",
            "Epoch 79 | Train loss 0.2104 acc 0.907 | Test loss 0.4248 acc 0.810\n",
            "Epoch 80 | Train loss 0.2065 acc 0.905 | Test loss 0.4292 acc 0.800\n",
            "Epoch 81 | Train loss 0.2105 acc 0.901 | Test loss 0.4345 acc 0.785\n",
            "Epoch 82 | Train loss 0.2193 acc 0.906 | Test loss 0.4398 acc 0.795\n",
            "Epoch 83 | Train loss 0.2196 acc 0.905 | Test loss 0.4342 acc 0.795\n",
            "Epoch 84 | Train loss 0.2012 acc 0.919 | Test loss 0.4387 acc 0.785\n",
            "Epoch 85 | Train loss 0.2150 acc 0.910 | Test loss 0.4338 acc 0.790\n",
            "Epoch 86 | Train loss 0.2052 acc 0.915 | Test loss 0.4385 acc 0.790\n",
            "Epoch 87 | Train loss 0.2088 acc 0.902 | Test loss 0.4425 acc 0.790\n",
            "Epoch 88 | Train loss 0.2147 acc 0.909 | Test loss 0.4384 acc 0.795\n",
            "Epoch 89 | Train loss 0.2077 acc 0.907 | Test loss 0.4217 acc 0.815\n",
            "Epoch 90 | Train loss 0.1994 acc 0.917 | Test loss 0.4420 acc 0.795\n",
            "Epoch 91 | Train loss 0.2013 acc 0.914 | Test loss 0.4434 acc 0.805\n",
            "Epoch 92 | Train loss 0.2042 acc 0.914 | Test loss 0.4501 acc 0.780\n",
            "Epoch 93 | Train loss 0.2064 acc 0.910 | Test loss 0.4470 acc 0.800\n",
            "Epoch 94 | Train loss 0.1998 acc 0.914 | Test loss 0.4519 acc 0.795\n",
            "Epoch 95 | Train loss 0.2040 acc 0.915 | Test loss 0.4571 acc 0.795\n",
            "Epoch 96 | Train loss 0.1968 acc 0.922 | Test loss 0.4537 acc 0.810\n",
            "Epoch 97 | Train loss 0.1955 acc 0.915 | Test loss 0.4528 acc 0.790\n",
            "Epoch 98 | Train loss 0.1956 acc 0.920 | Test loss 0.4663 acc 0.790\n",
            "Epoch 99 | Train loss 0.1962 acc 0.912 | Test loss 0.4459 acc 0.795\n",
            "Epoch 100 | Train loss 0.1988 acc 0.914 | Test loss 0.4766 acc 0.800\n",
            "Final Test — loss: 0.4766, acc: 0.800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r-VF2IWe1lJ"
      },
      "source": [
        "# **Test Inference**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_pad(text, max_len=10):\n",
        "    t = text.lower()\n",
        "    t = re.sub(r\"http\\S+\", \"\", t)\n",
        "    t = re.sub(r\"[^a-z0-9\\s]\", \"\", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    tokens = [w for w in t.split() if w not in stop_words]\n",
        "    if len(tokens) < max_len:\n",
        "        tokens = tokens + [\"<PAD>\"] * (max_len - len(tokens))\n",
        "    else:\n",
        "        tokens = tokens[:max_len]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "VRA_oIv3lgQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJCzUM2Me1Ie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c971a3c-0dee-4b02-988b-6f0a0b13fe61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POS] [0.0035899747163057327, 0.996410071849823] :: This charger is amazing, super fast and highly reliable.\n",
            "[NEG] [0.9969472289085388, 0.0030527871567755938] :: Worst headphones ever bought, awful sound and terrible build quality.\n",
            "[POS] [0.35570475459098816, 0.6442952156066895] :: Very satisfied with camera quality considering the affordable price range.\n",
            "[NEG] [0.9653716087341309, 0.03462842106819153] :: Product stopped working completely within one week of regular usage.\n",
            "[POS] [0.0027090604417026043, 0.9972909092903137] :: Excellent screen resolution, great brightness and strong battery backup too.\n",
            "[POS] [0.08565100282430649, 0.9143490195274353] :: This is the best light bulb I’ve ever used.\n",
            "[NEG] [0.9987524747848511, 0.0012475211406126618] :: Did not match the description, complete waste of my money.\n",
            "[POS] [0.0037493137642741203, 0.9962506890296936] :: This chair feels sturdy, comfortable, and worth every single penny.\n",
            "[POS] [0.0017618017736822367, 0.998238205909729] :: Absolutely love this phone case, protective, stylish and well-made.\n",
            "[NEG] [0.9993202686309814, 0.0006796938250772655] :: Speaker quality is poor, distorts quickly at medium volume levels.\n",
            "[NEG] [0.9635790586471558, 0.03642096742987633] :: Delivery arrived quickly with excellent packaging and no visible damages.\n",
            "[NEG] [0.9713364243507385, 0.02866356633603573] :: Avoid this item entirely, cheap build and constantly stops working.\n",
            "[POS] [0.010667734779417515, 0.9893323183059692] :: Best pair of scissors ever purchased, sharp, durable and reliable.\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def predict_texts(model, texts, device, emb_dim=8, max_len=10):\n",
        "    token_lists = [tokenize_and_pad(t, max_len=max_len) for t in texts]\n",
        "    feat_tokens = [[w for w in toks if w.isalpha()] for toks in token_lists]\n",
        "    feats_np = np.array(\n",
        "        [simple_feats_from_tokens(toks, raw_text=txt) for toks, txt in zip(feat_tokens, texts)],\n",
        "        dtype=\"float32\"\n",
        "    )\n",
        "    feats_np = feat_scaler.transform(feats_np)\n",
        "    X_feats = torch.tensor(feats_np).float().to(device)\n",
        "    X_seq_np = embed_sentences(token_lists, reduced_embeddings, dim=emb_dim)\n",
        "    X_seq_np = (X_seq_np - mu) / sd\n",
        "    X_seq = torch.tensor(X_seq_np).float().to(device)\n",
        "\n",
        "    model.eval()\n",
        "    logits = model(X_seq, X_feats)\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "    preds = logits.argmax(dim=1).cpu().numpy()\n",
        "    return preds, probs\n",
        "\n",
        "trial_sentences = [\n",
        "    \"This charger is amazing, super fast and highly reliable.\",\n",
        "    \"Worst headphones ever bought, awful sound and terrible build quality.\",\n",
        "    \"Very satisfied with camera quality considering the affordable price range.\",\n",
        "    \"Product stopped working completely within one week of regular usage.\",\n",
        "    \"Excellent screen resolution, great brightness and strong battery backup too.\",\n",
        "    \"This is the best light bulb I’ve ever used.\",\n",
        "    \"Did not match the description, complete waste of my money.\",\n",
        "    \"This chair feels sturdy, comfortable, and worth every single penny.\",\n",
        "    \"Absolutely love this phone case, protective, stylish and well-made.\",\n",
        "    \"Speaker quality is poor, distorts quickly at medium volume levels.\",\n",
        "    \"Delivery arrived quickly with excellent packaging and no visible damages.\",\n",
        "    \"Avoid this item entirely, cheap build and constantly stops working.\",\n",
        "    \"Best pair of scissors ever purchased, sharp, durable and reliable.\"\n",
        "]\n",
        "\n",
        "preds, probs = predict_texts(model, trial_sentences, device, emb_dim=8, max_len=10)\n",
        "\n",
        "for s, p, pr in zip(trial_sentences, preds, probs):\n",
        "    label = \"POS\" if int(p)==1 else \"NEG\"\n",
        "    print(f\"[{label}] {pr.tolist()} :: {s}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xBSoJuk-YX2-",
        "L0JVY7_xYTua",
        "DvGPyfvsY4FH",
        "kMko8N6ha-19",
        "wTN0vRiv2PJX",
        "tyGflm5x0H48",
        "ax1GmPGn3hnl",
        "vTirzz1ArZ1R",
        "qvRycYz9rG81"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}